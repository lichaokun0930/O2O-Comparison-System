"""
åˆ†æå»é‡å¯¹åŒ¹é…ç»“æœçš„å½±å“

é—®é¢˜ï¼šæœ¬åº—ABCDåŒ¹é…åˆ°ç«å¯¹AAAAï¼Œå»é‡åä¿ç•™ABï¼ŒCDå»å“ªäº†ï¼Ÿ
ç­”æ¡ˆï¼šCDè¢«è®¤ä¸ºæ˜¯"æœªåŒ¹é…"ï¼Œä¼šè¿›å…¥åç»­çš„è½¯åŒ¹é…æˆ–å·®å¼‚å“åŒ¹é…
"""

import pandas as pd
from pathlib import Path

def analyze_dedup_impact(excel_file):
    """åˆ†æå»é‡å¯¹å•†å“åŒ¹é…çš„å½±å“"""
    
    print(f"ğŸ“Š åˆ†æå»é‡å½±å“: {Path(excel_file).name}")
    print("="*80)
    
    # è¯»å–æ¨¡ç³ŠåŒ¹é…Sheet
    df_fuzzy = pd.read_excel(excel_file, sheet_name='2-åç§°æ¨¡ç³ŠåŒ¹é…(æ— æ¡ç )')
    
    # è¯†åˆ«åˆ—å
    a_name_col = [col for col in df_fuzzy.columns if 'å•†å“åç§°' in col and col.endswith('_é«˜æ¸¯åº—')][0]
    b_name_col = [col for col in df_fuzzy.columns if 'å•†å“åç§°' in col and col.endswith('_å¥½æƒ æ¥åº—')][0]
    score_col = 'composite_similarity_score' if 'composite_similarity_score' in df_fuzzy.columns else 'text_similarity'
    
    print(f"\nğŸ“Œ åˆ†æç»´åº¦:")
    print(f"   æœ¬åº—åˆ—: {a_name_col}")
    print(f"   ç«å¯¹åˆ—: {b_name_col}")
    print(f"   å¾—åˆ†åˆ—: {score_col}")
    
    # ç»Ÿè®¡ç«å¯¹ä¾§çš„åŒ¹é…æƒ…å†µ
    b_match_counts = df_fuzzy[b_name_col].value_counts()
    duplicated_b = b_match_counts[b_match_counts > 1]
    
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ ã€ç«å¯¹ä¾§åŒ¹é…ç»Ÿè®¡ã€‘")
    print(f"{'='*80}")
    print(f"æ€»åŒ¹é…è®°å½•æ•°: {len(df_fuzzy)}")
    print(f"å”¯ä¸€ç«å¯¹å•†å“: {len(b_match_counts)}")
    print(f"é‡å¤çš„ç«å¯¹å•†å“: {len(duplicated_b)} ä¸ª")
    print(f"é‡å¤åŒ¹é…æ€»æ•°: {duplicated_b.sum()} æ¡")
    print(f"å»é‡åå°†ä¿ç•™: {len(duplicated_b)} æ¡ï¼ˆæ¯ä¸ªç«å¯¹å•†å“1æ¡ï¼‰")
    print(f"å»é‡åå°†åˆ é™¤: {duplicated_b.sum() - len(duplicated_b)} æ¡")
    
    # åˆ†æè¢«åˆ é™¤çš„æœ¬åº—å•†å“çš„å»å‘
    print(f"\n{'='*80}")
    print(f"ğŸ” ã€è¢«åˆ é™¤åŒ¹é…çš„æœ¬åº—å•†å“å»å‘åˆ†æã€‘")
    print(f"{'='*80}")
    
    # æ‰¾å‡ºä¼šè¢«åˆ é™¤çš„è®°å½•
    deleted_records = []
    for b_name in duplicated_b.index:
        b_matches = df_fuzzy[df_fuzzy[b_name_col] == b_name].copy()
        b_matches = b_matches.sort_values(score_col, ascending=False)
        
        # ç¬¬ä¸€æ¡ä¿ç•™ï¼Œå…¶ä½™åˆ é™¤
        kept_record = b_matches.iloc[0]
        deleted = b_matches.iloc[1:]
        
        for idx, row in deleted.iterrows():
            deleted_records.append({
                'è¢«åˆ é™¤çš„æœ¬åº—å•†å“': row[a_name_col],
                'åŸåŒ¹é…çš„ç«å¯¹å•†å“': b_name,
                'å¾—åˆ†': row[score_col],
                'æ’å': list(b_matches.index).index(idx) + 1,
                'æ€»ç«äº‰è€…': len(b_matches)
            })
    
    deleted_df = pd.DataFrame(deleted_records)
    
    print(f"âœ… è¢«åˆ é™¤çš„åŒ¹é…æ€»æ•°: {len(deleted_df)} æ¡")
    print(f"\nè¿™äº›æœ¬åº—å•†å“çš„å¯èƒ½å»å‘:")
    print(f"   1ï¸âƒ£ è¿›å…¥è½¯åŒ¹é…é˜¶æ®µ â†’ å¯èƒ½åŒ¹é…åˆ°å…¶ä»–ç«å¯¹å•†å“")
    print(f"   2ï¸âƒ£ è¿›å…¥å·®å¼‚å“åŒ¹é… â†’ è·¨åˆ†ç±»åŒ¹é…åˆ°ç›¸ä¼¼å•†å“")
    print(f"   3ï¸âƒ£ æˆä¸ºç‹¬æœ‰å•†å“ â†’ å¦‚æœæ‰¾ä¸åˆ°ä»»ä½•åŒ¹é…")
    
    # è¯»å–ç‹¬æœ‰å•†å“Sheetï¼ŒæŸ¥çœ‹æœ‰å¤šå°‘"è¢«åˆ é™¤"çš„å•†å“æˆä¸ºäº†ç‹¬æœ‰å•†å“
    try:
        df_unique_a = pd.read_excel(excel_file, sheet_name='4-é«˜æ¸¯åº—-ç‹¬æœ‰å•†å“(å…¨éƒ¨)')
        
        # æ£€æŸ¥æœ‰å¤šå°‘è¢«åˆ é™¤çš„å•†å“å‡ºç°åœ¨ç‹¬æœ‰å•†å“ä¸­
        deleted_in_unique = deleted_df[deleted_df['è¢«åˆ é™¤çš„æœ¬åº—å•†å“'].isin(df_unique_a['å•†å“åç§°'])]
        
        print(f"\nğŸ“Š å®é™…å»å‘ç»Ÿè®¡:")
        print(f"   âŒ æˆä¸ºç‹¬æœ‰å•†å“: {len(deleted_in_unique)} ä¸ª ({len(deleted_in_unique)/len(deleted_df)*100:.1f}%)")
        print(f"   âœ… æ‰¾åˆ°å…¶ä»–åŒ¹é…: {len(deleted_df) - len(deleted_in_unique)} ä¸ª ({(len(deleted_df)-len(deleted_in_unique))/len(deleted_df)*100:.1f}%)")
        
        if len(deleted_in_unique) > 0:
            print(f"\nâš ï¸ ã€æˆä¸ºç‹¬æœ‰å•†å“çš„è¢«åˆ é™¤åŒ¹é…ã€‘ï¼ˆå‰10ä¸ªï¼‰:")
            for i, row in deleted_in_unique.head(10).iterrows():
                print(f"   {i+1}. {row['è¢«åˆ é™¤çš„æœ¬åº—å•†å“'][:70]}")
                print(f"      åŸåŒ¹é…: {row['åŸåŒ¹é…çš„ç«å¯¹å•†å“'][:70]}")
                print(f"      å¾—åˆ†: {row['å¾—åˆ†']:.3f} (æ’å {row['æ’å']}/{row['æ€»ç«äº‰è€…']})")
                print()
    
    except Exception as e:
        print(f"\nâš ï¸ æ— æ³•è¯»å–ç‹¬æœ‰å•†å“Sheet: {e}")
    
    # å±•ç¤ºå…¸å‹æ¡ˆä¾‹
    print(f"\n{'='*80}")
    print(f"ğŸ“‹ ã€å…¸å‹æ¡ˆä¾‹ï¼šå¤šå¯¹ä¸€åŒ¹é…ã€‘ï¼ˆå‰5ä¸ªï¼‰")
    print(f"{'='*80}")
    
    for i, (b_name, count) in enumerate(duplicated_b.head(5).items(), 1):
        b_matches = df_fuzzy[df_fuzzy[b_name_col] == b_name].copy()
        b_matches = b_matches.sort_values(score_col, ascending=False)
        
        print(f"\n{i}. ç«å¯¹å•†å“: {b_name[:80]}")
        print(f"   åŒ¹é…åˆ° {count} ä¸ªæœ¬åº—å•†å“:")
        
        for idx, row in b_matches.iterrows():
            status = "âœ… ä¿ç•™" if idx == b_matches.index[0] else "âŒ åˆ é™¤"
            print(f"      {status} æœ¬åº—: {row[a_name_col][:70]}")
            print(f"           å¾—åˆ†: {row[score_col]:.3f}")
    
    # ç”Ÿæˆå»å‘åˆ†ææŠ¥å‘Š
    output_file = str(Path(excel_file).parent / f"{Path(excel_file).stem}_å»å‘åˆ†æ.xlsx")
    
    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:
        # Sheet 1: è¢«åˆ é™¤çš„åŒ¹é…
        deleted_df.to_excel(writer, sheet_name='è¢«åˆ é™¤çš„åŒ¹é…', index=False)
        
        # Sheet 2: é‡å¤åŒ¹é…è¯¦æƒ…
        dup_details = []
        for b_name in duplicated_b.index:
            b_matches = df_fuzzy[df_fuzzy[b_name_col] == b_name].copy()
            b_matches = b_matches.sort_values(score_col, ascending=False)
            
            for idx, row in b_matches.iterrows():
                dup_details.append({
                    'ç«å¯¹å•†å“': b_name,
                    'æœ¬åº—å•†å“': row[a_name_col],
                    'å¾—åˆ†': row[score_col],
                    'æ˜¯å¦ä¿ç•™': 'ä¿ç•™' if idx == b_matches.index[0] else 'åˆ é™¤',
                    'åŒ¹é…æ’å': list(b_matches.index).index(idx) + 1,
                    'æ€»åŒ¹é…æ•°': len(b_matches)
                })
        
        pd.DataFrame(dup_details).to_excel(writer, sheet_name='é‡å¤åŒ¹é…è¯¦æƒ…', index=False)
    
    print(f"\n{'='*80}")
    print(f"âœ… å»å‘åˆ†ææŠ¥å‘Šå·²ä¿å­˜: {output_file}")
    print(f"{'='*80}")
    
    return deleted_df

def main():
    """ä¸»å‡½æ•°"""
    reports_dir = Path('reports')
    
    # æŸ¥æ‰¾æœ€æ–°çš„æ¯”ä»·æŠ¥å‘Š
    excel_files = [f for f in reports_dir.glob('matched_products_comparison_final_*.xlsx') 
                   if 'è¯Šæ–­' not in f.name and 'å»å‘' not in f.name]
    
    if not excel_files:
        print("âŒ æ‰¾ä¸åˆ°æ¯”ä»·æŠ¥å‘Š")
        return
    
    latest_file = max(excel_files, key=lambda x: x.stat().st_mtime)
    
    print(f"ğŸ¯ åˆ†ææœ€æ–°æŠ¥å‘Š: {latest_file.name}")
    print(f"ğŸ“… ä¿®æ”¹æ—¶é—´: {pd.Timestamp.fromtimestamp(latest_file.stat().st_mtime)}")
    print()
    
    analyze_dedup_impact(str(latest_file))

if __name__ == '__main__':
    main()
