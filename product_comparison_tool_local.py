# -*- coding: utf-8 -*-
r"""
å•†å“æ¯”å¯¹åˆ†æå·¥å…· v8.5ï¼ˆæœ¬åœ°æ‰§è¡Œç‰ˆï¼‰
python product_comparison_tool_local.py
.\.venv\Scripts\Activate.ps1
.\cpu_mode.ps1
.\cpu_mode.ps1
# é€‰æ‹©æ¨¡å‹ 5

# æ–¹æ³•1ï¼šç›´æ¥è¿è¡Œï¼ˆæ¨èï¼‰
cd "d:\Python1\O2O_Analysis\O2Oæ•°æ®åˆ†æ\æ¯”ä»·æ•°æ®"
$env:CUDA_VISIBLE_DEVICES=''; $env:USE_TORCH_SIM='0'
& "D:\åŠå…¬\Python\python.exe" product_comparison_tool_local.py
åŠŸèƒ½:
    - è‡ªåŠ¨å¯¹æ¯”ä¸¤ä¸ªåº—é“ºï¼ˆå¦‚ç¾å›¢ã€é¥¿äº†ä¹ˆï¼‰çš„å•†å“æ•°æ®ã€‚
    - æ”¯æŒâ€œæ¡ç ç²¾ç¡®åŒ¹é…â€å’Œâ€œå•†å“åç§°æ¨¡ç³ŠåŒ¹é…â€ä¸¤ç§æ¨¡å¼ã€‚
    - è¾“å‡ºè¯¦ç»†çš„ Excel æŠ¥å‘Šï¼ŒåŒ…å«åŒ¹é…ç»“æœã€ç‹¬æœ‰å•†å“ç­‰ã€‚

ä½¿ç”¨æ–¹æ³•:
    1) å°†æ­¤è„šæœ¬ä¸ä¸¤ä¸ª Excel æ–‡ä»¶æ”¾åŒä¸€æ–‡ä»¶å¤¹ã€‚
    2) åœ¨ä¸‹æ–¹ Config ä¸­é…ç½®åº—é“ºåä¸æ–‡ä»¶åã€‚
    3) è¿è¡Œè„šæœ¬åï¼Œç»“æœè‡ªåŠ¨å†™å…¥ reports/ ç›®å½•ï¼ˆå¸¦æ—¶é—´æˆ³ï¼‰ã€‚

ä¼˜åŒ–è®°å½•:
    - v8.5: å¼•å…¥è§„æ ¼ç›¸ä¼¼åº¦ç»´åº¦ï¼Œè¿›ä¸€æ­¥æå‡åŒ¹é…å‡†ç¡®ç‡ã€‚
    - v8.4: è°ƒæ•´ç»¼åˆè¯„åˆ†æƒé‡å¹¶å¼•å…¥æŸ”æ€§åˆ†ç±»ç›¸ä¼¼åº¦ã€‚
    - v8.3: æœ¬åœ°æ‰§è¡Œç‰ˆï¼Œç§»é™¤ Colab ä¾èµ–ã€‚
"""

# ==============================================================================
# 1. è‡ªåŠ¨ä¾èµ–æ£€æŸ¥ä¸å¯¼å…¥åº“
# ==============================================================================
import sys
import io

# ğŸ”§ ä¿®å¤æ‰“åŒ…å Windows emoji ç¼–ç é—®é¢˜ï¼ˆå¿…é¡»åœ¨æœ€å¼€å¤´ï¼‰
if sys.stdout is None or (hasattr(sys.stdout, 'encoding') and sys.stdout.encoding != 'utf-8'):
    if sys.stdout is not None and hasattr(sys.stdout, 'buffer'):
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    if sys.stderr is not None and hasattr(sys.stderr, 'buffer'):
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

import pandas as pd
import numpy as np
import re
import jieba
import os
import logging
import torch
import time
import ssl
import urllib3
from pathlib import Path

# ==============================================================================
# æ‰“åŒ…ç¯å¢ƒæ£€æµ‹ï¼šå¿…é¡»åœ¨å¯¼å…¥ SentenceTransformer ä¹‹å‰è®¾ç½®ç¯å¢ƒå˜é‡ï¼
# ==============================================================================
BUNDLED_MODEL_CACHE = None  # å…¨å±€å˜é‡ï¼Œå­˜å‚¨æ‰“åŒ…çš„æ¨¡å‹ç¼“å­˜è·¯å¾„

# ============================================================================
# æˆæƒå¯†é’¥é…ç½®ï¼ˆæ—¶é—´å¯†é’¥ç®—æ³• - æ— éœ€ç»´æŠ¤JSONæ–‡ä»¶ï¼‰
# ============================================================================
import hashlib
from datetime import datetime, timedelta
import platform
import subprocess
import uuid
from typing import Optional, Tuple

# ğŸ” ä¸»å¯†é’¥ç›å€¼ï¼ˆä¸ç”Ÿæˆå™¨ä¿æŒä¸€è‡´ï¼‰
MASTER_SALT = "O2O_COMPARISON_TOOL_2025_SECRET_SALT_V1"


def _fingerprint_cache_paths() -> Tuple[str, ...]:
    """è¿”å›æŒ‡çº¹ç¼“å­˜æ–‡ä»¶å€™é€‰è·¯å¾„ï¼ˆæŒ‰ä¼˜å…ˆçº§æ’åˆ—ï¼‰ã€‚"""
    paths: list[str] = []

    env_path = os.environ.get('O2O_FINGERPRINT_CACHE')
    if env_path:
        paths.append(env_path)

    if getattr(sys, 'frozen', False):
        base_dir = os.path.dirname(sys.executable)
    else:
        base_dir = os.path.dirname(os.path.abspath(__file__))
    paths.append(os.path.join(base_dir, '.fingerprint_cache'))

    appdata_dir = os.environ.get('APPDATA') or os.path.expanduser('~')
    if appdata_dir:
        paths.append(os.path.join(appdata_dir, 'O2OComparison', '.fingerprint_cache'))

    unique_paths: list[str] = []
    for candidate in paths:
        if candidate and candidate not in unique_paths:
            unique_paths.append(candidate)
    return tuple(unique_paths)


def _load_cached_fingerprint() -> Tuple[Optional[str], Optional[str]]:
    for cache_path in _fingerprint_cache_paths():
        try:
            if os.path.exists(cache_path):
                with open(cache_path, 'r', encoding='utf-8') as fp:
                    cached_fp = fp.read().strip()
                if cached_fp and len(cached_fp) == 16:
                    logging.debug("æŒ‡çº¹ç¼“å­˜å‘½ä¸­: %s", cache_path)
                    return cached_fp, cache_path
        except Exception as exc:
            logging.debug("è¯»å–æŒ‡çº¹ç¼“å­˜å¤±è´¥ %s: %s", cache_path, exc)
    return None, None


def _save_fingerprint_cache(fingerprint: str) -> Optional[str]:
    for cache_path in _fingerprint_cache_paths():
        try:
            directory = os.path.dirname(cache_path)
            if directory and not os.path.exists(directory):
                os.makedirs(directory, exist_ok=True)
            with open(cache_path, 'w', encoding='utf-8') as fp:
                fp.write(fingerprint)
            logging.debug("æŒ‡çº¹ç¼“å­˜å†™å…¥æˆåŠŸ: %s", cache_path)
            return cache_path
        except Exception as exc:
            logging.debug("å†™å…¥æŒ‡çº¹ç¼“å­˜å¤±è´¥ %s: %s", cache_path, exc)
            continue
    return None


def get_machine_fingerprint():
    """
    è·å–å½“å‰æœºå™¨çš„ç¡¬ä»¶æŒ‡çº¹ï¼ˆç”¨äºç»‘å®šå¯†é’¥ï¼‰
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. ä¼˜å…ˆä»ç¼“å­˜è¯»å–ï¼ˆ.fingerprint_cacheï¼‰
    2. ä½¿ç”¨å¿«é€Ÿæ–¹æ³•ï¼ˆMAC + æœºå™¨å + ç”¨æˆ·åï¼‰
    3. ç¼“å­˜ç»“æœé¿å…é‡å¤è®¡ç®—
    """
    cached_fp, _ = _load_cached_fingerprint()
    if cached_fp:
        return cached_fp
    
    # 2. ç”Ÿæˆæ–°æŒ‡çº¹ï¼ˆä½¿ç”¨å¿«é€Ÿæ–¹æ³•ï¼‰
    components = []
    
    try:
        # MAC åœ°å€ï¼ˆæœ€ç¨³å®šï¼Œé€Ÿåº¦å¿«ï¼‰
        try:
            mac = ':'.join(['{:02x}'.format((uuid.getnode() >> elements) & 0xff) 
                           for elements in range(0,2*6,2)][::-1])
            components.append(mac)
        except: pass
        
        # æœºå™¨å
        try:
            components.append(platform.node())
        except: pass
        
        # ç”¨æˆ·å
        try:
            components.append(os.getlogin())
        except:
            try:
                components.append(os.environ.get('USERNAME', ''))
            except: pass
        
        # Windowsç³»ç»Ÿï¼šä»…è·å–CPU IDï¼ˆå°½é‡å¿«é€Ÿï¼Œå¦‚å¤±è´¥ç«‹å³è·³è¿‡ï¼‰
        if platform.system() == 'Windows':
            try:
                start = time.perf_counter()
                cpu_id_raw = subprocess.check_output(
                    'wmic cpu get ProcessorId',
                    shell=True,
                    timeout=2,
                    creationflags=0x08000000  # CREATE_NO_WINDOWï¼Œé¿å…é—ªçª—
                )
                cpu_id = cpu_id_raw.decode(errors='ignore').split('\n')[1].strip()
                if cpu_id:
                    components.append(cpu_id)
                elapsed = time.perf_counter() - start
                if elapsed > 1:  # è®°å½•å¶å‘çš„æ…¢è°ƒç”¨ï¼Œä¾¿äºåç»­æ’æŸ¥
                    logging.debug("è·å–CPUæŒ‡çº¹è€—æ—¶ %.2fs", elapsed)
            except Exception as exc:
                logging.debug("è·å–CPUæŒ‡çº¹å¤±è´¥: %s", exc)
                pass  # è¶…æ—¶æˆ–å¤±è´¥åˆ™è·³è¿‡
        
        if not components:
            # å®Œå…¨å¤±è´¥ï¼Œä½¿ç”¨UUIDä½œä¸ºå…œåº•
            components.append(str(uuid.getnode()))
        
        fingerprint_str = '|'.join(components)
        fingerprint = hashlib.sha256(fingerprint_str.encode()).hexdigest()[:16]
        
        # 3. ç¼“å­˜ç»“æœï¼ˆå¤šè·¯å¾„å…œåº•ï¼‰
        _save_fingerprint_cache(fingerprint)
        
        return fingerprint
    except:
        return None

def verify_license_key_simple(license_key: str) -> tuple[bool, str]:
    """
    éªŒè¯çº¯æ—¶é—´å¯†é’¥ï¼ˆç®€åŒ–ç‰ˆï¼Œæ— ç¡¬ä»¶ç»‘å®šï¼‰
    
    Args:
        license_key: ç”¨æˆ·è¾“å…¥çš„å¯†é’¥ï¼ˆ12ä½ï¼‰
    
    Returns:
        (æ˜¯å¦æœ‰æ•ˆ, åˆ°æœŸæ—¥æœŸå­—ç¬¦ä¸²/é”™è¯¯ä¿¡æ¯)
    """
    # å‘å‰æ£€æŸ¥æœªæ¥1å¹´å†…çš„æ‰€æœ‰å¯èƒ½æ—¥æœŸ
    today = datetime.now()
    
    for days_offset in range(0, 366):
        check_date = today + timedelta(days=days_offset)
        expire_str = check_date.strftime("%Y%m%d")
        
        # ç®€åŒ–ç‰ˆï¼šä¸ä½¿ç”¨ç¡¬ä»¶æŒ‡çº¹ï¼Œåªç”¨æ—¥æœŸ
        raw_data = f"{expire_str}-{MASTER_SALT}"
        hash_obj = hashlib.sha256(raw_data.encode('utf-8'))
        expected_key = hash_obj.hexdigest()[:12].upper()
        
        if expected_key == license_key.upper():
            # æ‰¾åˆ°åŒ¹é…çš„å¯†é’¥ï¼Œæ£€æŸ¥æ˜¯å¦è¿‡æœŸ
            expire_date = datetime.strptime(expire_str, "%Y%m%d")
            if today <= expire_date:
                return True, expire_str
            else:
                return False, f"å¯†é’¥å·²è¿‡æœŸï¼ˆè¿‡æœŸæ—¥æœŸï¼š{expire_str}ï¼‰"
    
    # å‘åæ£€æŸ¥è¿‡å»30å¤©ï¼ˆå…è®¸å°å¹…æ—¶é’Ÿè¯¯å·®ï¼‰
    for days_offset in range(-30, 0):
        check_date = today + timedelta(days=days_offset)
        expire_str = check_date.strftime("%Y%m%d")
        
        raw_data = f"{expire_str}-{MASTER_SALT}"
        hash_obj = hashlib.sha256(raw_data.encode('utf-8'))
        expected_key = hash_obj.hexdigest()[:12].upper()
        
        if expected_key == license_key.upper():
            expire_date = datetime.strptime(expire_str, "%Y%m%d")
            if today <= expire_date:
                return True, expire_str
            else:
                return False, f"å¯†é’¥å·²è¿‡æœŸï¼ˆè¿‡æœŸæ—¥æœŸï¼š{expire_str}ï¼‰"
    
    return False, "å¯†é’¥æ— æ•ˆæˆ–æ ¼å¼é”™è¯¯"

# ç®¡ç†å‘˜ä¸»å¯†é’¥ï¼ˆå¯ä»¥è·³è¿‡ç¡¬ä»¶éªŒè¯ï¼‰
MASTER_KEY = "O2O_ADMIN_2025"

# ç¨‹åºè¿‡æœŸæ—¶é—´ï¼ˆé¢å¤–çš„å®‰å…¨æªæ–½ï¼Œé˜²æ­¢æ—§ç‰ˆæœ¬æµä¼ ï¼‰
PROGRAM_EXPIRE_DATE = "20260630"  # 2026å¹´6æœˆ30æ—¥è¿‡æœŸï¼ˆåŠå¹´åï¼‰

def check_program_expiration():
    """
    æ£€æŸ¥ç¨‹åºæ˜¯å¦è¿‡æœŸï¼ˆæ—¶é—´ç‚¸å¼¹ï¼Œé˜²æ­¢æ—§ç‰ˆæœ¬æµä¼ ï¼‰
    
    æ³¨æ„ï¼šæ­¤æ£€æŸ¥åœ¨æˆæƒé€šè¿‡åæ‰§è¡Œï¼Œé¿å…å½±å“å¯åŠ¨é€Ÿåº¦
    """
    try:
        expire_date = datetime.strptime(PROGRAM_EXPIRE_DATE, "%Y%m%d")
        if datetime.now() > expire_date:
            error_msg = (
                f"ç¨‹åºç‰ˆæœ¬å·²è¿‡æœŸ\n\n"
                f"æ­¤ç‰ˆæœ¬å·²äº {expire_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} è¿‡æœŸã€‚\n"
                f"è¯·è”ç³»ç®¡ç†å‘˜è·å–æœ€æ–°ç‰ˆæœ¬ã€‚\n\n"
                f"è¯´æ˜ï¼šä¸ºä¿è¯åŠŸèƒ½ç¨³å®šæ€§ï¼Œç¨‹åºæ¯åŠå¹´æ›´æ–°ä¸€æ¬¡ã€‚"
            )
            
            # GUIæ¨¡å¼ä½¿ç”¨messagebox
            if os.environ.get('GUI_MODE') == '1':
                try:
                    import tkinter as tk
                    from tkinter import messagebox
                    root = tk.Tk()
                    root.withdraw()
                    messagebox.showerror("ç¨‹åºå·²è¿‡æœŸ", error_msg)
                    root.destroy()
                except:
                    print(error_msg)
                    input("æŒ‰å›è½¦é”®é€€å‡º...")
            else:
                print("\n" + "="*60)
                print("  ç¨‹åºç‰ˆæœ¬å·²è¿‡æœŸ")
                print("="*60)
                print()
                print(f"  æ­¤ç‰ˆæœ¬å·²äº {expire_date.strftime('%Yå¹´%mæœˆ%dæ—¥')} è¿‡æœŸã€‚")
                print("  è¯·è”ç³»ç®¡ç†å‘˜è·å–æœ€æ–°ç‰ˆæœ¬ã€‚")
                print()
                print("  è¯´æ˜ï¼šä¸ºä¿è¯åŠŸèƒ½ç¨³å®šæ€§ï¼Œç¨‹åºæ¯åŠå¹´æ›´æ–°ä¸€æ¬¡ã€‚")
                print("="*60)
                print()
                input("æŒ‰å›è½¦é”®é€€å‡º...")
            return False
    except Exception as e:
        print(f"âš ï¸  æ—¶é—´æ£€æŸ¥å¼‚å¸¸: {e}")
    
    return True

def check_authorization():
    """
    æ£€æŸ¥æˆæƒå¯†é’¥ï¼ˆç®€åŒ–ç‰ˆ - çº¯æ—¶é—´å¯†é’¥ï¼Œæ— ç¡¬ä»¶ç»‘å®šï¼‰
    
    ä¼˜åŒ–ï¼š
    1. å»¶è¿Ÿ tkinter åˆå§‹åŒ–ï¼ˆä»…åœ¨çœŸæ­£éœ€è¦æ—¶åˆ›å»ºï¼‰
    2. ç§»é™¤ç¡¬ä»¶æŒ‡çº¹ä¾èµ–ï¼Œç®€åŒ–æµç¨‹
    3. å¿«é€Ÿå¤±è´¥ï¼ˆè¿‡æœŸæ£€æŸ¥åç½®ï¼‰
    """
    # å¼€å‘ç¯å¢ƒä¸æ£€æŸ¥æˆæƒ
    if not getattr(sys, 'frozen', False):
        return True
    
    # GUIæ¨¡å¼ï¼šä½¿ç”¨è‡ªå®šä¹‰è¾“å…¥å¯¹è¯æ¡†
    if os.environ.get('GUI_MODE') == '1':
        try:
            import tkinter as tk
            from tkinter import messagebox
            
            def ask_key_input(title, prompt):
                """è‡ªå®šä¹‰å¯†é’¥è¾“å…¥å¯¹è¯æ¡†"""
                dialog = tk.Tk()
                dialog.title(title)
                dialog.geometry("380x160")
                dialog.resizable(False, False)
                
                # å¼ºåˆ¶ç½®é¡¶å¹¶è·å–ç„¦ç‚¹
                dialog.attributes('-topmost', True)
                dialog.lift()
                dialog.focus_force()
                
                # å±…ä¸­æ˜¾ç¤º
                dialog.update_idletasks()
                x = (dialog.winfo_screenwidth() // 2) - (380 // 2)
                y = (dialog.winfo_screenheight() // 2) - (160 // 2)
                dialog.geometry(f"380x160+{x}+{y}")
                
                # æç¤ºæ ‡ç­¾
                label = tk.Label(dialog, text=prompt, font=("Arial", 10), justify='left', wraplength=350)
                label.pack(pady=15)
                
                # è¾“å…¥æ¡†
                entry = tk.Entry(dialog, width=35, font=("Arial", 11), show="*")
                entry.pack(pady=5)
                entry.focus_set()
                
                result = {'key': None}
                
                def on_ok():
                    result['key'] = entry.get()
                    dialog.destroy()
                
                def on_cancel():
                    result['key'] = None
                    dialog.destroy()
                
                # æŒ‰é’®æ¡†æ¶
                button_frame = tk.Frame(dialog)
                button_frame.pack(pady=15)
                
                ok_btn = tk.Button(button_frame, text="ç¡®å®š", width=10, command=on_ok)
                ok_btn.pack(side=tk.LEFT, padx=10)
                
                cancel_btn = tk.Button(button_frame, text="å–æ¶ˆ", width=10, command=on_cancel)
                cancel_btn.pack(side=tk.LEFT, padx=10)
                
                # å›è½¦ç¡®è®¤
                entry.bind('<Return>', lambda e: on_ok())
                
                # å¼ºåˆ¶çª—å£æ¿€æ´»
                dialog.after(100, lambda: dialog.focus_force())
                
                dialog.wait_window()
                return result['key']
            
            def show_message(msg_type, title, message):
                """æ˜¾ç¤ºæ¶ˆæ¯æ¡†"""
                temp_root = tk.Tk()
                temp_root.withdraw()
                temp_root.attributes('-topmost', True)
                
                if msg_type == 'info':
                    messagebox.showinfo(title, message, parent=temp_root)
                elif msg_type == 'warning':
                    messagebox.showwarning(title, message, parent=temp_root)
                elif msg_type == 'error':
                    messagebox.showerror(title, message, parent=temp_root)
                
                temp_root.destroy()
            
            try:
                # æœ€å¤š3æ¬¡è¾“å…¥æœºä¼š
                for attempt in range(3):
                    prompt = (
                        "O2O æ¯”ä»·å·¥å…· - æˆæƒéªŒè¯\n\n"
                        f"è¯·è¾“å…¥æˆæƒå¯†é’¥ ({attempt + 1}/3):"
                    )
                    user_key = ask_key_input("æˆæƒéªŒè¯", prompt)
                    
                    if user_key is None:  # ç”¨æˆ·ç‚¹å‡»å–æ¶ˆ
                        show_message('warning', "æˆæƒå–æ¶ˆ", "ç”¨æˆ·å–æ¶ˆæˆæƒï¼Œç¨‹åºå°†é€€å‡º")
                        return False
                    
                    user_key = user_key.strip()
                    if not user_key:
                        show_message('warning', "è¾“å…¥é”™è¯¯", "å¯†é’¥ä¸èƒ½ä¸ºç©º")
                        continue
                    
                    # æ£€æŸ¥ä¸»å¯†é’¥
                    if user_key == MASTER_KEY:
                        show_message('info', "æˆæƒæˆåŠŸ", "ä¸»å¯†é’¥éªŒè¯é€šè¿‡ï¼ˆæ°¸ä¹…æœ‰æ•ˆï¼‰")
                        
                        # æˆæƒé€šè¿‡åæ£€æŸ¥ç¨‹åºè¿‡æœŸæ—¶é—´
                        if not check_program_expiration():
                            return False
                        
                        return True
                    
                    # ä½¿ç”¨ç®€åŒ–ç‰ˆæ—¶é—´å¯†é’¥éªŒè¯
                    valid, result_msg = verify_license_key_simple(user_key)
                    
                    if valid:
                        # éªŒè¯é€šè¿‡
                        expire_date = datetime.strptime(result_msg, "%Y%m%d")
                        days_left = (expire_date - datetime.now()).days
                        success_msg = (
                            f"å¯†é’¥éªŒè¯é€šè¿‡\n\n"
                            f"æœ‰æ•ˆæœŸè‡³: {expire_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}\n"
                            f"å‰©ä½™å¤©æ•°: {days_left} å¤©"
                        )
                        show_message('info', "æˆæƒæˆåŠŸ", success_msg)
                        
                        # æˆæƒé€šè¿‡åæ£€æŸ¥ç¨‹åºè¿‡æœŸæ—¶é—´
                        if not check_program_expiration():
                            return False
                        
                        return True
                    else:
                        # éªŒè¯å¤±è´¥
                        remaining = 2 - attempt
                        if remaining > 0:
                            show_message('warning', "å¯†é’¥éªŒè¯å¤±è´¥", f"{result_msg}\n\nè¿˜æœ‰ {remaining} æ¬¡æœºä¼š")
                        else:
                            show_message('error', "å¯†é’¥éªŒè¯å¤±è´¥", result_msg)
                
                # 3æ¬¡å…¨éƒ¨å¤±è´¥
                fail_msg = (
                    "æˆæƒå¤±è´¥\n\n"
                    "å¯†é’¥éªŒè¯å¤±è´¥åŸå› å¯èƒ½æ˜¯ï¼š\n"
                    "  1. å¯†é’¥è¾“å…¥é”™è¯¯\n"
                    "  2. å¯†é’¥å·²è¿‡æœŸ\n\n"
                    "è¯·è”ç³»ç®¡ç†å‘˜è·å–æ­£ç¡®çš„æˆæƒå¯†é’¥"
                )
                show_message('error', "æˆæƒå¤±è´¥", fail_msg)
                return False
            
            except Exception as e:
                # GUIå¤±è´¥å›é€€åˆ°å‘½ä»¤è¡Œæ¨¡å¼
                print(f"GUIæˆæƒå¯¹è¯æ¡†å¤±è´¥: {e}")
                print("å›é€€åˆ°å‘½ä»¤è¡Œæ¨¡å¼...")
                # ç»§ç»­æ‰§è¡Œä¸‹é¢çš„å‘½ä»¤è¡Œé€»è¾‘
        
        except Exception as e:
            # å¯¼å…¥tkinterå¤±è´¥ï¼Œå›é€€åˆ°å‘½ä»¤è¡Œæ¨¡å¼
            print(f"GUIæ¨¡å—åŠ è½½å¤±è´¥: {e}")
            print("å›é€€åˆ°å‘½ä»¤è¡Œæ¨¡å¼...")
    
    # å‘½ä»¤è¡Œæ¨¡å¼ï¼šä½¿ç”¨printå’Œinput
    print("\n" + "="*60)
    print("  ğŸ” O2O æ¯”ä»·å·¥å…· - æˆæƒéªŒè¯")
    print("="*60)
    print()

    # æœ€å¤šå…è®¸3æ¬¡è¾“å…¥æœºä¼š
    for attempt in range(3):
        try:
            user_key = input("è¯·è¾“å…¥æˆæƒå¯†é’¥: ").strip()
        except:
            # GUIæ¨¡å¼ä¸‹input()å¯èƒ½å¤±è´¥
            print("âŒ æ— æ³•è·å–è¾“å…¥ï¼Œè¯·åœ¨å‘½ä»¤è¡Œæ¨¡å¼ä¸‹è¿è¡Œ")
            return False
        
        if not user_key:
            print("âŒ å¯†é’¥ä¸èƒ½ä¸ºç©º\n")
            continue
        
        # æ£€æŸ¥ä¸»å¯†é’¥ï¼ˆæ°¸ä¹…æœ‰æ•ˆï¼‰
        if user_key == MASTER_KEY:
            print("\nä¸»å¯†é’¥éªŒè¯é€šè¿‡ï¼ˆæ°¸ä¹…æœ‰æ•ˆï¼‰")
            print("="*60)
            
            # æˆæƒé€šè¿‡åæ£€æŸ¥ç¨‹åºè¿‡æœŸæ—¶é—´
            if not check_program_expiration():
                return False
            
            return True
        
        # ä½¿ç”¨ç®€åŒ–ç‰ˆæ—¶é—´å¯†é’¥éªŒè¯
        valid, result_msg = verify_license_key_simple(user_key)
        
        if valid:
            # éªŒè¯é€šè¿‡
            expire_date = datetime.strptime(result_msg, "%Y%m%d")
            days_left = (expire_date - datetime.now()).days
            print(f"\nâœ… å¯†é’¥éªŒè¯é€šè¿‡")
            print(f"   ğŸ“… æœ‰æ•ˆæœŸè‡³: {expire_date.strftime('%Yå¹´%mæœˆ%dæ—¥')}")
            print(f"   â° å‰©ä½™å¤©æ•°: {days_left} å¤©")
            print("="*60)
            
            # æˆæƒé€šè¿‡åæ£€æŸ¥ç¨‹åºè¿‡æœŸæ—¶é—´
            if not check_program_expiration():
                return False
            
            return True
        else:
            # éªŒè¯å¤±è´¥
            remaining = 2 - attempt
            if remaining > 0:
                print(f"âŒ å¯†é’¥éªŒè¯å¤±è´¥: {result_msg}")
                print(f"   è¿˜æœ‰ {remaining} æ¬¡æœºä¼š\n")
            else:
                print(f"âŒ å¯†é’¥éªŒè¯å¤±è´¥: {result_msg}\n")
    
    # 3æ¬¡è¾“å…¥å¤±è´¥
    print("="*60)
    print("  âŒ æˆæƒå¤±è´¥")
    print("="*60)
    print()
    print("  å¯†é’¥éªŒè¯å¤±è´¥åŸå› å¯èƒ½æ˜¯ï¼š")
    print("    1. å¯†é’¥è¾“å…¥é”™è¯¯")
    print("    2. å¯†é’¥å·²è¿‡æœŸ")
    print()
    print("  è¯·è”ç³»ç®¡ç†å‘˜è·å–æœ‰æ•ˆçš„æˆæƒå¯†é’¥ã€‚")
    print()
    print("="*60)
    print()
    input("æŒ‰å›è½¦é”®é€€å‡º...")
    return False

if getattr(sys, 'frozen', False):
    # æ‰“åŒ…åçš„ç¯å¢ƒ
    bundle_dir = Path(getattr(sys, '_MEIPASS', os.path.dirname(sys.executable)))
    bundled_model_cache = bundle_dir / '.cache' / 'huggingface'  # ğŸ”§ ä¿®å¤ï¼šæ­£ç¡®çš„ç¼“å­˜è·¯å¾„
    
    print(f"[INFO] Detected packaged environment")
    print(f"   _MEIPASS: {getattr(sys, '_MEIPASS', 'N/A')}")
    print(f"   bundle_dir: {bundle_dir}")
    print(f"   bundled_model_cache: {bundled_model_cache}")
    print(f"   exists: {bundled_model_cache.exists()}")
    
    if bundled_model_cache.exists():
        # ä¼˜å…ˆä½¿ç”¨æ‰“åŒ…çš„æ¨¡å‹
        hub_path = bundled_model_cache / 'hub'
        os.environ['HF_HOME'] = str(bundled_model_cache)
        os.environ['TRANSFORMERS_CACHE'] = str(hub_path)
        os.environ['SENTENCE_TRANSFORMERS_HOME'] = str(bundled_model_cache)
        BUNDLED_MODEL_CACHE = bundled_model_cache  # ä¿å­˜åˆ°å…¨å±€å˜é‡
        
        print(f"[OK] Using bundled model cache: {bundled_model_cache}")
        print(f"   HF_HOME: {os.environ.get('HF_HOME')}")
        print(f"   TRANSFORMERS_CACHE: {os.environ.get('TRANSFORMERS_CACHE')}")
        print(f"   SENTENCE_TRANSFORMERS_HOME: {os.environ.get('SENTENCE_TRANSFORMERS_HOME')}")
        print(f"[OFFLINE] No internet required for model loading")
        
        # åˆ—å‡ºæ‰¾åˆ°çš„æ¨¡å‹
        if hub_path.exists():
            models = list(hub_path.glob("models--*"))
            print(f"   Found {len(models)} models:")
            for model in models:
                print(f"     - {model.name}")
    else:
        print(f"[WARN] Bundled model cache not found")
        print(f"   Checked path: {bundled_model_cache}")
else:
    # å¼€å‘ç¯å¢ƒï¼Œä½¿ç”¨é»˜è®¤è·¯å¾„
    print(f"[DEV] Development mode (not packaged)")

# ç°åœ¨æ‰å¯¼å…¥ SentenceTransformerï¼Œæ­¤æ—¶ç¯å¢ƒå˜é‡å·²è®¾ç½®
from sentence_transformers import SentenceTransformer
try:
    from sentence_transformers import CrossEncoder
except ImportError:
    CrossEncoder = None
from typing import Iterable, Optional, Tuple, List
# ä½¿ç”¨æœ¬åœ°å®ç°çš„ä½™å¼¦ç›¸ä¼¼åº¦ä»¥é¿å…ä¾èµ– scikit-learnï¼ˆåœ¨ Py3.13 ä¸Šå¯èƒ½ç¼ºå°‘é¢„ç¼–è¯‘è½®å­ï¼‰
def cosine_similarity(a: np.ndarray, b: np.ndarray) -> np.ndarray:
    """è®¡ç®—ä¸¤ç»„å‘é‡çš„ä½™å¼¦ç›¸ä¼¼åº¦çŸ©é˜µã€‚
    a: (N, D), b: (M, D) -> è¿”å› (N, M)
    æ”¯æŒå¯é€‰ GPU åŠ é€Ÿï¼šè®¾ç½®ç¯å¢ƒå˜é‡ USE_TORCH_SIM=1 ä¸” CUDA å¯ç”¨æ—¶å¯ç”¨ã€‚
    """
    if a.size == 0 or b.size == 0:
        return np.zeros((a.shape[0], b.shape[0]))

    try:
        use_torch_sim = os.environ.get('USE_TORCH_SIM', '0') == '1' and torch.cuda.is_available()
    except Exception:
        use_torch_sim = False

    if use_torch_sim:
        try:
            with torch.no_grad():
                ta = torch.from_numpy(a).to('cuda', dtype=torch.float32)
                tb = torch.from_numpy(b).to('cuda', dtype=torch.float32)
                ta = torch.nn.functional.normalize(ta, p=2, dim=1)
                tb = torch.nn.functional.normalize(tb, p=2, dim=1)
                sim = ta @ tb.T
                return sim.cpu().numpy()
        except Exception as cuda_error:
            # CUDAé”™è¯¯å¤„ç†ï¼šæ‰“å°è­¦å‘Šå¹¶å›é€€åˆ°CPU
            print(f"âš ï¸ CUDAè®¡ç®—å¤±è´¥ï¼Œè‡ªåŠ¨åˆ‡æ¢åˆ°CPUæ¨¡å¼: {cuda_error}")
            # æ¸…ç†CUDAç¼“å­˜
            try:
                torch.cuda.empty_cache()
            except:
                pass
            # å¼ºåˆ¶ç¦ç”¨åç»­GPUä½¿ç”¨
            os.environ['USE_TORCH_SIM'] = '0'

    a_norm = np.linalg.norm(a, axis=1, keepdims=True)
    b_norm = np.linalg.norm(b, axis=1, keepdims=True)
    # é¿å…é™¤é›¶
    a_safe = a / (a_norm + 1e-12)
    b_safe = b / (b_norm + 1e-12)
    return a_safe @ b_safe.T
import warnings
import sys
import importlib
from tqdm.auto import tqdm
from tqdm.auto import tqdm as tqdm_auto
import unicodedata
import difflib
from decimal import Decimal
import hashlib
import joblib
from pathlib import Path
def _sanitize_sheet_name(name: str, existing: Optional[set] = None) -> str:
    r"""å°†å·¥ä½œè¡¨åæ¸…æ´—ä¸º Excel å¯æ¥å—çš„åç§°ï¼š
    - æ›¿æ¢éæ³•å­—ç¬¦ : \ / ? * [ ] ä¸ºä¸‹åˆ’çº¿
    - å»é¦–å°¾ç©ºç™½
    - æˆªæ–­è‡³ 31 ä¸ªå­—ç¬¦
    - ä¿è¯å”¯ä¸€ï¼šå¦‚å·²å­˜åœ¨ï¼Œåˆ™è¿½åŠ  _1/_2 ç­‰åç¼€
    """
    s = str(name or '').strip()
    s = re.sub(r'[:\\/\?\*\[\]]', '_', s)
    max_len = 31
    s = s[:max_len]
    if existing is not None:
        base = s
        i = 1
        while s in existing or not s:
            suffix = f"_{i}"
            s = (base[:max_len - len(suffix)] + suffix) if len(base) + len(suffix) > max_len else base + suffix
            i += 1
        existing.add(s)
    return s

# è§£å†³SSLè¯ä¹¦é—®é¢˜å’Œç½‘ç»œè¿æ¥é—®é¢˜
import requests
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

ssl._create_default_https_context = ssl._create_unverified_context
urllib3.disable_warnings(urllib3.exceptions.InsecureRequestWarning)

# é…ç½®æ›´å¥½çš„ç½‘ç»œé‡è¯•æœºåˆ¶
def setup_requests_session():
    session = requests.Session()
    retry_strategy = Retry(
        total=3,
        status_forcelist=[429, 500, 502, 503, 504],
        method_whitelist=["HEAD", "GET", "OPTIONS"]
    )
    adapter = HTTPAdapter(max_retries=retry_strategy)
    session.mount("http://", adapter)
    session.mount("https://", adapter)
    return session

REQUIRED_PACKAGES = [
    'pandas', 'numpy', 'jieba', 'torch', 'sentence_transformers', 'openpyxl', 'tqdm'
]

print("æ£€æŸ¥ä¾èµ–åº“...")
for pkg in REQUIRED_PACKAGES:
    try:
        if pkg == 'sklearn':
            importlib.import_module('sklearn.metrics')
        else:
            importlib.import_module(pkg)
        print(f"[OK] {pkg} - å·²å®‰è£…")
    except ImportError:
        print(f"[ERROR] ç¼ºå°‘ä¾èµ–åº“ï¼š{pkg}ï¼Œè¯·åœ¨ç»ˆç«¯è¿è¡Œï¼špip install {pkg}")
        sys.exit(1)
import joblib
import hashlib

warnings.filterwarnings('ignore')

# é…ç½®æ—¥å¿—ï¼šå¼ºåˆ¶ä½¿ç”¨ UTF-8 ç¼–ç é¿å… Emoji é”™è¯¯
import sys
import io

# å¼ºåˆ¶æ ‡å‡†è¾“å‡ºä½¿ç”¨ UTF-8 ç¼–ç 
# GUIæ¨¡å¼ä¸‹sys.stdout/stderrå¯èƒ½æ˜¯Noneï¼Œéœ€è¦å…ˆæ£€æŸ¥
if sys.stdout is not None and hasattr(sys.stdout, 'reconfigure'):
    sys.stdout.reconfigure(encoding='utf-8', errors='replace')
if sys.stderr is not None and hasattr(sys.stderr, 'reconfigure'):
    sys.stderr.reconfigure(encoding='utf-8', errors='replace')
elif sys.stdout is not None and hasattr(sys.stdout, 'buffer'):
    # Python 3.6 åŠä»¥ä¸‹ç‰ˆæœ¬çš„å…¼å®¹æ–¹æ¡ˆï¼ˆä»…å½“stdoutæœ‰æ•ˆæ—¶ï¼‰
    sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
    sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')

# é…ç½® logging ä½¿ç”¨ UTF-8
class UTF8StreamHandler(logging.StreamHandler):
    """å¼ºåˆ¶ä½¿ç”¨ UTF-8 ç¼–ç çš„æ—¥å¿—å¤„ç†å™¨"""
    def __init__(self, stream=None):
        # ç¡®ä¿ stream ä½¿ç”¨ UTF-8
        if stream is None:
            stream = sys.stderr
        super().__init__(stream)
        self.setFormatter(logging.Formatter('%(message)s'))
    
    def emit(self, record):
        try:
            msg = self.format(record)
            stream = self.stream
            # å¼ºåˆ¶å†™å…¥æ—¶ä½¿ç”¨ UTF-8
            stream.write(msg + self.terminator)
            stream.flush()
        except (UnicodeEncodeError, UnicodeDecodeError):
            # å¦‚æœè¿˜æ˜¯ç¼–ç å¤±è´¥ï¼Œç§»é™¤ emoji
            try:
                msg_clean = msg.encode('ascii', errors='ignore').decode('ascii')
                stream.write(msg_clean + self.terminator)
                stream.flush()
            except:
                pass  # å®Œå…¨å¤±è´¥åˆ™å¿½ç•¥

# é…ç½®æ ¹ logger
logger = logging.getLogger()
logger.setLevel(logging.INFO)
# ç§»é™¤æ‰€æœ‰ç°æœ‰çš„ handler
for handler in logger.handlers[:]:
    logger.removeHandler(handler)
# æ·»åŠ  UTF-8 handler
logger.addHandler(UTF8StreamHandler())

# Enable progress bars for pandas operations like .apply()
tqdm_auto.pandas()

# å…œåº•æ¨¡å¼ï¼ˆä»…å½“æ˜ç¡®å…è®¸æ—¶æ‰å¯ç”¨ï¼‰ï¼Œé»˜è®¤ç¦æ­¢ä»¥ä¿è¯ç²¾åº¦
SIMPLE_FALLBACK = os.environ.get('ALLOW_SIMPLE_FALLBACK', '0') == '1'

# ==============================================================================
# 2. ç¼“å­˜ç®¡ç†å™¨ï¼ˆæ€§èƒ½ä¼˜åŒ–æ ¸å¿ƒç»„ä»¶ï¼‰
# ==============================================================================
class CacheManager:
    """ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨ï¼Œæ”¯æŒå‘é‡ã€ç›¸ä¼¼åº¦çŸ©é˜µå’Œ Cross-Encoder ç»“æœç¼“å­˜"""
    
    def __init__(self, cache_dir: str = '.'):
        # ç¡®å®šç¼“å­˜ç›®å½•ï¼šæ‰“åŒ…ç¯å¢ƒä¼˜å…ˆä½¿ç”¨ prebuilt_cache
        if getattr(sys, 'frozen', False):
            # æ‰“åŒ…ç¯å¢ƒï¼šå°è¯•ä» _MEIPASS åŠ è½½é¢„æ„å»ºç¼“å­˜
            base_path = getattr(sys, '_MEIPASS', os.path.dirname(sys.executable))
            prebuilt_cache = Path(base_path) / 'prebuilt_cache'
            
            if prebuilt_cache.exists():
                self.cache_dir = prebuilt_cache
                logging.info(f"ğŸ¯ ä½¿ç”¨é¢„æ„å»ºç¼“å­˜: {self.cache_dir}")
            else:
                # å¦‚æœæ²¡æœ‰é¢„æ„å»ºç¼“å­˜ï¼Œä½¿ç”¨ç¨‹åºç›®å½•ï¼ˆå¯å†™ï¼‰
                self.cache_dir = Path(os.path.dirname(sys.executable))
                logging.info(f"ğŸ“‚ ä½¿ç”¨ç¨‹åºç›®å½•ç¼“å­˜: {self.cache_dir}")
        else:
            # å¼€å‘ç¯å¢ƒï¼šä½¿ç”¨æŒ‡å®šç›®å½•
            self.cache_dir = Path(cache_dir)
        
        self.cache_dir.mkdir(exist_ok=True)
        
        # ä¸‰ç§ç‹¬ç«‹ç¼“å­˜
        self.embedding_cache_file = self.cache_dir / 'embedding_cache.joblib'
        self.similarity_cache_file = self.cache_dir / 'similarity_matrix_cache.joblib'
        self.cross_encoder_cache_file = self.cache_dir / 'cross_encoder_cache.joblib'
        
        # åŠ è½½ç°æœ‰ç¼“å­˜
        self.embedding_cache = self._load_cache(self.embedding_cache_file)
        self.similarity_cache = self._load_cache(self.similarity_cache_file)
        self.cross_encoder_cache = self._load_cache(self.cross_encoder_cache_file)
        
        # ç¼“å­˜ç»Ÿè®¡
        self.stats = {
            'embedding_hits': 0,
            'embedding_misses': 0,
            'similarity_hits': 0,
            'similarity_misses': 0,
            'cross_encoder_hits': 0,
            'cross_encoder_misses': 0,
        }
    
    def _load_cache(self, cache_file: Path) -> dict:
        """åŠ è½½ç¼“å­˜æ–‡ä»¶"""
        if cache_file.exists():
            try:
                cache = joblib.load(cache_file)
                logging.info(f"âœ… åŠ è½½ç¼“å­˜: {cache_file.name} ({len(cache)} æ¡è®°å½•)")
                return cache
            except Exception as e:
                logging.warning(f"âš ï¸ ç¼“å­˜åŠ è½½å¤±è´¥ {cache_file.name}: {e}ï¼Œå°†é‡å»ºç¼“å­˜")
                return {}
        return {}
    
    def _save_cache(self, cache: dict, cache_file: Path):
        """ä¿å­˜ç¼“å­˜æ–‡ä»¶ï¼ˆå¢é‡å åŠ æ¨¡å¼ï¼‰"""
        try:
            # ğŸ†• å¢é‡å åŠ é€»è¾‘ï¼šå¦‚æœæ–‡ä»¶å·²å­˜åœ¨ï¼Œå…ˆåŠ è½½æ—§ç¼“å­˜ï¼Œç„¶ååˆå¹¶
            if cache_file.exists():
                try:
                    old_cache = joblib.load(cache_file)
                    old_count = len(old_cache)
                    
                    # åˆå¹¶ç¼“å­˜ï¼šæ–°ç¼“å­˜ä¼šè¦†ç›–æ—§ç¼“å­˜ä¸­çš„åŒåé”®
                    old_cache.update(cache)
                    
                    new_count = len(old_cache)
                    added_count = new_count - old_count
                    
                    # ä¿å­˜åˆå¹¶åçš„ç¼“å­˜
                    joblib.dump(old_cache, cache_file, compress=3)
                    
                    if added_count > 0:
                        logging.info(f"ğŸ’¾ ç¼“å­˜å åŠ ä¿å­˜: {cache_file.name} (æ–°å¢ {added_count} æ¡ï¼Œæ€»è®¡ {new_count} æ¡)")
                    else:
                        logging.info(f"ğŸ’¾ ç¼“å­˜æ›´æ–°: {cache_file.name} (æ— æ–°å¢ï¼Œæ€»è®¡ {new_count} æ¡)")
                    
                except Exception as e:
                    logging.warning(f"âš ï¸ æ—§ç¼“å­˜åŠ è½½å¤±è´¥ï¼Œå°†ç›´æ¥ä¿å­˜æ–°ç¼“å­˜: {e}")
                    joblib.dump(cache, cache_file, compress=3)
                    logging.info(f"ğŸ’¾ ä¿å­˜ç¼“å­˜: {cache_file.name} ({len(cache)} æ¡è®°å½•)")
            else:
                # æ–‡ä»¶ä¸å­˜åœ¨ï¼Œç›´æ¥ä¿å­˜
                joblib.dump(cache, cache_file, compress=3)
                logging.info(f"ğŸ’¾ ä¿å­˜ç¼“å­˜: {cache_file.name} ({len(cache)} æ¡è®°å½•)")
                
        except Exception as e:
            logging.error(f"âŒ ç¼“å­˜ä¿å­˜å¤±è´¥ {cache_file.name}: {e}")
    
    def get_embedding_cache_key(self, model_identifier: str, text: str) -> str:
        """ç”Ÿæˆå‘é‡ç¼“å­˜é”®"""
        cache_text = f"{model_identifier}||{text}"
        return hashlib.sha256(cache_text.encode('utf-8')).hexdigest()
    
    def get_similarity_cache_key(self, model_identifier: str, ids_a: List, ids_b: List) -> str:
        """ç”Ÿæˆç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜é”®"""
        # ä½¿ç”¨æ’åºåçš„å•†å“IDåˆ—è¡¨ç”Ÿæˆå”¯ä¸€é”®
        ids_a_str = ','.join(map(str, sorted(ids_a)))
        ids_b_str = ','.join(map(str, sorted(ids_b)))
        cache_text = f"{model_identifier}||{ids_a_str}||{ids_b_str}"
        return hashlib.sha256(cache_text.encode('utf-8')).hexdigest()
    
    def get_cross_encoder_cache_key(self, model_identifier: str, text_a: str, text_b: str) -> str:
        """ç”Ÿæˆ Cross-Encoder ç¼“å­˜é”®ï¼ˆæ–‡æœ¬å¯¹ï¼‰"""
        # æ ‡å‡†åŒ–é¡ºåºï¼šæŒ‰å­—å…¸åºæ’åˆ—ï¼Œç¡®ä¿ (A,B) å’Œ (B,A) ä½¿ç”¨åŒä¸€ç¼“å­˜
        if text_a > text_b:
            text_a, text_b = text_b, text_a
        cache_text = f"{model_identifier}||{text_a}||{text_b}"
        return hashlib.sha256(cache_text.encode('utf-8')).hexdigest()
    
    def get_embedding(self, model_identifier: str, text: str) -> Optional[np.ndarray]:
        """è·å–å‘é‡ç¼“å­˜"""
        key = self.get_embedding_cache_key(model_identifier, text)
        if key in self.embedding_cache:
            self.stats['embedding_hits'] += 1
            return self.embedding_cache[key]
        self.stats['embedding_misses'] += 1
        return None
    
    def set_embedding(self, model_identifier: str, text: str, vector: np.ndarray):
        """è®¾ç½®å‘é‡ç¼“å­˜"""
        key = self.get_embedding_cache_key(model_identifier, text)
        self.embedding_cache[key] = np.array(vector).flatten()
    
    def get_similarity_matrix(self, model_identifier: str, ids_a: List, ids_b: List) -> Optional[np.ndarray]:
        """è·å–ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜"""
        key = self.get_similarity_cache_key(model_identifier, ids_a, ids_b)
        if key in self.similarity_cache:
            self.stats['similarity_hits'] += 1
            return self.similarity_cache[key]
        self.stats['similarity_misses'] += 1
        return None
    
    def set_similarity_matrix(self, model_identifier: str, ids_a: List, ids_b: List, matrix: np.ndarray):
        """è®¾ç½®ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜"""
        key = self.get_similarity_cache_key(model_identifier, ids_a, ids_b)
        self.similarity_cache[key] = matrix
    
    def get_cross_encoder_score(self, model_identifier: str, text_a: str, text_b: str) -> Optional[float]:
        """è·å– Cross-Encoder åˆ†æ•°ç¼“å­˜"""
        key = self.get_cross_encoder_cache_key(model_identifier, text_a, text_b)
        if key in self.cross_encoder_cache:
            self.stats['cross_encoder_hits'] += 1
            return self.cross_encoder_cache[key]
        self.stats['cross_encoder_misses'] += 1
        return None
    
    def set_cross_encoder_score(self, model_identifier: str, text_a: str, text_b: str, score: float):
        """è®¾ç½® Cross-Encoder åˆ†æ•°ç¼“å­˜"""
        key = self.get_cross_encoder_cache_key(model_identifier, text_a, text_b)
        self.cross_encoder_cache[key] = float(score)
    
    def save_all(self):
        """ä¿å­˜æ‰€æœ‰ç¼“å­˜"""
        self._save_cache(self.embedding_cache, self.embedding_cache_file)
        self._save_cache(self.similarity_cache, self.similarity_cache_file)
        self._save_cache(self.cross_encoder_cache, self.cross_encoder_cache_file)
    
    def print_stats(self):
        """æ‰“å°ç¼“å­˜ç»Ÿè®¡ä¿¡æ¯"""
        total_embedding = self.stats['embedding_hits'] + self.stats['embedding_misses']
        total_similarity = self.stats['similarity_hits'] + self.stats['similarity_misses']
        total_cross = self.stats['cross_encoder_hits'] + self.stats['cross_encoder_misses']
        
        print("\n" + "="*60)
        print("ğŸ“Š ç¼“å­˜æ€§èƒ½ç»Ÿè®¡")
        print("="*60)
        
        if total_embedding > 0:
            hit_rate = self.stats['embedding_hits'] / total_embedding * 100
            print(f"å‘é‡ç¼“å­˜: {self.stats['embedding_hits']}/{total_embedding} å‘½ä¸­ ({hit_rate:.1f}%)")
        
        if total_similarity > 0:
            hit_rate = self.stats['similarity_hits'] / total_similarity * 100
            print(f"ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜: {self.stats['similarity_hits']}/{total_similarity} å‘½ä¸­ ({hit_rate:.1f}%)")
        
        if total_cross > 0:
            hit_rate = self.stats['cross_encoder_hits'] / total_cross * 100
            saved_time = self.stats['cross_encoder_hits'] * 0.01  # å‡è®¾æ¯æ¬¡èŠ‚çœ 10ms
            print(f"Cross-Encoder ç¼“å­˜: {self.stats['cross_encoder_hits']}/{total_cross} å‘½ä¸­ ({hit_rate:.1f}%)")
            print(f"é¢„ä¼°èŠ‚çœæ—¶é—´: {saved_time:.1f} ç§’")
        
        print("="*60 + "\n")

# å…¨å±€ç¼“å­˜ç®¡ç†å™¨å®ä¾‹
cache_manager = CacheManager()

# ==============================================================================
# 3. æ—¥å¿—ä¸å…¨å±€é…ç½® (éœ€è¦ä¿®æ”¹çš„å‚æ•°éƒ½åœ¨è¿™é‡Œ)
# ==============================================================================
class Config:
    # --- åº—é“ºåç§°é…ç½® (å»ºè®®ä½¿ç”¨ä¸Šä¼ ç›®å½•æ¨¡å¼ï¼Œæ— éœ€æ‰‹åŠ¨ä¿®æ”¹) ---
    # ğŸ’¡ æ¨èï¼šä½¿ç”¨ä¸Šä¼ ç›®å½•æ¨¡å¼ï¼ˆupload/æœ¬åº—ã€upload/ç«å¯¹ï¼‰ï¼Œè‡ªåŠ¨è¯†åˆ«åº—é“ºå
    # âš ï¸ ä»…åœ¨ç¦ç”¨ä¸Šä¼ ç›®å½•æ¨¡å¼æ—¶ï¼Œæ‰éœ€è¦æ‰‹åŠ¨ä¿®æ”¹è¿™é‡Œçš„åº—é“ºåå’Œæ–‡ä»¶å
    
    # >>> å¤‡ç”¨é…ç½®ï¼šæ‰‹åŠ¨æŒ‡å®šåº—é“ºåç§°å’Œæ–‡ä»¶å <<<
    STORE_A_NAME = 'æœ¬åº—'  # ä¸Šä¼ ç›®å½•æ¨¡å¼å¯ç”¨æ—¶ï¼Œæ­¤å€¼ä¼šè¢«è‡ªåŠ¨è¦†ç›–
    STORE_B_NAME = 'ç«å¯¹'  # ä¸Šä¼ ç›®å½•æ¨¡å¼å¯ç”¨æ—¶ï¼Œæ­¤å€¼ä¼šè¢«è‡ªåŠ¨è¦†ç›–
    # <<< åº—é“ºåç§°é…ç½®åŒºåŸŸç»“æŸ >>>
    
    # å¤‡ç”¨æ–‡ä»¶åï¼ˆä»…åœ¨ä¸Šä¼ ç›®å½•ä¸­æ— æ–‡ä»¶æ—¶ä½¿ç”¨ï¼‰
    STORE_A_FILENAME = 'æœ¬åº—æ•°æ®.xlsx'
    STORE_B_FILENAME = 'ç«å¯¹æ•°æ®.xlsx'
    
    # ä¸Šä¼ å…¥å£é…ç½® - é€šè¿‡ä¸åŒç›®å½•åŒºåˆ†æœ¬åº—å’Œç«å¯¹ï¼ˆğŸŒŸæ¨èæ¨¡å¼ï¼‰
    UPLOAD_DIR_STORE_A = 'upload/store_a'   # æœ¬åº—æ•°æ®ä¸Šä¼ ç›®å½•
    UPLOAD_DIR_STORE_B = 'upload/store_b'   # ç«å¯¹æ•°æ®ä¸Šä¼ ç›®å½•
    USE_UPLOAD_DIRS = True                  # å¯ç”¨ä¸Šä¼ ç›®å½•æ¨¡å¼ï¼ˆæ¨èä¿æŒä¸ºTrueï¼‰
    # âš ï¸ è‹¥è®¾ä¸º Falseï¼Œå°†ä½¿ç”¨ä¸Šé¢çš„ STORE_A_FILENAME å’Œ STORE_B_FILENAME
    OUTPUT_FILE = 'matched_products_comparison_final.xlsx'
    
    # æ¨¡å‹é€‰é¡¹é…ç½® - æ”¯æŒå¤šä¸ªé¢„å®šä¹‰æ¨¡å‹
    # ğŸ“Œ ä¸¤é˜¶æ®µåŒ¹é…ç­–ç•¥ï¼š
    #   1. Sentence-BERT (å¥å‘é‡) - å¿«é€Ÿç²—ç­›ï¼Œå°†å•†å“è½¬ä¸ºå‘é‡åè®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦
    #   2. Cross-Encoder (äº¤å‰ç¼–ç å™¨) - ç²¾å‡†ç²¾æ’ï¼Œç›´æ¥åˆ¤æ–­ä¸¤ä¸ªå•†å“æ˜¯å¦åŒ¹é…
    AVAILABLE_MODELS = {
        '1': {
            'name': 'paraphrase-multilingual-mpnet-base-v2',
            'display_name': 'æ ‡å‡†å¤šè¯­è¨€æ¨¡å‹ (åŸæ¨¡å‹)',
            'description': 'é€šç”¨å¤šè¯­è¨€æ¨¡å‹ï¼Œæˆç†Ÿç¨³å®š',
            'size': '~420MB',
            'speed': 'æ­£å¸¸',
            'accuracy': 'è‰¯å¥½',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ æ ¹æ®18:57æ­£å¸¸æ•°æ®åæ¨çš„é˜ˆå€¼
        },
        '2': {
            'name': 'BAAI/bge-base-zh-v1.5',
            'display_name': 'BGEä¸­æ–‡ä¼˜åŒ–æ¨¡å‹',
            'description': 'ä¸“ä¸ºä¸­æ–‡ä¼˜åŒ–ï¼Œå‡†ç¡®ç‡æå‡15-20%',
            'size': '~560MB',
            'speed': 'æ­£å¸¸',
            'accuracy': 'ä¼˜ç§€',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ ä¸æ¨¡å‹1ä¿æŒä¸€è‡´
        },
        '3': {
            'name': 'moka-ai/m3e-base',
            'display_name': 'M3Eç”µå•†åœºæ™¯æ¨¡å‹',
            'description': 'é’ˆå¯¹ç”µå•†åœºæ™¯ä¼˜åŒ–',
            'size': '~400MB',
            'speed': 'è¾ƒå¿«',
            'accuracy': 'ä¼˜ç§€',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ ä¸æ¨¡å‹1ä¿æŒä¸€è‡´
        },
        '4': {
            'name': 'BAAI/bge-large-zh-v1.5',
            'display_name': 'BGE-Large æ——èˆ°æ¨¡å‹ â­',
            'description': 'BGEç³»åˆ—æœ€å¼ºç‰ˆæœ¬ï¼Œ1024ç»´å‘é‡ï¼Œå‡†ç¡®ç‡æœ€é«˜',
            'size': '~1.3GB',
            'speed': 'è¾ƒæ…¢',
            'accuracy': 'é¡¶çº§',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ é™ä½é˜ˆå€¼ï¼Œå¢åŠ å¬å›ç‡
        },
        '5': {
            'name': 'BAAI/bge-m3',
            'display_name': 'BGE-M3 å¤šç²’åº¦æ¨¡å‹',
            'description': 'æ”¯æŒæ··åˆæ£€ç´¢ï¼Œå¤šè¯­è¨€å¤šç²’åº¦ï¼Œæœ€æ–°ä¸€ä»£æ¨¡å‹',
            'size': '~2.2GB',
            'speed': 'è¾ƒæ…¢',
            'accuracy': 'é¡¶çº§',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ å›å½’18:57å†å²æ•°æ®é˜ˆå€¼ï¼Œé…åˆä¸‰çº§åˆ†ç±»æ£€æŸ¥å…³é—­
        },
        '6': {
            'name': 'BAAI/bge-small-zh-v1.5',
            'display_name': 'BGE-Small è½»é‡æ¨¡å‹',
            'description': 'é€Ÿåº¦å¿«ï¼Œé€‚åˆå¤§æ‰¹é‡æ•°æ®ï¼Œå‡†ç¡®ç‡ç•¥ä½',
            'size': '~100MB',
            'speed': 'å¿«é€Ÿ',
            'accuracy': 'è‰¯å¥½+',
            'recommended_threshold': {'hard': 0.52, 'soft': 0.50},  # è½»é‡æ¨¡å‹ç•¥å®½æ¾
        },
        # ğŸš€ é«˜çº§æ¨¡å‹é€‰é¡¹
        '7': {
            'name': 'intfloat/multilingual-e5-large',
            'display_name': 'E5-Large å¤šè¯­è¨€æ——èˆ° ğŸŒ',
            'description': 'å¤šè¯­è¨€åœºæ™¯æœ€å¼ºï¼Œæ”¯æŒ100+è¯­è¨€ï¼Œ1024ç»´å‘é‡',
            'size': '~2.2GB',
            'speed': 'æ…¢',
            'accuracy': 'é¡¶çº§+',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ é™ä½é˜ˆå€¼ï¼Œå¢åŠ å¬å›ç‡
        },
        '8': {
            'name': 'GanymedeNil/text2vec-large-chinese',
            'display_name': 'Text2Vec-Large ä¸­æ–‡å¼ºåŒ– ğŸ‡¨ğŸ‡³',
            'description': 'ä¸­æ–‡è¯­ä¹‰ç†è§£æœ€å¼ºï¼Œ1024ç»´ï¼Œç”µå•†åœºæ™¯ä¼˜åŒ–',
            'size': '~1.3GB',
            'speed': 'è¾ƒæ…¢',
            'accuracy': 'é¡¶çº§',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ é™ä½é˜ˆå€¼ï¼Œå¢åŠ å¬å›ç‡
        },
        '9': {
            'name': 'BAAI/bge-large-zh-v1.5',  # å¤‡é€‰ï¼šæ¨èä½¿ç”¨é€‰é¡¹4
            'display_name': 'BGE-Large-ZH v1.5 (æ¨è) â­â­',
            'description': 'ä¸­æ–‡å•†å“åŒ¹é…é»„é‡‘æ ‡å‡†ï¼Œå‡†ç¡®ç‡æé«˜',
            'size': '~1.3GB',
            'speed': 'è¾ƒæ…¢',
            'accuracy': 'é¡¶çº§',
            'recommended_threshold': {'hard': 0.42, 'soft': 0.38},  # ğŸ”§ é™ä½é˜ˆå€¼ï¼Œå¢åŠ å¬å›ç‡
        },
    }
    
    # --- Cross-Encoder å¯ç”¨æ¨¡å‹é…ç½® ---
    AVAILABLE_CROSS_ENCODERS = {
        '1': {
            'name': 'cross-encoder/ms-marco-MiniLM-L-6-v2',
            'display_name': 'MS-Marco-MiniLM (é»˜è®¤)',
            'description': 'å¾®è½¯å¼€æºè½»é‡çº§æ¨¡å‹ï¼Œé€Ÿåº¦å¿«ä½†ä¸­æ–‡æ•ˆæœä¸€èˆ¬',
            'size': '~90MB',
            'speed': 'æå¿«',
            'accuracy': 'ä¸­ç­‰',
            'language': 'è‹±æ–‡ä¼˜å…ˆ'
        },
        '2': {
            'name': 'BAAI/bge-reranker-large',
            'display_name': 'BGE-Reranker-Large â­æ¨è',
            'description': 'ä¸­æ–‡ä¼˜åŒ–å¤§æ¨¡å‹ï¼Œå‡†ç¡®ç‡æå‡40%ï¼Œç”µå•†åœºæ™¯å¼ºåŠ›æ¨è',
            'size': '~1.3GB',
            'speed': 'ä¸­',
            'accuracy': 'æé«˜',
            'language': 'ä¸­è‹±åŒè¯­'
        },
        '3': {
            'name': 'BAAI/bge-reranker-base',
            'display_name': 'BGE-Reranker-Base âš¡å¹³è¡¡',
            'description': 'é€Ÿåº¦ä¸å‡†ç¡®ç‡å¹³è¡¡ï¼Œæ¯”Largeå¿«15%ï¼Œå‡†ç¡®ç‡æå‡25%',
            'size': '~309MB',
            'speed': 'å¿«',
            'accuracy': 'é«˜',
            'language': 'ä¸­è‹±åŒè¯­'
        },
        '4': {
            'name': 'cross-encoder/ms-marco-MiniLM-L-12-v2',
            'display_name': 'MS-Marco-MiniLM-L12',
            'description': 'MS-Marcoæ·±å±‚ç‰ˆæœ¬ï¼Œå‡†ç¡®ç‡ç•¥é«˜äºL-6ä½†é€Ÿåº¦è¾ƒæ…¢',
            'size': '~130MB',
            'speed': 'å¿«',
            'accuracy': 'ä¸­é«˜',
            'language': 'è‹±æ–‡ä¼˜å…ˆ'
        },
    }
    
    # é»˜è®¤æ¨¡å‹ï¼šæ‰“åŒ…ç‰ˆæœ¬ä½¿ç”¨ bge-large-zh-v1.5 (æ¨¡å‹4)
    # å¼€å‘ç¯å¢ƒå¯æ‰‹åŠ¨é€‰æ‹©å…¶ä»–æ¨¡å‹
    # ğŸ†• æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆGUIæ¨¡å¼ä¼ é€’ï¼‰
    SENTENCE_BERT_MODEL = os.environ.get('EMBEDDING_MODEL', 'BAAI/bge-base-zh-v1.5')  # é»˜è®¤å¹³è¡¡æ¨¡å¼
    ENABLE_MODEL_SELECTION = True  # å¯ç”¨è¿è¡Œæ—¶æ¨¡å‹é€‰æ‹©
    EMBEDDING_CACHE_FILE = 'embedding_cache.joblib'
    # å¯¼å‡ºç›®å½•ï¼ˆç›¸å¯¹äºè„šæœ¬æ‰€åœ¨ç›®å½•ï¼‰ã€‚é»˜è®¤ç»Ÿä¸€å†™å…¥ reports/ ä¾¿äºç®¡ç†
    OUTPUT_DIR = 'reports'

    # å‘é‡ç¼–ç æ‰¹å¤§å°ï¼ˆæ ¹æ®GPUæ˜¾å­˜è‡ªåŠ¨è°ƒæ•´ï¼‰
    # æ˜¾å­˜ â‰¥8GB: 256 (æœ€å¿«)
    # æ˜¾å­˜ 4-8GB: 64 (ä¿å®ˆï¼Œé˜²æ­¢RTX 2060 6GBæ˜¾å­˜æº¢å‡º)
    # æ˜¾å­˜ <4GB: 32 (éå¸¸ä¿å®ˆ)
    # CPUæ¨¡å¼: 32 (é¿å…å†…å­˜æº¢å‡º)
    ENCODE_BATCH_SIZE = int(os.environ.get('ENCODE_BATCH_SIZE', '64'))  # é™ä½é»˜è®¤å€¼ä»128â†’64

    # å¯é€‰ï¼šå¼ºåˆ¶è®¡ç®—è®¾å¤‡ï¼ˆ'cuda' æˆ– 'cpu'ï¼‰ï¼Œä¸º None æ—¶è‡ªåŠ¨æ£€æµ‹
    FORCE_DEVICE: Optional[str] = None

    # æ˜¯å¦å¯¼å‡ºæ¸…æ´—åçš„æ•°æ®ç›¸å…³ Sheetï¼ˆåº—Aæ¸…æ´—ã€åº—Bæ¸…æ´—ã€åˆå¹¶æ¸…æ´—å¯¹æ¯”ï¼‰
    EXPORT_CLEANED_SHEETS = False

    # --- æˆæœ¬é¢„æµ‹åŠŸèƒ½é…ç½® (ğŸ†• ç¬¬ä¸€é˜¶æ®µ) ---
    ENABLE_COST_PREDICTION = True  # æ€»å¼€å…³ï¼šå¯ç”¨æˆæœ¬é¢„æµ‹åŠŸèƒ½
    COST_PREDICTION_STRATEGY = 'markup_rate'  # é¢„æµ‹ç­–ç•¥ï¼š'markup_rate'=åŠ ä»·ç‡æ³•
    EXPORT_COST_SHEETS = True  # å¯¼å‡ºæˆæœ¬åˆ†æç›¸å…³ Sheet
    COST_COLUMN_NAME = 'æˆæœ¬'  # æˆæœ¬åˆ—åç§°ï¼ˆæœ¬åº—æ•°æ®ä¸­çš„åˆ—åï¼‰
    
    # æˆæœ¬é¢„æµ‹å‚æ•°
    COST_PREDICTION_MIN_SAMPLES = 3  # å“ç±»æœ€å°‘æ ·æœ¬é‡ï¼ˆå°‘äºæ­¤å€¼é™çº§åˆ°ä¸Šçº§åˆ†ç±»ï¼‰
    COST_CONFIDENCE_THRESHOLD = 0.5  # æœ€ä½ç½®ä¿¡åº¦é˜ˆå€¼
    
    # ğŸ†• å”®ä»·åŠ æƒé¢„æµ‹é…ç½®
    USE_SALE_PRICE_WEIGHT = True  # æ˜¯å¦ä½¿ç”¨å”®ä»·è¿›è¡ŒåŠ æƒé¢„æµ‹
    ORIGINAL_PRICE_WEIGHT = 0.7  # åŸä»·æƒé‡ï¼ˆå®šä»·ç­–ç•¥ï¼Œç¨³å®šæ€§é«˜ï¼‰
    SALE_PRICE_WEIGHT = 0.3  # å”®ä»·æƒé‡ï¼ˆå®é™…åˆ©æ¶¦ï¼Œåæ˜ ä¿ƒé”€ï¼‰
    
    # ğŸ›¡ï¸ æç«¯æŠ˜æ‰£ä¿æŠ¤æœºåˆ¶ï¼ˆé˜²æ­¢å¼•æµå“æ±¡æŸ“æˆæœ¬é¢„æµ‹ï¼‰
    MIN_DISCOUNT_RATE = 0.50  # æœ€ä½æŠ˜æ‰£ç‡50%ï¼ˆå”®ä»·ä½äºåŸä»·50%æ—¶è§†ä¸ºå¼‚å¸¸ä¿ƒé”€ï¼Œä¸ä½¿ç”¨å”®ä»·é¢„æµ‹ï¼‰
    MAX_DISCOUNT_RATE = 1.05  # æœ€é«˜æŠ˜æ‰£ç‡105%ï¼ˆå”®ä»·è¶…è¿‡åŸä»·5%æ—¶å¯èƒ½æ˜¯æ•°æ®é”™è¯¯ï¼‰
    SALE_PRICE_WEIGHT_DECAY_THRESHOLD = 0.70  # æŠ˜æ‰£ç‡ä½äº70%æ—¶ï¼Œå”®ä»·æƒé‡è¡°å‡
    
    # ğŸ¯ éåŒ¹é…å•†å“ç½®ä¿¡åº¦æƒ©ç½šï¼ˆå“ç±»æ³›åŒ–é£é™©ï¼‰
    NON_MATCHED_CONFIDENCE_PENALTY = 0.15  # éåŒ¹é…å•†å“ç½®ä¿¡åº¦é™ä½15%ï¼ˆæ— åŒæ¬¾éªŒè¯ï¼‰
    
    # --- Cross-Encoder (ç²¾æ’æ¨¡å‹) é…ç½® ---
    # Cross-Encoder ç”¨äºç²¾ç¡®åˆ¤æ–­ä¸¤ä¸ªå•†å“æ˜¯å¦åŒ¹é…ï¼Œå‡†ç¡®ç‡è¿œé«˜äºSentence-BERT
    # ğŸ’¡ å‡çº§å»ºè®®ï¼š
    #   - å½“å‰: ms-marco-MiniLM-L-6-v2 (è‹±æ–‡è®­ç»ƒï¼Œä¸­æ–‡æ•ˆæœä¸€èˆ¬)
    #   - æ¨è: BAAI/bge-reranker-large (ä¸­æ–‡ä¼˜åŒ–ï¼Œå‡†ç¡®ç‡æå‡40%)
    #   - ç”µå•†: BAAI/bge-reranker-base (é€Ÿåº¦ä¸å‡†ç¡®ç‡å¹³è¡¡)
    USE_LOCAL_CROSS_ENCODER = False
    # ONLINE_CROSS_ENCODER = 'cross-encoder/ms-marco-MiniLM-L-6-v2'  # è‹±æ–‡æ¨¡å‹ï¼ˆä¸æ¨èä¸­æ–‡åœºæ™¯ï¼‰
    # ğŸ†• æ”¯æŒé€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆGUIæ¨¡å¼ä¼ é€’ï¼‰
    ONLINE_CROSS_ENCODER = os.environ.get('RERANKER_MODEL', 'BAAI/bge-reranker-base')  # é»˜è®¤å¹³è¡¡æ¨¡å¼
    # ONLINE_CROSS_ENCODER = 'BAAI/bge-reranker-base'   # âš¡ å¹³è¡¡é€‰é¡¹ï¼šé€Ÿåº¦å¿«15%
    LOCAL_CROSS_ENCODER_PATH = 'D:/AI_Models/cross-encoder-model' # â€¼ï¸æ›¿æ¢ä¸ºä½ çš„æ¨¡å‹æ–‡ä»¶å¤¹å®é™…è·¯å¾„

    # ç¦»çº¿é¦–é€‰ï¼šæœ¬åœ° Sentence-BERT æ¨¡å‹ç›®å½•ï¼ˆåŒ…å« config.jsonã€pytorch_model.bin/safetensors ç­‰ï¼‰
    USE_LOCAL_SENTENCE_BERT = False
    LOCAL_SENTENCE_BERT_PATH = 'D:/AI_Models/sentence-transformers/paraphrase-multilingual-mpnet-base-v2'

    # FUZZY_MATCH_PARAMS å·²è¢«ç§»åŠ¨åˆ°å…·ä½“çš„åŒ¹é…å‡½æ•°å†…éƒ¨ï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„æ§åˆ¶
    # FUZZY_MATCH_PARAMS = {
    #     "price_similarity_percent": 20,
    #     "composite_threshold": 0.2, # ğŸ”„ Colab tuned: 0.2
    #     "strict_threshold_for_generic_cat": 0.30, # è¿›ä¸€æ­¥æé«˜å¯¹â€œå¼±åŒ¹é…â€çš„å®¡æŸ¥æ ‡å‡†
    #     "text_weight": 0.5, # ğŸ”„ è°ƒæ•´æ–‡æœ¬æƒé‡ï¼Œä¸ºåˆ†ç±»æƒé‡è®©å‡ºç©ºé—´
    #     "brand_weight": 0.3, # ğŸ”„ å“ç‰Œæƒé‡30%
    #     "category_weight": 0.1, #  å¯ç”¨åˆ†ç±»æƒé‡10%ï¼Œæå‡åˆ†ç±»åŒ¹é…ç‡
    #     "specs_weight": 0.1,
    #     "candidates_to_check": 1000, # ğŸ”„ å¢å¤§åˆ°1000ï¼ŒColabä¸­æ£€æŸ¥æ‰€æœ‰æ½œåœ¨åŒ¹é…é¡¹
    #     "require_category_match": False, # ğŸ”„ æµ‹è¯•ï¼šå…³é—­åˆ†ç±»è¿‡æ»¤ï¼Œä½¿ç”¨å…¨é‡åŒ¹é…æ¨¡å¼
    # }

# ==============================================================================
# 3. æ ¸å¿ƒè¾…åŠ©å‡½æ•°
# ==============================================================================
# å¸¸è§å“ç‰Œåˆ—è¡¨ï¼ˆåŸºäºæ•°æ®åˆ†ææ‰©å±•ï¼‰
COMMON_BRANDS = [
    'å›ä¹å®', 'å‘³å…¨', 'æ–°å¸Œæœ›', 'å…¬ç‰›', 'æµ·æ°æµ·è¯º', 'ç€šæ€', 'åº·ç›Šåšå£«', 'æƒ é€‰', 'é˜¿å°”å‘æ–¯',
    'ç¾çš„', 'SKG', 'éº¦å¾·æ°', 'å…ƒæ°”æ£®æ—', 'BGM', 'ä¹é˜³', 'å°èµ¤å…”', 'æ¥ä¹', 'å¤é£',
    'luckyç†Š', 'é¸¿å°˜', 'å† é“¶', 'æ³“è±'
]
COMMON_BRANDS = [brand.lower() for brand in COMMON_BRANDS]  # è½¬ä¸ºå°å†™ä¾¿äºåŒ¹é…

def clean_text(text):
    if isinstance(text, str):
        text = re.sub(r'[^\u4e00-\u9fa5a-zA-Z0-9\s]', '', text)
        return text.lower().strip()
    return ""

def extract_brand(name, vendor_category):
    if isinstance(name, str):
        name_lower = name.lower()
        match = re.search(r'[ã€\[ï¼ˆ(](.*?)[ã€‘\])ï¼‰]', name_lower)
        if match:
            return match.group(1).strip()
    if isinstance(vendor_category, str):
        parts = [p.strip() for p in vendor_category.split('>') if p.strip()]
        if len(parts) > 0:
            return parts[0]
    return "å…¶ä»–"

def extract_brand_enhanced(text):
    """æå–å“ç‰Œä¿¡æ¯ - ä½¿ç”¨æ‰©å±•çš„å“ç‰Œåˆ—è¡¨å’Œæ­£åˆ™åŒ¹é…ï¼ˆColabç‰ˆæœ¬æ•´åˆï¼‰"""
    if pd.isna(text) or not text:
        return ""
    
    text_lower = text.lower()
    
    # é¦–å…ˆæ£€æŸ¥å·²çŸ¥å“ç‰Œåˆ—è¡¨
    for brand in COMMON_BRANDS:
        if brand in text_lower:
            return brand
    
    # è‹±æ–‡å“ç‰Œæ¨¡å¼ (2-20å­—ç¬¦ï¼Œå¯å«æ•°å­—)
    english_pattern = r'\b([A-Za-z][A-Za-z0-9]{1,19})\b'
    # ä¸­æ–‡å“ç‰Œæ¨¡å¼ (2-8å­—ç¬¦)
    chinese_pattern = r'[\u4e00-\u9fff]{2,8}'
    
    matches = re.findall(english_pattern, text) + re.findall(chinese_pattern, text)
    
    if matches:
        # è¿”å›æœ€é•¿çš„åŒ¹é…é¡¹ï¼ˆé€šå¸¸æ˜¯å“ç‰Œåï¼‰
        return max(matches, key=len)
    
    return ""

def extract_specifications(text):
    """æå–äº§å“è§„æ ¼ä¿¡æ¯ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    if pd.isna(text) or not text:
        return {}
    
    text = str(text)
    specs = {}
    
    # å®¹é‡/é‡é‡è§„æ ¼
    volume_pattern = r'(\d+(?:\.\d+)?)\s*([mlkgLå…‹å‡æ¯«å‡å…¬æ–¤æ–¤])'
    volume_matches = re.findall(volume_pattern, text)
    for value, unit in volume_matches:
        specs[f'å®¹é‡({unit})'] = float(value)
    
    # å°ºå¯¸è§„æ ¼
    size_pattern = r'(\d+(?:\.\d+)?)\s*[xX*Ã—]\s*(\d+(?:\.\d+)?)\s*[xX*Ã—]?\s*(\d+(?:\.\d+)?)?'
    size_matches = re.findall(size_pattern, text)
    if size_matches:
        dims = size_matches[0]
        if dims[2]:  # ä¸‰ç»´
            specs['å°ºå¯¸'] = f"{dims[0]}Ã—{dims[1]}Ã—{dims[2]}"
        else:  # äºŒç»´
            specs['å°ºå¯¸'] = f"{dims[0]}Ã—{dims[1]}"
    
    # åŠŸç‡è§„æ ¼
    power_pattern = r'(\d+(?:\.\d+)?)\s*(w|W|ç“¦|åŠŸç‡)'
    power_matches = re.findall(power_pattern, text)
    if power_matches:
        specs['åŠŸç‡(W)'] = float(power_matches[0][0])
    
    return specs

def categorize_price_band(price):
    """ä»·æ ¼åˆ†å±‚ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    if pd.isna(price) or price == 0:
        return "æœªçŸ¥"
    
    if price <= 20:
        return "ä½ä»·ä½(â‰¤20)"
    elif price <= 50:
        return "ä¸­ä½ä»·ä½(20-50)"
    elif price <= 100:
        return "ä¸­ä»·ä½(50-100)"
    elif price <= 200:
        return "ä¸­é«˜ä»·ä½(100-200)"
    else:
        return "é«˜ä»·ä½(>200)"

def calculate_feature_similarity(features1, features2):
    """è®¡ç®—ç‰¹å¾ç›¸ä¼¼åº¦ï¼ˆåŸºäºè§„æ ¼å‚æ•°ï¼‰ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    if not features1 or not features2:
        return 0.0
    
    # æ‰¾åˆ°å…±åŒçš„è§„æ ¼é”®
    common_keys = set(features1.keys()) & set(features2.keys())
    if not common_keys:
        return 0.0
    
    similarity_scores = []
    for key in common_keys:
        val1, val2 = features1[key], features2[key]
        if isinstance(val1, (int, float)) and isinstance(val2, (int, float)):
            # æ•°å€¼å‹ï¼šè®¡ç®—ç›¸å¯¹å·®å¼‚
            max_val = max(val1, val2)
            if max_val > 0:
                similarity = 1 - abs(val1 - val2) / max_val
                similarity_scores.append(similarity)
        elif str(val1).lower() == str(val2).lower():
            # å­—ç¬¦å‹ï¼šå®Œå…¨åŒ¹é…
            similarity_scores.append(1.0)
    
    return sum(similarity_scores) / len(similarity_scores) if similarity_scores else 0.0

def calculate_discount(row, sale_price_col, original_price_col):
    """è®¡ç®—æŠ˜æ‰£ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    try:
        sale_price = pd.to_numeric(row.get(sale_price_col, 0), errors='coerce')
        original_price = pd.to_numeric(row.get(original_price_col, 0), errors='coerce')
        
        if pd.isna(sale_price) or pd.isna(original_price) or original_price == 0:
            return None
            
        discount = (original_price - sale_price) / original_price * 100
        return round(discount, 2)
    except:
        return None

def tokenize_text(text):
    """æ–‡æœ¬åˆ†è¯ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    if pd.isna(text) or not text:
        return []
    
    text = str(text).lower()
    # ç®€å•åˆ†è¯ï¼šæŒ‰ç©ºæ ¼å’Œå¸¸è§åˆ†éš”ç¬¦åˆ†å‰²
    import re
    tokens = re.findall(r'[\u4e00-\u9fa5a-zA-Z0-9]+', text)
    return [token for token in tokens if len(token) > 1]

def standardize_brand(brand):
    """å“ç‰Œæ ‡å‡†åŒ–ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    if pd.isna(brand) or not brand:
        return ""
    
    brand = str(brand).lower().strip()
    # ç§»é™¤å¸¸è§çš„å“ç‰Œåç¼€
    suffixes_to_remove = ['ç‰Œ', 'å“ç‰Œ', 'å…¬å¸', 'co', 'ltd', 'inc']
    for suffix in suffixes_to_remove:
        if brand.endswith(suffix):
            brand = brand[:-len(suffix)].strip()
    
    return brand

def get_average_word_vector(tokens, word2vec_model, vector_size):
    """è·å–è¯å‘é‡å¹³å‡å€¼ï¼ˆColabç‰ˆæœ¬æ–°å¢åŠŸèƒ½ï¼‰"""
    if not tokens or not word2vec_model:
        return np.zeros(vector_size)
    
    vectors = []
    for token in tokens:
        try:
            if hasattr(word2vec_model, 'wv') and token in word2vec_model.wv:
                vectors.append(word2vec_model.wv[token])
            elif hasattr(word2vec_model, '__getitem__') and token in word2vec_model:
                vectors.append(word2vec_model[token])
        except:
            continue
    
    if vectors:
        return np.mean(vectors, axis=0)
    else:
        return np.zeros(vector_size)

def extract_specs(name: str) -> str:
    """ä»å•†å“åç§°ä¸­æå–è§„æ ¼ä¿¡æ¯"""
    if not isinstance(name, str):
        return ""
    
    # åŒ¹é…å¸¸è§çš„è§„æ ¼å•ä½
    # ä¾‹å¦‚: 500ml, 1.5L, 2kg, 300g, 12*50g, 6è¿åŒ…, 5ç‰‡, 12æ”¯/ç›’
    patterns = [
        r'(\d+\.?\d*\s*[gGå…‹])',
        r'(\d+\.?\d*\s*[kK][gGåƒå…‹])',
        r'(\d+\.?\d*\s*[mM][lLæ¯«å‡])',
        r'(\d+\.?\d*\s*[lLå‡])',
        r'(\d+\s*[\*xX]\s*\d+\s*[gGå…‹]?)', # 12*50g
        r'(\d+\s*[è¿åŒ…ç‰‡è¢‹è£…æ”¯å¬])' # 6è¿åŒ…
    ]
    found_specs = []
    for pattern in patterns:
        matches = re.findall(pattern, name)
        found_specs.extend([re.sub(r'\s', '', m).lower() for m in matches])
    
    return " ".join(sorted(list(set(found_specs)))) # æ’åºå»é‡ï¼Œç¡®ä¿é¡ºåºä¸å½±å“æ¯”è¾ƒ

def calculate_feature_similarity(row_a, row_b):
    # å“ç‰Œç›¸ä¼¼åº¦è®¡ç®—
    brand_a = row_a.get('standardized_brand')
    brand_b = row_b.get('standardized_brand')
    brand_similarity = 1 if brand_a and brand_b and brand_a != 'å…¶ä»–' and brand_a == brand_b else 0

    # è§„æ ¼ç›¸ä¼¼åº¦è®¡ç®—
    specs_a = row_a.get('specs')
    specs_b = row_b.get('specs')
    specs_similarity = 1 if specs_a and specs_a == specs_b else 0

    # ğŸ”§ æ–°å¢ï¼šåˆ†ç±»ç›¸ä¼¼åº¦è®¡ç®—
    # ä¸€çº§åˆ†ç±»ç›¸ä¼¼åº¦
    cat1_a = row_a.get('ç¾å›¢ä¸€çº§åˆ†ç±»', '')
    cat1_b = row_b.get('ç¾å›¢ä¸€çº§åˆ†ç±»', '')
    cat1_similarity = 1 if cat1_a and cat1_b and str(cat1_a) == str(cat1_b) else 0
    
    # ä¸‰çº§åˆ†ç±»ç›¸ä¼¼åº¦  
    cat3_a = row_a.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', '')
    cat3_b = row_b.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', '')
    cat3_similarity = 1 if cat3_a and cat3_b and str(cat3_a) == str(cat3_b) else 0
    
    # ç»¼åˆåˆ†ç±»ç›¸ä¼¼åº¦ï¼ˆä¸€çº§åˆ†ç±»æƒé‡æ›´é«˜ï¼‰
    category_similarity = cat1_similarity * 0.7 + cat3_similarity * 0.3

    return brand_similarity, category_similarity, specs_similarity, False  # ä¿æŒå‘åå…¼å®¹

# === å‚æ•°è¦†ç›–/é«˜å‡†ç¡®ç‡é¢„è®¾ ===
def _as_float(env_key: str, default: Optional[float]) -> Optional[float]:
    v = os.environ.get(env_key)
    if v is None:
        return default
    try:
        return float(v)
    except Exception:
        return default

def _as_int(env_key: str, default: Optional[int]) -> Optional[int]:
    v = os.environ.get(env_key)
    if v is None:
        return default
    try:
        return int(v)
    except Exception:
        return default

def override_match_params(params: dict, phase: str) -> dict:
    """å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡è¦†ç›–åŒ¹é…å‚æ•°ï¼›å¹¶æä¾›é«˜å‡†ç¡®ç‡é¢„è®¾ã€‚
    å¯ç”¨ç¯å¢ƒå˜é‡ï¼š
      - COMPARE_STRICT=1  å¯ç”¨é«˜å‡†ç¡®ç‡é¢„è®¾ï¼ˆæ›´çª„ä»·æ ¼çª—ã€æ›´é«˜é˜ˆå€¼ã€å¯å¼ºåˆ¶å“ç‰Œä¸€è‡´ï¼‰
      - MATCH_PRICE_WINDOW_HARD / MATCH_PRICE_WINDOW_SOFT ï¼ˆç™¾åˆ†æ¯”æ•´æ•°ï¼Œå¦‚ 15ï¼‰
      - MATCH_THRESHOLD_HARD / MATCH_THRESHOLD_SOFT ï¼ˆ0-1 æµ®ç‚¹ï¼‰
      - MATCH_TEXT_WEIGHT / MATCH_BRAND_WEIGHT / MATCH_CATEGORY_WEIGHT / MATCH_SPECS_WEIGHT
      - MATCH_REQUIRE_BRAND=1  å¼ºåˆ¶å“ç‰Œä¸€è‡´ï¼ˆå½“ä¸¤ä¾§å“ç‰Œå‡éç©ºä¸”éâ€œå…¶ä»–â€ï¼‰
    """
    phase_upper = (phase or '').upper()
    out = dict(params)

    # é¢„è®¾ï¼šé«˜å‡†ç¡®ç‡
    if os.environ.get('COMPARE_STRICT', '0') == '1':
        if phase_upper == 'HARD':
            out['price_similarity_percent'] = min(out.get('price_similarity_percent', 15), 12)
            out['composite_threshold'] = max(out.get('composite_threshold', 0.5), 0.6)
        else:  # SOFT
            out['price_similarity_percent'] = min(out.get('price_similarity_percent', 20), 15)
            out['composite_threshold'] = max(out.get('composite_threshold', 0.3), 0.55)  # ğŸ”§ å…¼å®¹é«˜è´¨é‡æ¨¡å‹(å¦‚BGE-M3)
        # æå‡æ–‡æœ¬/å“ç‰Œæƒé‡ï¼ˆæ›´ä¿å®ˆï¼‰
        out['text_weight'] = max(out.get('text_weight', 0.5), 0.6)
        out['brand_weight'] = max(out.get('brand_weight', 0.3), 0.35)
        out['require_brand_match'] = True

    # ç»†ç²’åº¦è¦†ç›–
    price_env = _as_int(f"MATCH_PRICE_WINDOW_{phase_upper}", None)
    if price_env is not None:
        out['price_similarity_percent'] = price_env
    thr_env = _as_float(f"MATCH_THRESHOLD_{phase_upper}", None)
    if thr_env is not None:
        out['composite_threshold'] = thr_env

    tw = _as_float('MATCH_TEXT_WEIGHT', None)
    bw = _as_float('MATCH_BRAND_WEIGHT', None)
    cw = _as_float('MATCH_CATEGORY_WEIGHT', None)
    sw = _as_float('MATCH_SPECS_WEIGHT', None)
    if tw is not None: out['text_weight'] = tw
    if bw is not None: out['brand_weight'] = bw
    if cw is not None: out['category_weight'] = cw
    if sw is not None: out['specs_weight'] = sw

    if os.environ.get('MATCH_REQUIRE_BRAND', '0') == '1':
        out['require_brand_match'] = True
    if os.environ.get('MATCH_REQUIRE_CAT3', '0') == '1':
        out['require_cat3_match'] = True
    if os.environ.get('MATCH_REQUIRE_SPECS', '0') == '1':
        out['require_specs_match'] = True
    mto = _as_int('MATCH_MIN_TOKEN_OVERLAP', None)
    if mto is not None:
        out['min_token_overlap'] = max(0, int(mto))

    return out

def load_and_process_store_data(filepath: str, model: Optional[SentenceTransformer], cache_path: str = None, role: Optional[str] = None) -> Tuple[pd.DataFrame, pd.DataFrame]:
    if not filepath or not os.path.exists(filepath):
        logging.error(f"æ–‡ä»¶è·¯å¾„æ— æ•ˆ: {filepath}")
        return pd.DataFrame(), pd.DataFrame()

    try:
        # å°è¯•å¤šç§ç¼–ç è¯»å– Excelï¼ˆä¿®å¤ GBK ç¼–ç é”™è¯¯ï¼‰
        try:
            df = pd.read_excel(filepath, engine='openpyxl')
        except Exception as e1:
            try:
                df = pd.read_excel(filepath, engine='xlrd')
            except Exception as e2:
                # å¦‚æœæ˜¯ CSV æ–‡ä»¶ï¼Œå°è¯•å¤šç§ç¼–ç 
                if filepath.lower().endswith('.csv'):
                    for encoding in ['utf-8', 'gbk', 'gb2312', 'utf-8-sig']:
                        try:
                            df = pd.read_csv(filepath, encoding=encoding)
                            break
                        except Exception:
                            continue
                    else:
                        raise Exception(f"æ— æ³•ç”¨ä»»ä½•ç¼–ç è¯»å– CSV æ–‡ä»¶: {e1}")
                else:
                    raise Exception(f"Excel è¯»å–å¤±è´¥: {e1}")
        
        # ğŸ”§ æ™ºèƒ½æ£€æµ‹å¤šè¡¨å¤´å’Œæ±‡æ€»è¡¨ï¼ˆä¿®å¤å¾å·é—®é¢˜é—¨åº—ç­‰ç‰¹æ®Šæ ¼å¼ï¼‰
        # æ£€æµ‹æ˜¯å¦å­˜åœ¨å¤§é‡ "Unnamed" åˆ—æˆ–ç¬¬ä¸€è¡Œæ˜¯æ±‡æ€»æ ‡é¢˜
        unnamed_count = sum(1 for col in df.columns if 'Unnamed' in str(col))
        if unnamed_count > 5 or (len(df) > 0 and any(keyword in str(df.iloc[0, 0]) for keyword in ['æ¦‚è§ˆ', 'æ±‡æ€»', 'ç»Ÿè®¡', 'é—¨åº—'])):
            logging.warning(f"æ£€æµ‹åˆ°å¤šè¡¨å¤´æˆ–æ±‡æ€»è¡¨æ ¼å¼ï¼Œå°è¯•æ™ºèƒ½è§£æ...")
            # å°è¯•è·³è¿‡å‰å‡ è¡Œæ‰¾åˆ°çœŸæ­£çš„æ•°æ®è¡¨å¤´
            for skip_rows in range(1, min(10, len(df))):
                try:
                    df_test = pd.read_excel(filepath, skiprows=skip_rows, engine='openpyxl')
                    # æ£€æŸ¥æ˜¯å¦æœ‰æ ‡å‡†åˆ—å
                    if 'å•†å“åç§°' in df_test.columns or 'å”®ä»·' in df_test.columns:
                        df = df_test
                        logging.info(f"âœ… æ™ºèƒ½è§£ææˆåŠŸï¼šè·³è¿‡å‰{skip_rows}è¡Œï¼Œæ‰¾åˆ°æ•°æ®è¡¨å¤´")
                        break
                except:
                    continue
            else:
                logging.error(f"âŒ æ— æ³•è§£æå¤šè¡¨å¤´æ ¼å¼ï¼Œè¯·æ£€æŸ¥æ–‡ä»¶: {filepath}")
                return pd.DataFrame(), pd.DataFrame()
                
    except Exception as e:
        logging.error(f"è¯»å–æ–‡ä»¶ {filepath} å¤±è´¥: {e}")
        return pd.DataFrame(), pd.DataFrame()

    # æ ‡å‡†åŒ–åˆ—åï¼šå»é™¤ç©ºæ ¼å’Œç‰¹æ®Šå­—ç¬¦
    df.columns = df.columns.str.strip()  # å»é™¤é¦–å°¾ç©ºæ ¼
    df.columns = df.columns.str.replace(r'\s+', '', regex=True)  # å»é™¤æ‰€æœ‰ç©ºæ ¼
    
    # ğŸ†• åˆ—ååˆ«åæ˜ å°„ï¼ˆå…¼å®¹ä¸åŒçš„åˆ—åæ ¼å¼ï¼‰
    column_aliases = {
        'è§„æ ¼åç§°': 'è§„æ ¼',
        'åº—å†…åˆ†ç±»': 'å•†å®¶åˆ†ç±»',
        'æ¡å½¢ç (upc/eanç­‰)': 'æ¡ç ',
        'æ¡å½¢ç ': 'æ¡ç ',
        'upc': 'æ¡ç ',
        'ean': 'æ¡ç ',
        'è´§å·': 'åº—å†…ç ',
        'åº—å†…è´§å·': 'åº—å†…ç ',
        'é‡‡è´­æˆæœ¬': 'æˆæœ¬',
        'è¿›è´§æˆæœ¬': 'æˆæœ¬',
        'æˆæœ¬ä»·': 'æˆæœ¬',
    }
    
    # åº”ç”¨åˆ«åæ˜ å°„
    df.rename(columns=column_aliases, inplace=True)
    
    # è°ƒè¯•ï¼šæ‰“å°å®é™…åˆ—å
    filename = os.path.basename(filepath)
    logging.info(f"ğŸ“‹ [{filename}] è¯»å–åˆ°çš„åˆ—å: {', '.join(df.columns.tolist())}")
    
    # å®šä¹‰å¿…éœ€åˆ—å’Œå¯é€‰åˆ—
    required_cols = ['å•†å“åç§°', 'åŸä»·', 'å”®ä»·']  # æ ¸å¿ƒå¿…éœ€åˆ—
    optional_cols = ['æ¡ç ', 'å•†å®¶åˆ†ç±»', 'æœˆå”®', 'åº“å­˜', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'åº—å†…ç ', 'è§„æ ¼', 'å•ä½', 'æˆæœ¬']  # å¯é€‰åˆ—ï¼ˆğŸ†• æ·»åŠ æˆæœ¬ï¼‰
    
    # æ£€æŸ¥å¿…éœ€åˆ—
    missing_required = [col for col in required_cols if col not in df.columns]
    if missing_required:
        logging.error(f"[{filename}] æ–‡ä»¶ç¼ºå°‘å¿…éœ€åˆ—: {missing_required}")
        return pd.DataFrame(), pd.DataFrame()
    
    # è‡ªåŠ¨è¡¥å……å¯é€‰åˆ—ï¼ˆå…è®¸æœ¬åº—å’Œç«å¯¹åˆ—ä¸ä¸€è‡´ï¼‰
    for col in optional_cols:
        if col not in df.columns:
            df[col] = np.nan
            logging.info(f"[{filename}] æ–‡ä»¶ä¸­ç¼ºå°‘ã€Œ{col}ã€åˆ—ï¼Œå·²è‡ªåŠ¨å¡«å……ä¸ºç©ºå€¼ã€‚")

    # æ¡ç ç»Ÿä¸€å½’ä¸€åŒ–ï¼š
    # - å»ç§‘å­¦è®¡æ•°æ³•ï¼ˆ1.234E+12 -> 1234000000000ï¼‰
    # - å»å°æ•°ï¼ˆ1234567890123.0 -> 1234567890123ï¼‰
    # - å»éæ•°å­—å­—ç¬¦
    # æ³¨æ„ï¼šè‹¥æºæ–‡ä»¶åœ¨ Excel ä¸­å·²ä»¥æ•°å­—æ ¼å¼ä¿å­˜ä¸”ä¸¢å¤±å‰å¯¼é›¶ï¼Œåˆ™æ— æ³•è¿˜åŸå‰å¯¼é›¶ï¼›å»ºè®®åœ¨æºæ–‡ä»¶ä¸­å°†æ¡ç åˆ—è®¾ä¸ºâ€œæ–‡æœ¬â€ã€‚
    def _normalize_barcode(v):
        if pd.isna(v):
            return np.nan
        s = str(v).strip()
        if not s:
            return np.nan
        # ç§‘å­¦è®¡æ•°æ³• -> åè¿›åˆ¶å­—ç¬¦ä¸²
        if 'e' in s.lower():
            try:
                s = format(Decimal(s), 'f')
            except Exception:
                pass
        # å»å°æ•°éƒ¨åˆ†
        if '.' in s:
            s = s.split('.')[0]
        # ä»…ä¿ç•™æ•°å­—
        s = re.sub(r'\D', '', s)
        return s or np.nan

    try:
        df['æ¡ç '] = df['æ¡ç '].apply(_normalize_barcode).astype('object')
    except Exception:
        # å…œåº•ï¼šå°½é‡ä¸è®©æ¡ç åˆ—å¯¼è‡´å´©æºƒ
        df['æ¡ç '] = df['æ¡ç '].astype(str).str.replace(r'\.0$', '', regex=True).str.strip()
        df['æ¡ç '].replace(['nan', 'None', ''], np.nan, inplace=True)
    df['cleaned_å•†å“åç§°'] = df['å•†å“åç§°'].apply(clean_text)
    df['standardized_brand'] = df.apply(lambda row: extract_brand(row['å•†å“åç§°'], row['å•†å®¶åˆ†ç±»']), axis=1)
    df['specs'] = df['å•†å“åç§°'].apply(extract_specs) # æ–°å¢ï¼šæå–è§„æ ¼

    # å…¼å®¹åˆ†ç±»å­—æ®µ
    df['ä¸€çº§åˆ†ç±»'] = df['ç¾å›¢ä¸€çº§åˆ†ç±»'].fillna(df['å•†å®¶åˆ†ç±»'].apply(lambda x: str(x).split('>')[0] if pd.notna(x) else ''))
    
    def get_cat3(row):
        if pd.notna(row['ç¾å›¢ä¸‰çº§åˆ†ç±»']):
            return row['ç¾å›¢ä¸‰çº§åˆ†ç±»']
        if pd.notna(row['å•†å®¶åˆ†ç±»']) and '>' in str(row['å•†å®¶åˆ†ç±»']):
            parts = str(row['å•†å®¶åˆ†ç±»']).split('>')
            return parts[2] if len(parts) > 2 else parts[-1]  # ä¼˜å…ˆå–ç¬¬ä¸‰çº§ï¼Œå¦åˆ™å–æœ€åä¸€çº§
        return ''
    df['ä¸‰çº§åˆ†ç±»'] = df.apply(get_cat3, axis=1)
    df['cleaned_ä¸€çº§åˆ†ç±»'] = df['ä¸€çº§åˆ†ç±»'].apply(clean_text)
    df['cleaned_ä¸‰çº§åˆ†ç±»'] = df['ä¸‰çº§åˆ†ç±»'].apply(clean_text)

    # === å‘é‡ç¼–ç å‰ï¼šå¯é€‰é¢„è¿‡æ»¤ï¼ˆæŒ‰ä¸€çº§åˆ†ç±»ï¼‰ä¸é‡‡æ ·ï¼Œå‡å°‘è®¡ç®—è§„æ¨¡ ===
    try:
        cat_list_env = os.environ.get('COMPARE_CAT1_LIST')
        cat_regex_env = os.environ.get('COMPARE_CAT1_REGEX')
        original_len = len(df)
        if cat_list_env:
            items = [s.strip().lower() for s in re.split(r'[;,ï¼Œï¼›]\s*', cat_list_env) if s.strip()]
            df = df[df['ä¸€çº§åˆ†ç±»'].astype(str).str.lower().isin(items)]
        if cat_regex_env:
            pattern = re.compile(cat_regex_env, flags=re.IGNORECASE)
            df = df[df['ä¸€çº§åˆ†ç±»'].astype(str).apply(lambda s: bool(pattern.search(s)))]
        if len(df) != original_len:
            logging.info(f"é¢„è¿‡æ»¤(ä¸€çº§åˆ†ç±»)åï¼š{original_len} -> {len(df)}")

        # é‡‡æ ·ä¸Šé™ï¼ˆä»…æµ‹è¯•ç”¨ï¼‰ï¼šCOMPARE_MAX_A / COMPARE_MAX_B
        if role:
            max_key = f"COMPARE_MAX_{role.upper()}"
            max_n = os.environ.get(max_key)
            if max_n and str(max_n).isdigit():
                n = int(max_n)
                if n > 0 and len(df) > n:
                    df = df.head(n)
                    logging.info(f"åº”ç”¨{max_key}={n}ï¼šæˆªå–å‰ {n} æ¡ç”¨äºå¿«é€Ÿæµ‹è¯•")
    except Exception as _:
        pass

    # --- å‘é‡ç”Ÿæˆä¸ç¼“å­˜ ---
    if SIMPLE_FALLBACK or model is None:
        logging.info("ç®€åŒ–å…œåº•æ¨¡å¼ï¼šè·³è¿‡å‘é‡ç¼–ç ï¼Œåç»­é‡‡ç”¨è½»é‡æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆæ— éœ€æ¨¡å‹ï¼‰")
        # æ”¾ä¸€ä¸ªå ä½åˆ—ï¼Œä¿æŒåç»­æµç¨‹ä¸æŠ¥é”™
        df['vector'] = [np.zeros(1)] * len(df)
    else:
        # ğŸ”§ è·å–æ¨¡å‹åç§°ç”¨äºç¼“å­˜é”®ï¼ˆç¡®ä¿ä¸åŒæ¨¡å‹ä½¿ç”¨ä¸åŒç¼“å­˜ï¼‰
        model_name = 'unknown'
        
        try:
            # SentenceTransformer çš„æ ‡å‡†ç»“æ„ï¼šmodel._modules['0'].auto_model.config._name_or_path
            if hasattr(model, '_modules') and '0' in model._modules:
                if hasattr(model._modules['0'], 'auto_model'):
                    model_name = model._modules['0'].auto_model.config._name_or_path
            # å¤‡é€‰æ–¹æ³•ï¼šä» model_name å±æ€§è·å–
            elif hasattr(model, 'model_name'):
                model_name = model.model_name
            # å¤‡é€‰æ–¹æ³•ï¼šä» _model_name å±æ€§è·å–
            elif hasattr(model, '_model_name'):
                model_name = model._model_name
        except Exception as e:
            logging.warning(f"æ— æ³•è·å–æ¨¡å‹åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼ 'unknown': {e}")
            model_name = 'unknown'
        
        # ç¼“å­˜é”®ï¼šéœ€è¦è§„èŒƒåŒ–è·¯å¾„
        model_identifier = model_name.replace('/', '_').replace('\\', '_')
        
        # æ—¥å¿—æ˜¾ç¤ºï¼šä¿æŒåŸå§‹æ¨¡å‹åç§°ï¼ˆæ›´å‹å¥½ï¼‰
        display_name = model_name if len(model_name) < 80 else model_name[:40] + "..." + model_name[-35:]
        logging.info(f"æ­£åœ¨ä¸ºã€Œ{os.path.basename(filepath)}ã€çš„å•†å“ç”Ÿæˆæ–‡æœ¬å‘é‡ (æ¨¡å‹: {display_name})...")
        
        texts = (df['cleaned_å•†å“åç§°'] + ' ' + df['cleaned_ä¸€çº§åˆ†ç±»'] + ' ' + df['cleaned_ä¸‰çº§åˆ†ç±»']).astype(str).tolist()

        #  ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨



                # ğŸš€ ä½¿ç”¨ç»Ÿä¸€çš„ç¼“å­˜ç®¡ç†å™¨
        texts_to_encode = []
        indices_to_encode = []
        final_embeddings = [None] * len(df)
        
        for i, text in enumerate(texts):
            cached_vector = cache_manager.get_embedding(model_identifier, text)
            if cached_vector is not None:
                final_embeddings[i] = cached_vector
            else:
                texts_to_encode.append(text)
                indices_to_encode.append(i)

        # ğŸ¯ æ˜¾ç¤ºç¼“å­˜å‘½ä¸­ç»Ÿè®¡
        cache_hit_count = len(df) - len(texts_to_encode)
        cache_hit_rate = (cache_hit_count / len(df) * 100) if len(df) > 0 else 0
        if cache_hit_count > 0:
            logging.info(f"ğŸ’¾ å‘é‡ç¼“å­˜å‘½ä¸­: {cache_hit_count}/{len(df)} æ¡ ({cache_hit_rate:.1f}%)")
        
        if texts_to_encode:
            logging.info(f"Cache miss {len(texts_to_encode)} items, computing new vectors...")
            
            # ğŸš€ ä¼˜åŒ–1: è‡ªåŠ¨è°ƒæ•´batch_sizeï¼ˆæ ¹æ®GPUæ˜¾å­˜ï¼‰
            optimal_batch_size = Config.ENCODE_BATCH_SIZE
            try:
                import torch
                if torch.cuda.is_available():
                    gpu_mem_gb = torch.cuda.get_device_properties(0).total_memory / 1024**3
                    if gpu_mem_gb >= 8:
                        optimal_batch_size = 256  # 8GB+ GPU
                    elif gpu_mem_gb >= 6:
                        optimal_batch_size = 64   # 6-8GB GPU (RTX 2060ï¼Œä¿å®ˆæ‰¹å¤§å°)
                    elif gpu_mem_gb >= 4:
                        optimal_batch_size = 48   # 4-6GB GPU
                    else:
                        optimal_batch_size = 32   # <4GB GPU
                    logging.info(f"GPU detected ({gpu_mem_gb:.1f}GB), optimal batch_size={optimal_batch_size}")
            except:
                pass
            
            t0 = time.perf_counter()
            # ğŸš€ ä¼˜åŒ–2: æ‰¹é‡ç¼–ç  + é¢„å½’ä¸€åŒ–
            new_embeddings = model.encode(
                texts_to_encode, 
                show_progress_bar=True, 
                batch_size=optimal_batch_size,
                convert_to_numpy=True,
                normalize_embeddings=True  # é¢„å½’ä¸€åŒ–ï¼ŒåŠ é€Ÿåç»­ä½™å¼¦ç›¸ä¼¼åº¦è®¡ç®—
            )
            t1 = time.perf_counter()
            
            # ğŸ§¹ æ¸…ç†GPUç¼“å­˜ï¼ˆé˜²æ­¢CUDAç´¯ç§¯é”™è¯¯ï¼‰
            try:
                import torch
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
                    torch.cuda.synchronize()
            except Exception:
                pass
            
            speed = len(texts_to_encode) / (t1 - t0)
            logging.info(f"Vector encoding complete: {len(texts_to_encode)} items in {t1 - t0:.2f}s ({speed:.1f} items/s, batch={optimal_batch_size})")
            
            for i, embedding in enumerate(new_embeddings):
                original_index = indices_to_encode[i]
                # ğŸ”§ ç¡®ä¿å‘é‡æ ¼å¼ç»Ÿä¸€ï¼šå±•å¹³ä¸ºä¸€ç»´æ•°ç»„
                vec = np.array(embedding).flatten()
                final_embeddings[original_index] = vec
                # ä¿å­˜åˆ°ç¼“å­˜
                cache_manager.set_embedding(model_identifier, texts[original_index], vec)
        else:
            logging.info(f"All vectors loaded from cache ({len(df)} items), encoding skipped")
        
        # ğŸ”§ ç¡®ä¿æ‰€æœ‰å‘é‡éƒ½æ˜¯ä¸€ç»´æ•°ç»„
        embeddings = [np.array(e).flatten() if e is not None else np.zeros(1) for e in final_embeddings]
        df['vector'] = list(embeddings)

    df_with_barcode = df[df['æ¡ç '].notna()].copy().drop_duplicates(subset=['æ¡ç '], keep='first')
    df_no_barcode = df[df['æ¡ç '].isna()].copy()

    logging.info(f"å¤„ç†å®Œæˆ: æ€»å•†å“ {len(df)} | æœ‰æ¡ç  {len(df_with_barcode)} | æ— æ¡ç  {len(df_no_barcode)}")
    return df_with_barcode, df_no_barcode

def check_model_exists(model_name: str) -> bool:
    """æ£€æŸ¥SentenceTransformeræ¨¡å‹æ˜¯å¦å·²ç¼“å­˜åˆ°æœ¬åœ°"""
    import torch
    from pathlib import Path
    
    # æ£€æŸ¥HuggingFace Hubç¼“å­˜ä½ç½®ï¼ˆæ–°ç‰ˆæœ¬çš„ç¼“å­˜ç»“æ„ï¼‰
    hub_cache_path = Path.home() / ".cache" / "huggingface" / "hub" / f"models--sentence-transformers--{model_name}"
    
    if hub_cache_path.exists():
        # æ£€æŸ¥æ˜¯å¦æœ‰snapshotsç›®å½•å’Œæ¨¡å‹æ–‡ä»¶
        snapshots_dir = hub_cache_path / "snapshots"
        if snapshots_dir.exists():
            # æŸ¥æ‰¾ä»»ä½•å­ç›®å½•ä¸­çš„æ¨¡å‹æ–‡ä»¶
            for snapshot_dir in snapshots_dir.iterdir():
                if snapshot_dir.is_dir():
                    model_files = list(snapshot_dir.glob("*.safetensors")) + list(snapshot_dir.glob("*.bin")) + list(snapshot_dir.glob("pytorch_model.bin"))
                    if model_files:
                        return True
    
    # æ£€æŸ¥æ—§ç‰ˆæœ¬çš„ç¼“å­˜ä½ç½®
    old_cache_paths = [
        Path.home() / ".cache" / "torch" / "sentence_transformers" / model_name.replace("/", "_"),
        Path.home() / ".cache" / "huggingface" / "transformers" / model_name.replace("/", "_"),
    ]
    
    for path in old_cache_paths:
        if path.exists() and (list(path.glob("*.bin")) or list(path.glob("*.safetensors"))):
            return True
    
    return False

# ==============================================================================
# 4. ä¸»æµç¨‹å‡½æ•°
# ==============================================================================

def get_local_model_path(model_name: str) -> str:
    """
    åœ¨æ‰“åŒ…ç¯å¢ƒä¸‹ï¼Œè¿”å›æœ¬åœ°æ¨¡å‹çš„å®é™…è·¯å¾„ï¼›å¦åˆ™è¿”å›æ¨¡å‹åç§°
    """
    global BUNDLED_MODEL_CACHE
    if BUNDLED_MODEL_CACHE and BUNDLED_MODEL_CACHE.exists():
        # æ‰“åŒ…ç¯å¢ƒï¼Œå°è¯•ä½¿ç”¨æœ¬åœ°æ¨¡å‹
        hub_path = BUNDLED_MODEL_CACHE / 'hub'
        # å°†æ¨¡å‹åç§°è½¬æ¢ä¸ºç›®å½•åæ ¼å¼ï¼Œå¦‚ BAAI/bge-large-zh-v1.5 -> models--BAAI--bge-large-zh-v1.5
        model_dir_name = 'models--' + model_name.replace('/', '--')
        local_model_dir = hub_path / model_dir_name
        
        if local_model_dir.exists():
            # æ‰¾åˆ°refs/mainè·å–snapshot
            refs_file = local_model_dir / 'refs' / 'main'
            if refs_file.exists():
                snapshot_id = refs_file.read_text().strip()
                snapshot_path = local_model_dir / 'snapshots' / snapshot_id
                if snapshot_path.exists():
                    print(f"ğŸ“ ä½¿ç”¨æ‰“åŒ…çš„æœ¬åœ°æ¨¡å‹: {snapshot_path}")
                    return str(snapshot_path)
        
        print(f"âš ï¸ æ‰“åŒ…çš„æ¨¡å‹ç›®å½•ä¸å®Œæ•´ï¼Œå›é€€åˆ°ä½¿ç”¨æ¨¡å‹åç§°: {model_name}")
    
    return model_name

def _normalize_filename_for_match(name: str) -> str:
    """
    ç”¨äºæ–‡ä»¶åå®½æ¾åŒ¹é…çš„å½’ä¸€åŒ–ï¼š
    - NFKC è§„æ•´ï¼ˆå…¨è§’->åŠè§’ï¼‰
    - å°å†™ã€å»é¦–å°¾ç©ºç™½
    - ç»Ÿä¸€çŸ­æ¨ªçº¿/ç ´æŠ˜å·ï¼šâ€”ã€â€“ã€ï¼ -> -
    - å»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
    - ç§»é™¤å¸¸è§æ‰©å±•ååç¼€ï¼ˆ.xlsx/.xlsï¼‰
    - ç»Ÿä¸€ä¸­æ–‡æ‹¬å·ä¸ºè‹±æ–‡æ‹¬å·
    """
    if not isinstance(name, str):
        name = str(name or "")
    s = unicodedata.normalize('NFKC', name).lower().strip()
    s = s.replace('â€”', '-').replace('â€“', '-').replace('ï¼', '-')
    s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')').replace('ã€', '[').replace('ã€‘', ']')
    # å»é™¤æ‰©å±•å
    s = re.sub(r"\.(xlsx|xls)$", "", s)
    # å»é™¤æ‰€æœ‰ç©ºç™½
    s = re.sub(r"\s+", "", s)
    return s

def get_adaptive_threshold(model_name: str, cfg, match_type: str = 'soft') -> float:
    """æ ¹æ®æ¨¡å‹è‡ªåŠ¨è·å–æ¨èé˜ˆå€¼
    
    Args:
        model_name: æ¨¡å‹åç§°
        cfg: é…ç½®å¯¹è±¡
        match_type: 'hard' æˆ– 'soft'
    
    Returns:
        æ¨èçš„é˜ˆå€¼
    """
    models = getattr(cfg, 'AVAILABLE_MODELS', {})
    
    # æŸ¥æ‰¾æ¨¡å‹å¯¹åº”çš„æ¨èé˜ˆå€¼
    for model_info in models.values():
        if model_info['name'] == model_name:
            thresholds = model_info.get('recommended_threshold', {'hard': 0.5, 'soft': 0.5})
            threshold = thresholds.get(match_type, 0.5)
            print(f"ğŸ“Š [{match_type.upper()}åŒ¹é…] å½“å‰æ¨¡å‹æ¨èé˜ˆå€¼: {threshold:.2f}")
            return threshold
    
    # é»˜è®¤é˜ˆå€¼
    print(f"âš ï¸ æœªæ‰¾åˆ°æ¨¡å‹é…ç½®ï¼Œä½¿ç”¨é»˜è®¤é˜ˆå€¼: 0.5")
    return 0.5

def select_embedding_model(cfg) -> str:
    """äº¤äº’å¼é€‰æ‹©åµŒå…¥æ¨¡å‹"""
    if not getattr(cfg, 'ENABLE_MODEL_SELECTION', True):
        return cfg.SENTENCE_BERT_MODEL
    
    # æ£€æŸ¥æ˜¯å¦é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šäº†æ¨¡å‹
    env_model = os.environ.get('SENTENCE_BERT_MODEL')
    if env_model:
        print(f"ğŸ”§ é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šæ¨¡å‹: {env_model}")
        return env_model
    
    # æ£€æµ‹æ ‡å‡†è¾“å…¥æ˜¯å¦å¯ç”¨ï¼ˆæ˜¯å¦è¢«é‡å®šå‘æˆ–ç®¡é“è¾“å…¥ï¼‰
    import sys
    
    # å¢åŠ ç¯å¢ƒå˜é‡æ£€æµ‹ï¼šGUIæ¨¡å¼ä¸‹å¼ºåˆ¶ä½¿ç”¨é»˜è®¤æ¨¡å‹
    if os.environ.get('GUI_MODE') == '1':
        model_mode = os.environ.get('MODEL_MODE', 'æœªçŸ¥')
        print(f"â„¹ï¸ æ£€æµ‹åˆ°GUIæ¨¡å¼ï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„æ¨¡å‹: {model_mode}")
        print(f"   åµŒå…¥æ¨¡å‹: {cfg.SENTENCE_BERT_MODEL}")
        return cfg.SENTENCE_BERT_MODEL
    
    if not sys.stdin or not hasattr(sys.stdin, 'isatty') or not sys.stdin.isatty():
        print("â„¹ï¸ æ£€æµ‹åˆ°éäº¤äº’å¼æ¨¡å¼ï¼ˆæ ‡å‡†è¾“å…¥è¢«é‡å®šå‘ï¼‰ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
        return cfg.SENTENCE_BERT_MODEL
    
    print("\n" + "="*70)
    print("ğŸ¤– åµŒå…¥æ¨¡å‹é€‰æ‹©")
    print("="*70)
    print("è¯·é€‰æ‹©ç”¨äºå•†å“æ¯”å¯¹çš„åµŒå…¥æ¨¡å‹ï¼š\n")
    
    models = getattr(cfg, 'AVAILABLE_MODELS', {})
    if not models:
        return cfg.SENTENCE_BERT_MODEL
    
    # æ˜¾ç¤ºæ¨¡å‹é€‰é¡¹
    for key, model_info in sorted(models.items()):
        print(f"  [{key}] {model_info['display_name']}")
        print(f"      ğŸ“ è¯´æ˜: {model_info['description']}")
        print(f"      ğŸ“¦ å¤§å°: {model_info['size']} | âš¡ é€Ÿåº¦: {model_info['speed']} | ğŸ¯ å‡†ç¡®ç‡: {model_info['accuracy']}")
        print()
    
    print("ğŸ’¡ æç¤º:")
    print("   - é€‰é¡¹4 (BGE-Large) â­ æœ€å¼ºæ€§èƒ½ï¼Œå‡†ç¡®ç‡æœ€é«˜ï¼Œé€‚åˆé«˜è´¨é‡è¦æ±‚åœºæ™¯")
    print("   - é€‰é¡¹5 (BGE-M3) æœ€æ–°ä¸€ä»£ï¼Œæ”¯æŒæ··åˆæ£€ç´¢ï¼Œå¤šè¯­è¨€åœºæ™¯æœ€ä¼˜")
    print("   - é€‰é¡¹2 (BGE-Base) æ€§èƒ½ä¸é€Ÿåº¦å¹³è¡¡ï¼Œæ¨èæ—¥å¸¸ä½¿ç”¨")
    print("   - é€‰é¡¹6 (BGE-Small) é€Ÿåº¦æœ€å¿«ï¼Œé€‚åˆå¤§æ‰¹é‡æ•°æ®å¤„ç†")
    if not getattr(sys, 'frozen', False):
        # ä»…åœ¨å¼€å‘ç¯å¢ƒæ˜¾ç¤ºä¸‹è½½æç¤º
        print("   - é¦–æ¬¡ä½¿ç”¨æ–°æ¨¡å‹éœ€è¦ä¸‹è½½ï¼Œçº¦5-30åˆ†é’Ÿï¼ˆè§†æ¨¡å‹å¤§å°ï¼‰")
    print()
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    while True:
        try:
            choice = input("è¯·è¾“å…¥æ¨¡å‹ç¼–å· (1-6, å›è½¦=ä½¿ç”¨é»˜è®¤æ¨¡å‹1): ").strip()
            
            # é»˜è®¤é€‰æ‹©
            if not choice:
                choice = '1'
                print(f"âœ… ä½¿ç”¨é»˜è®¤æ¨¡å‹: {models['1']['display_name']}")
            
            if choice in models:
                selected_model = models[choice]['name']
                print(f"\nâœ… å·²é€‰æ‹©: {models[choice]['display_name']}")
                print(f"   æ¨¡å‹è·¯å¾„: {selected_model}")
                print(f"   é¢„æœŸæ•ˆæœ: {models[choice]['description']}")
                print("="*70)
                return selected_model
            else:
                print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-{len(models)} ä¹‹é—´çš„æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·å–æ¶ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            return cfg.SENTENCE_BERT_MODEL
        except Exception as e:
            print(f"âŒ è¾“å…¥é”™è¯¯: {e}ï¼Œè¯·é‡æ–°è¾“å…¥")

def select_cross_encoder_model(cfg) -> str:
    """äº¤äº’å¼é€‰æ‹© Cross-Encoder ç²¾æ’æ¨¡å‹"""
    if not getattr(cfg, 'ENABLE_MODEL_SELECTION', True):
        return cfg.ONLINE_CROSS_ENCODER
    
    # æ£€æŸ¥æ˜¯å¦é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šäº†æ¨¡å‹
    env_model = os.environ.get('CROSS_ENCODER_MODEL')
    if env_model:
        print(f"ğŸ”§ é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®š Cross-Encoder æ¨¡å‹: {env_model}")
        return env_model
    
    # å¢åŠ GUIæ¨¡å¼æ£€æµ‹
    if os.environ.get('GUI_MODE') == '1':
        model_mode = os.environ.get('MODEL_MODE', 'æœªçŸ¥')
        print(f"â„¹ï¸ æ£€æµ‹åˆ°GUIæ¨¡å¼ï¼Œä½¿ç”¨ç”¨æˆ·é€‰æ‹©çš„ç²¾æ’æ¨¡å‹: {model_mode}")
        print(f"   ç²¾æ’æ¨¡å‹: {cfg.ONLINE_CROSS_ENCODER}")
        return cfg.ONLINE_CROSS_ENCODER
    
    # æ£€æµ‹æ ‡å‡†è¾“å…¥æ˜¯å¦å¯ç”¨
    import sys
    if not sys.stdin or not hasattr(sys.stdin, 'isatty') or not sys.stdin.isatty():
        print("â„¹ï¸ æ£€æµ‹åˆ°éäº¤äº’å¼æ¨¡å¼ï¼Œä½¿ç”¨é»˜è®¤ Cross-Encoder æ¨¡å‹")
        return cfg.ONLINE_CROSS_ENCODER
    
    print("\n" + "="*70)
    print("ğŸ¯ Cross-Encoder ç²¾æ’æ¨¡å‹é€‰æ‹©")
    print("="*70)
    print("Cross-Encoder ç”¨äºç²¾å‡†ç²¾æ’ï¼Œæå‡åŒ¹é…å‡†ç¡®ç‡\n")
    
    models = getattr(cfg, 'AVAILABLE_CROSS_ENCODERS', {})
    if not models:
        return cfg.ONLINE_CROSS_ENCODER
    
    # æ˜¾ç¤ºæ¨¡å‹é€‰é¡¹
    for key, model_info in sorted(models.items()):
        print(f"  [{key}] {model_info['display_name']}")
        print(f"      ğŸ“ è¯´æ˜: {model_info['description']}")
        print(f"      ğŸ“¦ å¤§å°: {model_info['size']} | âš¡ é€Ÿåº¦: {model_info['speed']} | ğŸ¯ å‡†ç¡®ç‡: {model_info['accuracy']}")
        print(f"      ğŸŒ è¯­è¨€: {model_info['language']}")
        print()
    
    print("ğŸ’¡ æç¤º:")
    print("   - é€‰é¡¹2 (BGE-Reranker-Large) â­ ä¸­æ–‡åœºæ™¯å¼ºåŠ›æ¨èï¼Œå‡†ç¡®ç‡+40%")
    print("   - é€‰é¡¹3 (BGE-Reranker-Base) âš¡ é€Ÿåº¦ä¸å‡†ç¡®ç‡å¹³è¡¡ï¼Œå‡†ç¡®ç‡+25%")
    print("   - é€‰é¡¹1 (MS-Marco-MiniLM) é€Ÿåº¦æœ€å¿«ï¼Œä½†ä¸­æ–‡æ•ˆæœä¸€èˆ¬")
    print("   - Cross-Encoder ä¸ Sentence-BERT é…åˆä½¿ç”¨ï¼Œä¸¤é˜¶æ®µåŒ¹é…æ›´ç²¾å‡†")
    if not getattr(sys, 'frozen', False):
        # ä»…åœ¨å¼€å‘ç¯å¢ƒæ˜¾ç¤ºä¸‹è½½æç¤º
        print("   - é¦–æ¬¡ä½¿ç”¨æ–°æ¨¡å‹éœ€è¦ä¸‹è½½ï¼Œçº¦1-10åˆ†é’Ÿï¼ˆè§†æ¨¡å‹å¤§å°ï¼‰")
    print()
    
    # è·å–ç”¨æˆ·é€‰æ‹©
    while True:
        try:
            choice = input(f"è¯·è¾“å…¥æ¨¡å‹ç¼–å· (1-{len(models)}, å›è½¦=ä½¿ç”¨é»˜è®¤æ¨¡å‹1): ").strip()
            
            # é»˜è®¤é€‰æ‹©
            if not choice:
                choice = '1'
                print(f"âœ… ä½¿ç”¨é»˜è®¤æ¨¡å‹: {models['1']['display_name']}")
            
            if choice in models:
                selected_model = models[choice]['name']
                print(f"\nâœ… å·²é€‰æ‹©: {models[choice]['display_name']}")
                print(f"   æ¨¡å‹è·¯å¾„: {selected_model}")
                print(f"   é¢„æœŸæ•ˆæœ: {models[choice]['description']}")
                print("="*70)
                return selected_model
            else:
                print(f"âŒ æ— æ•ˆé€‰æ‹©ï¼Œè¯·è¾“å…¥ 1-{len(models)} ä¹‹é—´çš„æ•°å­—")
        except KeyboardInterrupt:
            print("\n\nâš ï¸ ç”¨æˆ·å–æ¶ˆé€‰æ‹©ï¼Œä½¿ç”¨é»˜è®¤æ¨¡å‹")
            return cfg.ONLINE_CROSS_ENCODER
        except Exception as e:
            print(f"âŒ è¾“å…¥é”™è¯¯: {e}ï¼Œè¯·é‡æ–°è¾“å…¥")

def scan_excel_files_in_dir(directory: str) -> List[str]:
    """æ‰«ææŒ‡å®šç›®å½•ä¸­çš„æ‰€æœ‰Excelæ–‡ä»¶"""
    excel_files = []
    if not os.path.exists(directory):
        return excel_files
    try:
        for f in os.listdir(directory):
            if f.lower().endswith((".xlsx", ".xls")) and not f.startswith("~$"):
                excel_files.append(f)
    except Exception as e:
        logging.error(f"âŒ æ‰«æç›®å½• {directory} æ—¶å‡ºé”™ï¼š{e}")
    return sorted(excel_files)

def get_latest_file_from_upload_dir(upload_dir: str, store_type: str) -> Tuple[Optional[str], str]:
    """ä»ä¸Šä¼ ç›®å½•è·å–æœ€æ–°çš„Excelæ–‡ä»¶ï¼Œè¿”å›(æ–‡ä»¶è·¯å¾„, åº—é“ºåç§°)"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    full_upload_dir = os.path.join(base_dir, upload_dir)
    
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    if not os.path.exists(full_upload_dir):
        return None, ""
    
    excel_files = scan_excel_files_in_dir(full_upload_dir)
    
    if not excel_files:
        return None, ""
    
    # å¦‚æœæœ‰å¤šä¸ªæ–‡ä»¶ï¼Œé€‰æ‹©æœ€æ–°çš„
    if len(excel_files) > 1:
        files_with_time = []
        for f in excel_files:
            filepath = os.path.join(full_upload_dir, f)
            mtime = os.path.getmtime(filepath)
            files_with_time.append((f, mtime, filepath))
        
        # æŒ‰ä¿®æ”¹æ—¶é—´æ’åºï¼Œæœ€æ–°çš„åœ¨å‰
        files_with_time.sort(key=lambda x: x[1], reverse=True)
        latest_file = files_with_time[0][2]
        latest_filename = files_with_time[0][0]
        
        print(f"ğŸ“‹ {store_type}ä¸Šä¼ ç›®å½•å‘ç° {len(excel_files)} ä¸ªæ–‡ä»¶ï¼Œä½¿ç”¨æœ€æ–°æ–‡ä»¶: {latest_filename}")
        if len(excel_files) > 1:
            print(f"   ğŸ’¡ æç¤º: å…¶ä»–æ–‡ä»¶å°†è¢«å¿½ç•¥")
            for fname, mtime, _ in files_with_time[1:]:
                mtime_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(mtime))
                print(f"      - {fname} ({mtime_str})")
    else:
        latest_file = os.path.join(full_upload_dir, excel_files[0])
        latest_filename = excel_files[0]
    
    # ä»æ–‡ä»¶åæå–åº—é“ºåç§°
    store_name = os.path.splitext(latest_filename)[0][:40]
    
    return latest_file, store_name

def detect_files_from_upload_dirs(cfg) -> Tuple[Optional[str], Optional[str], str, str]:
    """ä»ä¸Šä¼ ç›®å½•æ£€æµ‹æ–‡ä»¶ï¼Œè¿”å›(æœ¬åº—æ–‡ä»¶è·¯å¾„, ç«å¯¹æ–‡ä»¶è·¯å¾„, æœ¬åº—åç§°, ç«å¯¹åç§°)"""
    base_dir = os.path.dirname(os.path.abspath(__file__))
    
    upload_a_dir = getattr(cfg, 'UPLOAD_DIR_STORE_A', 'upload/store_a')
    upload_b_dir = getattr(cfg, 'UPLOAD_DIR_STORE_B', 'upload/store_b')
    
    print("\n" + "="*60)
    print("ğŸ“‚ Upload Directory Detection")
    print("="*60)
    print(f"ğŸ“ Store A: {upload_a_dir}")
    print(f"ğŸ“ Store B: {upload_b_dir}")
    print()
    
    # æ£€æµ‹æœ¬åº—æ–‡ä»¶
    store_a_file, store_a_name = get_latest_file_from_upload_dir(upload_a_dir, "Store A")
    
    # æ£€æµ‹ç«å¯¹æ–‡ä»¶
    store_b_file, store_b_name = get_latest_file_from_upload_dir(upload_b_dir, "Store B")
    
    # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
    if store_a_file:
        size = os.path.getsize(store_a_file)
        size_str = f"{size/1024:.1f}KB" if size < 1024*1024 else f"{size/(1024*1024):.1f}MB"
        mtime = os.path.getmtime(store_a_file)
        mtime_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(mtime))
        print(f"âœ… Store A: {store_a_name}.xlsx ({size_str}, {mtime_str})")
    else:
        print(f"âŒ Store A: No Excel files found")
    
    if store_b_file:
        size = os.path.getsize(store_b_file)
        size_str = f"{size/1024:.1f}KB" if size < 1024*1024 else f"{size/(1024*1024):.1f}MB"
        mtime = os.path.getmtime(store_b_file)
        mtime_str = time.strftime("%Y-%m-%d %H:%M", time.localtime(mtime))
        print(f"âœ… Store B: {store_b_name}.xlsx ({size_str}, {mtime_str})")
    else:
        print(f"âŒ Store B: No Excel files found")
    
    print("="*60)
    
    return store_a_file, store_b_file, store_a_name, store_b_name

def get_local_filepath(filename: str) -> Optional[str]:
    current_directory = os.path.dirname(os.path.abspath(__file__))
    # 1) ç²¾ç¡®åŒ¹é…ï¼ˆåŒ…å«åŸæ‰©å±•åï¼‰
    filepath = os.path.join(current_directory, filename)
    if os.path.exists(filepath):
        logging.info(f"âœ… æ–‡ä»¶ '{filename}' å·²æ‰¾åˆ°ã€‚")
        return filepath

    # 2) å®½æ¾åŒ¹é…ï¼šåœ¨åŒç›®å½•æŸ¥æ‰¾ .xls/.xlsxï¼Œåšå½’ä¸€åŒ–æ¯”å¯¹ï¼ˆè·³è¿‡ ~$/ä¸´æ—¶æ–‡ä»¶ï¼‰
    try:
        target_norm = _normalize_filename_for_match(filename)
        candidates = [f for f in os.listdir(current_directory) if f.lower().endswith((".xlsx", ".xls")) and not f.startswith("~$")]
        # å…ˆå°è¯•å½’ä¸€åŒ–å®Œå…¨ç›¸ç­‰
        for cand in candidates:
            if _normalize_filename_for_match(cand) == target_norm:
                path = os.path.join(current_directory, cand)
                logging.info(f"âœ… æœªæ‰¾åˆ°ç²¾ç¡®åŒåï¼Œä½†æ‰¾åˆ°å½’ä¸€åŒ–åŒ¹é…æ–‡ä»¶ï¼š'{cand}'ï¼ˆç”± '{filename}' åŒ¹é…ï¼‰")
                return path
        # å†å°è¯•ç›¸ä¼¼åº¦åŒ¹é…ï¼ˆâ‰¥0.9ï¼‰
        ratios = [(cand, difflib.SequenceMatcher(None, _normalize_filename_for_match(cand), target_norm).ratio()) for cand in candidates]
        if ratios:
            best_cand, best_score = max(ratios, key=lambda x: x[1])
            if best_score >= 0.9:
                path = os.path.join(current_directory, best_cand)
                logging.warning(f"âš ï¸ ç²¾ç¡®æ–‡ä»¶æœªæ‰¾åˆ°ï¼Œä½¿ç”¨ç›¸ä¼¼æ–‡ä»¶ï¼š'{best_cand}'ï¼ˆç›¸ä¼¼åº¦ {best_score:.2f}ï¼Œç”± '{filename}' åŒ¹é…ï¼‰")
                return path
        logging.error(f"âŒ é”™è¯¯ï¼šæ–‡ä»¶ '{filename}' åœ¨è„šæœ¬æ‰€åœ¨ç›®å½•æœªæ‰¾åˆ°ï¼å½“å‰ç›®å½•ä¸ºï¼š{current_directory}ã€‚ç›®å½•å†…å¯ç”¨æ–‡ä»¶ï¼š{candidates}")
        return None
    except Exception as e:
        logging.error(f"âŒ åœ¨ç›®å½• '{current_directory}' æœç´¢æ–‡ä»¶æ—¶å‡ºé”™ï¼š{e}")
        return None

def match_by_barcode(df_a: pd.DataFrame, df_b: pd.DataFrame, name_a: str, name_b: str) -> pd.DataFrame:
    if df_a.empty or df_b.empty:
        return pd.DataFrame()

    merged = pd.merge(df_a, df_b, on='æ¡ç ', how='inner', suffixes=(f'_{name_a}', f'_{name_b}'))
    if merged.empty:
        return merged

    def _ensure_suffix(columns: Iterable[str], suffix: str) -> None:
        for col in columns:
            if col == 'æ¡ç ':
                continue
            target = f"{col}_{suffix}"
            if target in merged.columns:
                continue
            if col in merged.columns:
                merged.rename(columns={col: target}, inplace=True)

    _ensure_suffix(df_a.columns, name_a)
    _ensure_suffix(df_b.columns, name_b)

    # ä¸ºä¸¤ä¾§è¡¥å……å¸¦åç¼€çš„æ¡ç åˆ—ï¼Œä¾¿äºä¸‹æ¸¸ç¨³å®šå¼•ç”¨
    barcode_col_a = f"æ¡ç _{name_a}"
    barcode_col_b = f"æ¡ç _{name_b}"
    if barcode_col_a not in merged.columns:
        merged[barcode_col_a] = merged['æ¡ç ']
    if barcode_col_b not in merged.columns:
        merged[barcode_col_b] = merged['æ¡ç ']
    return merged

def perform_hard_category_matching(df_a: pd.DataFrame, df_b: pd.DataFrame, name_a: str, name_b: str, cross_encoder=None, cfg=None) -> Tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:
    """
    é˜¶æ®µä¸€ï¼šç¡¬åˆ†ç±»ä¼˜å…ˆåŒ¹é…
    - æ‰¾å‡ºAåº—å’ŒBåº—ä¸­ï¼Œä¸€çº§åˆ†ç±»å’Œä¸‰çº§åˆ†ç±»å®Œå…¨ä¸€è‡´çš„å•†å“ã€‚
    - åœ¨è¿™äº›åˆ†ç±»å®Œå…¨ç›¸åŒçš„åˆ†ç»„å†…ï¼Œè¿›è¡Œé«˜ç²¾åº¦çš„æ¨¡ç³ŠåŒ¹é…ã€‚
    - è¿”å›åŒ¹é…ä¸Šçš„å•†å“ï¼Œä»¥åŠAåº—å’ŒBåº—ä¸­æœªåŒ¹é…çš„å•†å“ã€‚
    """
    if df_a.empty or df_b.empty:
        return pd.DataFrame(), df_a, df_b

    # ç¡®ä¿åˆ†ç±»åˆ—å­˜åœ¨
    if 'ä¸€çº§åˆ†ç±»' not in df_a.columns or 'ä¸‰çº§åˆ†ç±»' not in df_a.columns or \
       'ä¸€çº§åˆ†ç±»' not in df_b.columns or 'ä¸‰çº§åˆ†ç±»' not in df_b.columns:
        logging.warning("âš ï¸ ç¡¬åˆ†ç±»åŒ¹é…é˜¶æ®µç¼ºå°‘åˆ†ç±»åˆ—ï¼Œè·³è¿‡æ­¤é˜¶æ®µã€‚")
        return pd.DataFrame(), df_a, df_b

    # åˆ›å»ºå”¯ä¸€çš„åˆ†ç±»ID
    df_a['category_id'] = df_a['ä¸€çº§åˆ†ç±»'].astype(str) + '_' + df_a['ä¸‰çº§åˆ†ç±»'].astype(str)
    df_b['category_id'] = df_b['ä¸€çº§åˆ†ç±»'].astype(str) + '_' + df_b['ä¸‰çº§åˆ†ç±»'].astype(str)

    # æ‰¾å‡ºå…±æœ‰çš„åˆ†ç±»ID
    common_categories = set(df_a['category_id']) & set(df_b['category_id'])
    logging.info(f"ç¡¬åˆ†ç±»åŒ¹é…ï¼šæ‰¾åˆ° {len(common_categories)} ä¸ªå…±åŒçš„å•†å“åˆ†ç±»ã€‚")

    all_hard_matches = []
    
    # è·å–è‡ªé€‚åº”é˜ˆå€¼
    adaptive_threshold = 0.5  # é»˜è®¤å€¼
    if cfg:
        adaptive_threshold = get_adaptive_threshold(cfg.SENTENCE_BERT_MODEL, cfg, match_type='hard')
    
    # å¤åˆ¶ä¸€ä»½å‚æ•°ç”¨äºç¡¬åŒ¹é…ï¼Œé€šå¸¸ç¡¬åŒ¹é…çš„é˜ˆå€¼å¯ä»¥æ›´é«˜
    hard_match_params = {
        "price_similarity_percent": 15,
        "composite_threshold": adaptive_threshold,  # ä½¿ç”¨è‡ªé€‚åº”é˜ˆå€¼
        "text_weight": 0.6, # æå‡æ–‡æœ¬æƒé‡
        "brand_weight": 0.3, # å“ç‰Œæƒé‡
        "specs_weight": 0.1, # è§„æ ¼æƒé‡
        "category_weight": 0.0, # ç¡¬åˆ†ç±»åŒ¹é…é˜¶æ®µï¼Œåˆ†ç±»å·²100%ç›¸åŒï¼Œæƒé‡ä¸º0
        "candidates_to_check": int(os.environ.get('MATCH_TOPK_HARD', '20')),
        "require_category_match": False, # åœ¨è¿™ä¸ªå‡½æ•°å†…éƒ¨ï¼Œåˆ†ç±»å·²ç»åŒ¹é…ï¼Œä¸éœ€è¦å†æ¬¡æ£€æŸ¥
        "require_cat3_match": False,  # âœ… ç¡¬åˆ†ç±»å·²ç»æŒ‰category_idåˆ†ç»„ï¼Œæ— éœ€äºŒæ¬¡æ£€æŸ¥
    }
    hard_match_params = override_match_params(hard_match_params, phase='HARD')

    # è®°å½•æ‰€æœ‰åœ¨ç¡¬åŒ¹é…ä¸­å¤„ç†è¿‡çš„å•†å“ç´¢å¼•
    matched_indices_a = set()
    matched_indices_b = set()

    for category in tqdm(common_categories, desc="Hard Category Match", dynamic_ncols=True, mininterval=0.5, file=sys.stdout, ascii=True):
        group_a = df_a[df_a['category_id'] == category]
        group_b = df_b[df_b['category_id'] == category]

        if group_a.empty or group_b.empty:
            continue

        # åœ¨åˆ†ç±»åˆ†ç»„å†…è¿›è¡Œæ¨¡ç³ŠåŒ¹é…
        # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯ä¸€ä¸ªé€šç”¨çš„åŒ¹é…æ ¸å¿ƒé€»è¾‘ï¼Œæˆ‘ä»¬æŠŠå®ƒå‘½åä¸º _core_fuzzy_match
        matches_in_group = _core_fuzzy_match(group_a, group_b, name_a, name_b, hard_match_params, cross_encoder)

        if not matches_in_group.empty:
            all_hard_matches.append(matches_in_group)
            
            # âœ… ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨åŸå§‹ç´¢å¼•ï¼ˆåŒ…æ‹¬è¢«å»é‡åˆ é™¤çš„CDå•†å“ï¼‰
            if 'all_matched_indices_a' in matches_in_group.attrs:
                matched_indices_a.update(matches_in_group.attrs['all_matched_indices_a'])
                matched_indices_b.update(matches_in_group.attrs['all_matched_indices_b'])
                print(f"   âœ… ä½¿ç”¨åŸå§‹ç´¢å¼•: æœ¬åº—{len(matches_in_group.attrs['all_matched_indices_a'])}ä¸ª, ç«å¯¹{len(matches_in_group.attrs['all_matched_indices_b'])}ä¸ª")
            else:
                # å…œåº•ï¼šä½¿ç”¨å»é‡åçš„ç´¢å¼•ï¼ˆæ—§é€»è¾‘ï¼‰
                matched_indices_a.update(matches_in_group[f'index_{name_a}'].tolist())
                matched_indices_b.update(matches_in_group[f'index_{name_b}'].tolist())

    if not all_hard_matches:
        return pd.DataFrame(), df_a.drop(columns=['category_id']), df_b.drop(columns=['category_id'])

    final_hard_matches = pd.concat(all_hard_matches, ignore_index=True)

    # æ‰¾å‡ºæœªåŒ¹é…çš„å•†å“
    unmatched_a = df_a[~df_a.index.isin(matched_indices_a)].copy()
    unmatched_b = df_b[~df_b.index.isin(matched_indices_b)].copy()

    # æ¸…ç†è¾…åŠ©åˆ—
    final_hard_matches = final_hard_matches.drop(columns=[f'index_{name_a}', f'index_{name_b}'], errors='ignore')
    unmatched_a = unmatched_a.drop(columns=['category_id'], errors='ignore')
    unmatched_b = unmatched_b.drop(columns=['category_id'], errors='ignore')
    
    return final_hard_matches, unmatched_a, unmatched_b


def perform_soft_fuzzy_matching(df_a: pd.DataFrame, df_b: pd.DataFrame, name_a: str, name_b: str, cross_encoder=None, cfg=None) -> pd.DataFrame:
    """
    é˜¶æ®µäºŒï¼šè½¯åˆ†ç±»å…œåº•åŒ¹é…
    - å¯¹æ‰€æœ‰åœ¨ç¡¬åˆ†ç±»åŒ¹é…ä¸­æœªæ‰¾åˆ°åŒ¹é…çš„å‰©ä½™å•†å“è¿›è¡ŒåŒ¹é…ã€‚
    - âœ… æ€§èƒ½ä¼˜åŒ–ï¼šæ”¹ä¸ºæŒ‰ä¸€çº§åˆ†ç±»åˆ†ç»„åŒ¹é…ï¼Œé¿å…å…¨é‡æ¯”å¯¹
    """
    if df_a.empty or df_b.empty:
        return pd.DataFrame()

    # ç¡®ä¿åˆ†ç±»åˆ—å­˜åœ¨
    if 'ä¸€çº§åˆ†ç±»' not in df_a.columns or 'ä¸€çº§åˆ†ç±»' not in df_b.columns:
        logging.warning("âš ï¸ è½¯åˆ†ç±»åŒ¹é…é˜¶æ®µç¼ºå°‘ä¸€çº§åˆ†ç±»åˆ—ï¼Œä½¿ç”¨å…¨é‡åŒ¹é…ï¼ˆæ€§èƒ½è¾ƒå·®ï¼‰ã€‚")
        return _perform_soft_match_without_grouping(df_a, df_b, name_a, name_b, cross_encoder, cfg)
    
    # âœ… æ€§èƒ½ä¼˜åŒ–ï¼šæŒ‰ä¸€çº§åˆ†ç±»åˆ†ç»„åŒ¹é…
    df_a['cat1_group'] = df_a['ä¸€çº§åˆ†ç±»'].astype(str)
    df_b['cat1_group'] = df_b['ä¸€çº§åˆ†ç±»'].astype(str)
    
    common_cat1 = set(df_a['cat1_group']) & set(df_b['cat1_group'])
    logging.info(f"è½¯åˆ†ç±»åŒ¹é…ï¼šæ‰¾åˆ° {len(common_cat1)} ä¸ªå…±åŒçš„ä¸€çº§åˆ†ç±»ï¼Œå°†åˆ†ç»„å¤„ç†ï¼ˆé¿å…å…¨é‡æ¯”å¯¹ï¼‰")
    
    all_soft_matches = []
    matched_indices_a = set()
    matched_indices_b = set()
    
    # è·å–è‡ªé€‚åº”é˜ˆå€¼
    adaptive_threshold = 0.5
    if cfg:
        adaptive_threshold = get_adaptive_threshold(cfg.SENTENCE_BERT_MODEL, cfg, match_type='soft')
    
    # è½¯åŒ¹é…å‚æ•°
    soft_match_params = {
        "price_similarity_percent": 20,
        "composite_threshold": adaptive_threshold,
        "text_weight": 0.5,
        "brand_weight": 0.3,
        "category_weight": 0.1,
        "specs_weight": 0.1,
        "candidates_to_check": int(os.environ.get('MATCH_TOPK_SOFT', '100')),
        "require_category_match": False,  # âœ… å·²åˆ†ç»„ï¼Œæ— éœ€å†æ£€æŸ¥ä¸€çº§åˆ†ç±»
        "require_cat3_match": True,  # ğŸ”§ å¼€å¯ä¸‰çº§åˆ†ç±»å¼ºåˆ¶åŒ¹é…
        "require_brand_match": False,  # å¯é€‰ï¼šè®¾ä¸ºTrueå¼ºåˆ¶å“ç‰Œä¸€è‡´
    }
    soft_match_params = override_match_params(soft_match_params, phase='SOFT')
    
    # æŒ‰ä¸€çº§åˆ†ç±»åˆ†ç»„åŒ¹é…
    for cat1 in tqdm(common_cat1, desc="Soft Category Match (Optimized)", dynamic_ncols=True, mininterval=0.5, file=sys.stdout, ascii=True):
        group_a = df_a[df_a['cat1_group'] == cat1]
        group_b = df_b[df_b['cat1_group'] == cat1]
        
        if group_a.empty or group_b.empty:
            continue
        
        # åœ¨åˆ†ç»„å†…åŒ¹é…ï¼ˆæ€§èƒ½æå‡ï¼šä» NÃ—M é™ä¸º nÃ—mï¼Œå…¶ä¸­ n,m << N,Mï¼‰
        matches_in_group = _core_fuzzy_match(group_a, group_b, name_a, name_b, soft_match_params, cross_encoder)
        
        if not matches_in_group.empty:
            all_soft_matches.append(matches_in_group)
            
            # âœ… ä¿®å¤ï¼šä¼˜å…ˆä½¿ç”¨åŸå§‹ç´¢å¼•
            if 'all_matched_indices_a' in matches_in_group.attrs:
                matched_indices_a.update(matches_in_group.attrs['all_matched_indices_a'])
                matched_indices_b.update(matches_in_group.attrs['all_matched_indices_b'])
            else:
                matched_indices_a.update(matches_in_group[f'index_{name_a}'].tolist())
                matched_indices_b.update(matches_in_group[f'index_{name_b}'].tolist())
    
    # === ğŸ”§ æ–¹æ¡ˆ2Cï¼šæ™ºèƒ½æ··åˆç­–ç•¥ - ä¸‰çº§åˆ†ç±»è¡¥å……åŒ¹é… ===
    enable_cat3_fallback = os.environ.get('ENABLE_CAT3_FALLBACK', '1') == '1'
    
    if enable_cat3_fallback and 'ä¸‰çº§åˆ†ç±»' in df_a.columns and 'ä¸‰çº§åˆ†ç±»' in df_b.columns:
        # æ‰¾å‡ºæœªåŒ¹é…çš„å•†å“
        unmatched_a = df_a[~df_a.index.isin(matched_indices_a)].copy()
        unmatched_b = df_b[~df_b.index.isin(matched_indices_b)].copy()
        
        # æ™ºèƒ½ç­›é€‰ï¼šåªå¯¹å¯èƒ½è¢«é”™è¯¯åˆ†ç±»çš„å•†å“è¿›è¡Œä¸‰çº§åˆ†ç±»åŒ¹é…
        def is_likely_misclassified(row):
            """åˆ¤æ–­å•†å“æ˜¯å¦å¯èƒ½è¢«é”™è¯¯åˆ†ç±»"""
            name = str(row.get('å•†å“åç§°', '')).lower()
            price = pd.to_numeric(row.get('åŸä»·', 0), errors='coerce')
            
            # æ¡ä»¶1: åŒ…å«çŸ¥åå“ç‰Œå…³é”®è¯
            brand_keywords = ['å¯å£å¯ä¹', 'ç™¾äº‹', 'åº·å¸ˆå‚…', 'ç»Ÿä¸€', 'é›€å·¢', 'ä¼Šåˆ©', 'è’™ç‰›', 'å†œå¤«å±±æ³‰', 
                            'å¨ƒå“ˆå“ˆ', 'è¾¾åˆ©å›­', 'å¥¥åˆ©å¥¥', 'ä¹äº‹', 'å«é¾™', 'ä¸‰åªæ¾é¼ ', 'è‰¯å“é“ºå­']
            has_brand = any(brand in name for brand in brand_keywords)
            
            # æ¡ä»¶2: ä»·æ ¼åœ¨å…¸å‹èŒƒå›´ï¼ˆæ’é™¤å¼‚å¸¸å•†å“ï¼‰
            normal_price = 1 <= price <= 100 if pd.notna(price) else True
            
            # æ¡ä»¶3: å•†å“åç§°é•¿åº¦é€‚ä¸­ï¼ˆæ’é™¤æè¿°è¿‡çŸ­æˆ–è¿‡é•¿çš„å¼‚å¸¸æ•°æ®ï¼‰
            name_len_ok = 5 <= len(name) <= 100
            
            return (has_brand or normal_price) and name_len_ok
        
        if not unmatched_a.empty and not unmatched_b.empty:
            candidates_a = unmatched_a[unmatched_a.apply(is_likely_misclassified, axis=1)]
            candidates_b = unmatched_b[unmatched_b.apply(is_likely_misclassified, axis=1)]
            
            if not candidates_a.empty and not candidates_b.empty:
                # æŒ‰ä¸‰çº§åˆ†ç±»åˆ†ç»„
                candidates_a['cat3_group'] = candidates_a['ä¸‰çº§åˆ†ç±»'].astype(str)
                candidates_b['cat3_group'] = candidates_b['ä¸‰çº§åˆ†ç±»'].astype(str)
                
                common_cat3 = set(candidates_a['cat3_group']) & set(candidates_b['cat3_group'])
                
                if common_cat3:
                    logging.info(f"ğŸ”§ ä¸‰çº§åˆ†ç±»è¡¥å……åŒ¹é…ï¼šæ‰¾åˆ° {len(common_cat3)} ä¸ªå…±åŒä¸‰çº§åˆ†ç±»ï¼Œå€™é€‰å•†å“ A:{len(candidates_a)} B:{len(candidates_b)}")
                    
                    cat3_matches = []
                    # ä¿®å¤è¿›åº¦æ¡æ˜¾ç¤ºï¼šæ·»åŠ  leave=True ç¡®ä¿å®Œæˆåä¿ç•™ï¼Œncols=80 å›ºå®šå®½åº¦
                    pbar = tqdm(common_cat3, desc="   L3 Category Supplement", 
                               ncols=100, mininterval=1.0, 
                               file=sys.stdout, leave=True, ascii=True)
                    
                    for cat3 in pbar:
                        group_a_cat3 = candidates_a[candidates_a['cat3_group'] == cat3]
                        group_b_cat3 = candidates_b[candidates_b['cat3_group'] == cat3]
                        
                        if group_a_cat3.empty or group_b_cat3.empty:
                            continue
                        
                        # ä½¿ç”¨ç›¸åŒçš„åŒ¹é…å‚æ•°ï¼Œä½†ä¸å¼ºåˆ¶ä¸€çº§åˆ†ç±»
                        cat3_params = soft_match_params.copy()
                        cat3_params['require_category_match'] = False  # å…è®¸ä¸€çº§åˆ†ç±»ä¸åŒ
                        cat3_params['require_cat3_match'] = True  # å¼ºåˆ¶ä¸‰çº§åˆ†ç±»ç›¸åŒ
                        
                        matches_cat3 = _core_fuzzy_match(group_a_cat3, group_b_cat3, name_a, name_b, cat3_params, cross_encoder)
                        
                        if not matches_cat3.empty:
                            cat3_matches.append(matches_cat3)
                    
                    pbar.close()  # æ˜¾å¼å…³é—­è¿›åº¦æ¡ï¼Œç¡®ä¿æ­£ç¡®æ¢è¡Œ
                    sys.stdout.flush()  # åˆ·æ–°è¾“å‡ºç¼“å†²
                    
                    if cat3_matches:
                        cat3_matches_df = pd.concat(cat3_matches, ignore_index=True)
                        cat3_matches_df = cat3_matches_df.drop(columns=[f'index_{name_a}', f'index_{name_b}'], errors='ignore')
                        all_soft_matches.append(cat3_matches_df)
                        logging.info(f"   âœ… ä¸‰çº§åˆ†ç±»è¡¥å……åŒ¹é…æˆåŠŸï¼šæ–°å¢ {len(cat3_matches_df)} æ¡è·¨ä¸€çº§åˆ†ç±»åŒ¹é…")
                
                candidates_a.drop(columns=['cat3_group'], errors='ignore', inplace=True)
                candidates_b.drop(columns=['cat3_group'], errors='ignore', inplace=True)
    
    if not all_soft_matches:
        # æ¸…ç†è¾…åŠ©åˆ—
        df_a.drop(columns=['cat1_group'], errors='ignore', inplace=True)
        df_b.drop(columns=['cat1_group'], errors='ignore', inplace=True)
        return pd.DataFrame()
    
    final_soft_matches = pd.concat(all_soft_matches, ignore_index=True)
    final_soft_matches = final_soft_matches.drop(columns=[f'index_{name_a}', f'index_{name_b}'], errors='ignore')
    
    # æ¸…ç†è¾…åŠ©åˆ—
    df_a.drop(columns=['cat1_group'], errors='ignore', inplace=True)
    df_b.drop(columns=['cat1_group'], errors='ignore', inplace=True)
    
    return final_soft_matches


def _perform_soft_match_without_grouping(df_a: pd.DataFrame, df_b: pd.DataFrame, name_a: str, name_b: str, cross_encoder=None, cfg=None) -> pd.DataFrame:
    """
    å…œåº•æ–¹æ¡ˆï¼šä¸åˆ†ç»„çš„å…¨é‡è½¯åŒ¹é…ï¼ˆæ€§èƒ½è¾ƒå·®ï¼Œä»…åœ¨ç¼ºå°‘åˆ†ç±»åˆ—æ—¶ä½¿ç”¨ï¼‰
    """
    adaptive_threshold = 0.5
    if cfg:
        adaptive_threshold = get_adaptive_threshold(cfg.SENTENCE_BERT_MODEL, cfg, match_type='soft')

    soft_match_params = {
        "price_similarity_percent": 20,
        "composite_threshold": adaptive_threshold,
        "text_weight": 0.5,
        "brand_weight": 0.3,
        "category_weight": 0.1,
        "specs_weight": 0.1,
        "candidates_to_check": int(os.environ.get('MATCH_TOPK_SOFT', '100')),
        "require_category_match": True,
        "require_cat3_match": True,
    }
    soft_match_params = override_match_params(soft_match_params, phase='SOFT')

    soft_matches = _core_fuzzy_match(df_a, df_b, name_a, name_b, soft_match_params, cross_encoder)
    
    if not soft_matches.empty:
        soft_matches = soft_matches.drop(columns=[f'index_{name_a}', f'index_{name_b}'], errors='ignore')

    return soft_matches


def _core_fuzzy_match(df_a: pd.DataFrame, df_b: pd.DataFrame, name_a: str, name_b: str, params: dict, cross_encoder=None) -> pd.DataFrame:
    """
    æ¨¡ç³ŠåŒ¹é…çš„æ ¸å¿ƒè®¡ç®—é€»è¾‘ï¼Œè¢«ç¡¬åŒ¹é…å’Œè½¯åŒ¹é…å…±åŒè°ƒç”¨ã€‚
    """
    if df_a.empty or df_b.empty:
        return pd.DataFrame()

    k = params.get('candidates_to_check', 50)
    matched_products = []

    # é¢„å¤„ç† B ä¾§æ•°å€¼åˆ—
    df_b_temp = df_b.copy()
    df_b_temp['åŸä»·_numeric'] = pd.to_numeric(df_b_temp['åŸä»·'], errors='coerce')

    use_simple = SIMPLE_FALLBACK or (df_a['vector'].iloc[0].shape == (1,))
    sim_matrix = None
    top_k_indices = None
    
    if not use_simple:
        # ğŸš€ P1: ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜ä¼˜åŒ–
        try:
            df_a_vectors = np.vstack([np.array(v).flatten() for v in df_a['vector']])
            df_b_vectors = np.vstack([np.array(v).flatten() for v in df_b['vector']])
            
            # å°è¯•ä»ç¼“å­˜è·å–ç›¸ä¼¼åº¦çŸ©é˜µ
            # æå–æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆå‡è®¾å‘é‡å·²ç»åŒ…å«æ¨¡å‹ä¿¡æ¯ï¼‰
            model_identifier = "default"  # é»˜è®¤å€¼
            if hasattr(cross_encoder, 'model_name'):
                model_identifier = cross_encoder.model_name.replace('/', '_').replace('\\', '_')
            
            # ä½¿ç”¨å•†å“ç´¢å¼•ä½œä¸ºç¼“å­˜é”®
            ids_a = df_a.index.tolist()
            ids_b = df_b.index.tolist()
            
            cached_matrix = cache_manager.get_similarity_matrix(model_identifier, ids_a, ids_b)
            
            if cached_matrix is not None:
                sim_matrix = cached_matrix
                logging.debug(f"âœ… ç›¸ä¼¼åº¦çŸ©é˜µç¼“å­˜å‘½ä¸­: {len(ids_a)}Ã—{len(ids_b)}")
            else:
                # è®¡ç®—æ–°çš„ç›¸ä¼¼åº¦çŸ©é˜µ
                sim_matrix = cosine_similarity(df_a_vectors, df_b_vectors)
                # ä¿å­˜åˆ°ç¼“å­˜
                cache_manager.set_similarity_matrix(model_identifier, ids_a, ids_b, sim_matrix)
                logging.debug(f"ğŸ’¾ ç›¸ä¼¼åº¦çŸ©é˜µå·²ç¼“å­˜: {len(ids_a)}Ã—{len(ids_b)}")
            
            top_k_indices = np.argsort(sim_matrix, axis=1)[:, -k:]
        except Exception as e:
            logging.warning(f"âš ï¸ å‘é‡ç›¸ä¼¼åº¦è®¡ç®—å¤±è´¥ï¼Œé™çº§ä¸ºé€å¯¹æ¯”è¾ƒ: {e}")
            use_simple = True
            top_k_indices = None

    for i in tqdm(range(len(df_a)), desc=f"Core Fuzzy Match ({name_a} vs {name_b})", leave=False, dynamic_ncols=True, mininterval=0.5, file=sys.stdout, ascii=True):
        row_a = df_a.iloc[i]
        price_a = pd.to_numeric(row_a['åŸä»·'], errors='coerce')
        if pd.isna(price_a) or price_a == 0:
            continue

        price_min = price_a * (1 - params['price_similarity_percent'] / 100)
        price_max = price_a * (1 + params['price_similarity_percent'] / 100)

        best_overall_score = -1
        best_match_row_b = None

        candidate_pairs = []
        valid_candidates = []

        if use_simple:
            # ç®€åŒ–æ¨¡å¼ï¼šå…ˆç”¨ä»·æ ¼+ï¼ˆå¯é€‰ï¼‰åˆ†ç±»ç­›é€‰ï¼Œå†ç”¨ difflib æ–‡æœ¬ç›¸ä¼¼åº¦å– Top-K
            mask = df_b_temp['åŸä»·_numeric'].between(price_min, price_max)
            if params.get("require_category_match", False):
                mask &= (df_b_temp['ä¸€çº§åˆ†ç±»'].astype(str) == str(row_a.get('ä¸€çº§åˆ†ç±»', '')))
            if params.get('require_cat3_match', False):
                mask &= (df_b_temp['ä¸‰çº§åˆ†ç±»'].astype(str) == str(row_a.get('ä¸‰çº§åˆ†ç±»','')))
            cand_df = df_b_temp[mask]
            if cand_df.empty:
                continue
            # è®¡ç®—æ–‡æœ¬ç›¸ä¼¼åº¦ï¼ˆdifflibï¼‰
            a_text = f"{row_a.get('cleaned_å•†å“åç§°','')} {row_a.get('cleaned_ä¸€çº§åˆ†ç±»','')} {row_a.get('cleaned_ä¸‰çº§åˆ†ç±»','')}"
            scores = []
            cand_rows = []
            for _, rb in cand_df.iterrows():
                b_text = f"{rb.get('cleaned_å•†å“åç§°','')} {rb.get('cleaned_ä¸€çº§åˆ†ç±»','')} {rb.get('cleaned_ä¸‰çº§åˆ†ç±»','')}"
                try:
                    s = difflib.SequenceMatcher(None, a_text, b_text).ratio()
                except Exception:
                    s = 0.0
                scores.append(s)
                cand_rows.append(rb)
            if not scores:
                continue
            # é€‰ Top-K
            order = np.argsort(np.array(scores))[-k:]
            valid_candidates = [cand_rows[idx] for idx in order]
            candidate_pairs = [[row_a['å•†å“åç§°'], r['å•†å“åç§°']] for r in valid_candidates]
        else:
            # ç²¾æ’ï¼šå¯¹ç²—ç­›å‡ºçš„å€™é€‰å•†å“è¿›è¡Œè¯¦ç»†æ‰“åˆ†
            for b_idx in top_k_indices[i]:
                row_b = df_b_temp.iloc[b_idx]
                # ä»·æ ¼è¿‡æ»¤
                if not (price_min <= row_b['åŸä»·_numeric'] <= price_max):
                    continue
                # æ–°å¢ï¼šå¼ºåˆ¶åˆ†ç±»è¿‡æ»¤ï¼ˆå¦‚æœå‚æ•°è¦æ±‚ï¼‰
                if params.get("require_category_match", False):
                    cat1_a = str(row_a.get('ä¸€çº§åˆ†ç±»', '')).strip()
                    cat1_b = str(row_b.get('ä¸€çº§åˆ†ç±»', '')).strip()
                    # è¦æ±‚ä¸¤ä¾§åˆ†ç±»éƒ½éç©ºä¸”å®Œå…¨åŒ¹é…
                    if not cat1_a or not cat1_b or cat1_a != cat1_b:
                        continue
                candidate_pairs.append([row_a['å•†å“åç§°'], row_b['å•†å“åç§°']])
                valid_candidates.append(row_b)

        # å¦‚æœè¦æ±‚å“ç‰Œ/ä¸‰çº§åˆ†ç±»/è§„æ ¼ä¸€è‡´ï¼Œåˆ™æå‰è¿‡æ»¤å€™é€‰
        if params.get('require_brand_match', False):
            def _brand_ok(ra, rb):
                ba = str(ra.get('standardized_brand') or '').strip().lower()
                bb = str(rb.get('standardized_brand') or '').strip().lower()
                if not ba or not bb or ba == 'å…¶ä»–' or bb == 'å…¶ä»–':
                    return False
                return ba == bb
            new_pairs = []
            new_valid = []
            for pair, rb in zip(candidate_pairs, valid_candidates):
                if _brand_ok(row_a, rb):
                    new_pairs.append(pair)
                    new_valid.append(rb)
            candidate_pairs, valid_candidates = new_pairs, new_valid

        if params.get('require_cat3_match', False) and candidate_pairs:
            new_pairs = []
            new_valid = []
            cat3a = str(row_a.get('ä¸‰çº§åˆ†ç±»',''))
            for pair, rb in zip(candidate_pairs, valid_candidates):
                if str(rb.get('ä¸‰çº§åˆ†ç±»','')) == cat3a:
                    new_pairs.append(pair)
                    new_valid.append(rb)
            candidate_pairs, valid_candidates = new_pairs, new_valid

        if params.get('require_specs_match', False) and candidate_pairs:
            new_pairs = []
            new_valid = []
            sa = str(row_a.get('specs') or '').strip()
            for pair, rb in zip(candidate_pairs, valid_candidates):
                sb = str(rb.get('specs') or '').strip()
                if sa and sb and sa == sb:
                    new_pairs.append(pair)
                    new_valid.append(rb)
            candidate_pairs, valid_candidates = new_pairs, new_valid

        # æœ€å°åˆ†è¯é‡å ï¼ˆåŸºäº cleaned_å•†å“åç§°ï¼‰ï¼Œç”¨äºè¿‡æ»¤è¯­ä¹‰å®Œå…¨ä¸ç›¸å¹²çš„æ¡ç›®
        min_overlap = int(params.get('min_token_overlap', 0) or 0)
        if min_overlap > 0 and candidate_pairs:
            a_tokens = set(tokenize_text(row_a.get('cleaned_å•†å“åç§°','')))
            new_pairs = []
            new_valid = []
            for pair, rb in zip(candidate_pairs, valid_candidates):
                b_tokens = set(tokenize_text(rb.get('cleaned_å•†å“åç§°','')))
                if len(a_tokens & b_tokens) >= min_overlap:
                    new_pairs.append(pair)
                    new_valid.append(rb)
            candidate_pairs, valid_candidates = new_pairs, new_valid

        if not candidate_pairs:
            continue

        # ğŸš€ P0: ä½¿ç”¨Cross-Encoderè¿›è¡Œç²¾æ’æ‰“åˆ†ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        if cross_encoder and not use_simple:
            # è·å–æ¨¡å‹æ ‡è¯†ç¬¦ï¼ˆæ”¯æŒå¤šç§ CrossEncoder ç»“æ„ï¼‰
            ce_model_identifier = "default"
            try:
                # æ–¹æ³•1: ä» model_name å±æ€§è·å–
                if hasattr(cross_encoder, 'model_name'):
                    ce_model_identifier = cross_encoder.model_name
                # æ–¹æ³•2: ä» config._name_or_path è·å–
                elif hasattr(cross_encoder, 'config') and hasattr(cross_encoder.config, '_name_or_path'):
                    ce_model_identifier = cross_encoder.config._name_or_path
                # æ–¹æ³•3: ä» _name_or_path è·å–
                elif hasattr(cross_encoder, '_name_or_path'):
                    ce_model_identifier = cross_encoder._name_or_path
                # æ–¹æ³•4: ä»æ¨¡å‹çš„ç¬¬ä¸€å±‚è·å–
                elif hasattr(cross_encoder, 'model') and hasattr(cross_encoder.model, 'config'):
                    ce_model_identifier = cross_encoder.model.config._name_or_path
            except Exception as e:
                logging.warning(f"æ— æ³•è·å– Cross-Encoder æ¨¡å‹åç§°ï¼Œä½¿ç”¨é»˜è®¤å€¼: {e}")
            
            ce_model_identifier = ce_model_identifier.replace('/', '_').replace('\\', '_')
            
            # æ‰¹é‡æ£€æŸ¥ç¼“å­˜
            cached_scores = []
            pairs_to_predict = []
            pairs_to_predict_indices = []
            
            for idx, pair in enumerate(candidate_pairs):
                text_a, text_b = pair[0], pair[1]
                cached_score = cache_manager.get_cross_encoder_score(ce_model_identifier, text_a, text_b)
                if cached_score is not None:
                    cached_scores.append((idx, cached_score))
                else:
                    pairs_to_predict.append(pair)
                    pairs_to_predict_indices.append(idx)
            
            # åˆå§‹åŒ–åˆ†æ•°æ•°ç»„
            raw_scores = [None] * len(candidate_pairs)
            
            # å¡«å……ç¼“å­˜å‘½ä¸­çš„åˆ†æ•°
            for idx, score in cached_scores:
                raw_scores[idx] = score
            
            # æ‰¹é‡é¢„æµ‹æœªç¼“å­˜çš„æ–‡æœ¬å¯¹
            if pairs_to_predict:
                new_scores = cross_encoder.predict(pairs_to_predict, show_progress_bar=False)
                
                # ğŸ§¹ æ¸…ç†GPUç¼“å­˜ï¼ˆé˜²æ­¢CUDAç´¯ç§¯é”™è¯¯ï¼‰
                try:
                    import torch
                    if torch.cuda.is_available():
                        torch.cuda.empty_cache()
                        torch.cuda.synchronize()
                except Exception:
                    pass
                
                for i, score in enumerate(new_scores):
                    original_idx = pairs_to_predict_indices[i]
                    raw_scores[original_idx] = score
                    # ä¿å­˜åˆ°ç¼“å­˜
                    text_a, text_b = pairs_to_predict[i]
                    cache_manager.set_cross_encoder_score(ce_model_identifier, text_a, text_b, float(score))
            
            # Sigmoidå½’ä¸€åŒ–
            text_scores = 1 / (1 + np.exp(-np.array(raw_scores)))
        else:
            # ç®€åŒ–æ¨¡å¼æˆ–æ—  CrossEncoderï¼š
            if use_simple:
                # å·²æŒ‰ difflib é€‰å‡ºå€™é€‰ï¼Œè¿™é‡Œå†æ¬¡å– difflib åˆ†æ•°ä½œä¸ºæ–‡æœ¬ç›¸ä¼¼åº¦
                a_text = f"{row_a.get('cleaned_å•†å“åç§°','')} {row_a.get('cleaned_ä¸€çº§åˆ†ç±»','')} {row_a.get('cleaned_ä¸‰çº§åˆ†ç±»','')}"
                text_scores = []
                for row_b in valid_candidates:
                    b_text = f"{row_b.get('cleaned_å•†å“åç§°','')} {row_b.get('cleaned_ä¸€çº§åˆ†ç±»','')} {row_b.get('cleaned_ä¸‰çº§åˆ†ç±»','')}"
                    try:
                        text_scores.append(difflib.SequenceMatcher(None, a_text, b_text).ratio())
                    except Exception:
                        text_scores.append(0.0)
            else:
                # ä½¿ç”¨å‘é‡ä½™å¼¦ç›¸ä¼¼åº¦
                text_scores = [sim_matrix[i, df_b.index.get_loc(row.name)] for row in valid_candidates]

        for idx, row_b in enumerate(valid_candidates):
            text_sim = text_scores[idx]
            
            # è®¡ç®—å“ç‰Œã€åˆ†ç±»ã€è§„æ ¼ç­‰ç‰¹å¾çš„ç›¸ä¼¼åº¦
            brand_sim, cat_sim, specs_sim, _ = calculate_feature_similarity(row_a, row_b)

            # è®¡ç®—ç»¼åˆå¾—åˆ†ï¼ˆå¯¹å“ç‰Œå®Œå…¨ä¸€è‡´ç»™äºˆè½»å¾®åŠ æˆï¼‰
            brand_bonus = 0.05 if (params.get('require_brand_match', False) and brand_sim == 1) else 0.0
            composite_score = (
                text_sim * params.get('text_weight', 0.6) +
                brand_sim * params.get('brand_weight', 0.2) +
                cat_sim * params.get('category_weight', 0.1) +
                specs_sim * params.get('specs_weight', 0.1) +
                brand_bonus
            )

            if composite_score > best_overall_score and composite_score >= params['composite_threshold']:
                best_overall_score = composite_score
                best_match_row_b = row_b

        if best_match_row_b is not None:
            match_info = {}
            for col in df_a.columns.difference(['vector', 'category_id']):
                match_info[f"{col}_{name_a}"] = row_a[col]
            for col in df_b.columns.difference(['vector', 'category_id', 'åŸä»·_numeric']):
                match_info[f"{col}_{name_b}"] = best_match_row_b[col]
            
            match_info['composite_similarity_score'] = best_overall_score
            # ä¿å­˜åŸå§‹ç´¢å¼•ï¼Œç”¨äºåç»­ä»æœªåŒ¹é…åˆ—è¡¨ä¸­æ’é™¤
            match_info[f'index_{name_a}'] = row_a.name
            match_info[f'index_{name_b}'] = best_match_row_b.name
            matched_products.append(match_info)

    # ğŸ”§ ã€ä¿®å¤ã€‘ç«å¯¹ä¾§å»é‡ï¼šè®°å½•æ‰€æœ‰åŸå§‹ç´¢å¼•ï¼Œé¿å…CDå•†å“è¢«è¯¯åˆ¤ä¸ºç‹¬æœ‰å•†å“
    matched_df = pd.DataFrame(matched_products)
    if not matched_df.empty and f'index_{name_b}' in matched_df.columns:
        before_dedup = len(matched_df)
        
        # âœ… å…³é”®ä¿®å¤ï¼šå»é‡å‰å…ˆè®°å½•æ‰€æœ‰åŸå§‹ç´¢å¼•ï¼ˆåŒ…æ‹¬å³å°†è¢«åˆ é™¤çš„CDå•†å“ï¼‰
        all_matched_a_indices = matched_df[f'index_{name_a}'].tolist()
        all_matched_b_indices = matched_df[f'index_{name_b}'].tolist()
        
        # æŒ‰å¾—åˆ†æ’åºï¼Œä¿ç•™æ¯ä¸ªç«å¯¹å•†å“çš„æœ€ä½³åŒ¹é…
        matched_df = matched_df.sort_values('composite_similarity_score', ascending=False)
        matched_df = matched_df.drop_duplicates(subset=[f'index_{name_b}'], keep='first')
        after_dedup = len(matched_df)
        
        if before_dedup > after_dedup:
            print(f"   ğŸ”§ ç«å¯¹ä¾§å»é‡: ç§»é™¤ {before_dedup - after_dedup} ä¸ªé‡å¤åŒ¹é…ï¼ˆä¿ç•™å¾—åˆ†æœ€é«˜çš„åŒ¹é…ï¼‰")
        
        # âœ… å°†åŸå§‹ç´¢å¼•ä¿å­˜ä¸ºDataFrameå±æ€§ï¼Œä¾›è°ƒç”¨æ–¹ä½¿ç”¨
        matched_df.attrs['all_matched_indices_a'] = all_matched_a_indices
        matched_df.attrs['all_matched_indices_b'] = all_matched_b_indices
    
    return matched_df

class DifferentialMatchConfig:
    """å·®å¼‚å“åŒ¹é…åŠ¨æ€æƒé‡é…ç½®"""
    
    # æ˜¯å¦å¼ºåˆ¶è¦æ±‚ä¸‰çº§åˆ†ç±»ä¸€è‡´ï¼ˆé»˜è®¤Falseï¼Œå…è®¸ä¸‰çº§åˆ†ç±»ä¸åŒä½†ä¼šæ ‡è®°è­¦å‘Šï¼‰
    REQUIRE_CAT3_MATCH = True  # ğŸ”§ å¼€å¯ä¸‰çº§åˆ†ç±»å¼ºåˆ¶åŒ¹é…ï¼Œä¸è½¯åˆ†ç±»é˜¶æ®µä¿æŒä¸€è‡´
    
    # å“ç±»æƒé‡é…ç½®
    CATEGORY_WEIGHTS = {
        # å·®å¼‚å“åŒ¹é…ï¼šå„å“ç±»é˜ˆå€¼é…ç½®ï¼ˆä¼˜åŒ–ï¼šæé«˜ä¸‹é™ï¼Œå‡å°‘ä¸ç›¸å…³åŒ¹é…ï¼‰
        # é¥®æ–™ç±»ï¼šå“ç‰Œå¤šï¼Œåç§°ç›¸ä¼¼ï¼Œä»·æ ¼æ•æ„Ÿ
        'é¥®æ–™': {
            'similarity_min': 0.42,  # ğŸ”§ æé«˜ä¸‹é™ 0.35â†’0.42
            'similarity_max': 0.65,
            'price_tolerance': 0.35,
            'description': 'å“ç‰Œä¼—å¤šï¼Œä»·æ ¼æ•æ„Ÿ'
        },
        # é›¶é£Ÿç±»ï¼šå“ç±»ç¹æ‚ï¼Œè§„æ ¼å¤šæ ·
        'ä¼‘é—²é£Ÿå“': {
            'similarity_min': 0.40,  # ğŸ”§ æé«˜ä¸‹é™ 0.30â†’0.40
            'similarity_max': 0.62,
            'price_tolerance': 0.40,
            'description': 'å“ç±»ç¹æ‚ï¼Œè§„æ ¼å¤šæ ·'
        },
        'ç²®æ²¹è°ƒå‘³': {
            'similarity_min': 0.42,  # ğŸ”§ æé«˜ä¸‹é™ 0.35â†’0.42
            'similarity_max': 0.65,
            'price_tolerance': 0.50,
            'description': 'å“ç‰Œå·®å¼‚å¤§ï¼Œä»·æ ¼èŒƒå›´å¹¿'
        },
        'æ–¹ä¾¿é£Ÿå“': {
            'similarity_min': 0.40,  # ğŸ”§ æé«˜ä¸‹é™ 0.30â†’0.40
            'similarity_max': 0.62,
            'price_tolerance': 0.45,
            'description': 'å“ç±»å¤šæ ·ï¼Œè§„æ ¼å·®å¼‚å¤§'
        },
        'ä¹³å“çƒ˜ç„™': {
            'similarity_min': 0.42,  # ğŸ”§ æé«˜ä¸‹é™ 0.35â†’0.42
            'similarity_max': 0.65,
            'price_tolerance': 0.40,
            'description': 'å“ç‰Œé›†ä¸­ï¼Œä»·æ ¼ç¨³å®š'
        },
        # æ—¥ç”¨å“ç±»ï¼šå“ç‰Œå·®å¼‚å¤§ï¼ŒåŠŸèƒ½ç›¸ä¼¼å³å¯
        'ä¸ªäººæŠ¤ç†': {
            'similarity_min': 0.40,  # ğŸ”§ æé«˜ä¸‹é™ 0.32â†’0.40
            'similarity_max': 0.62,
            'price_tolerance': 0.40,
            'description': 'åŠŸèƒ½ç›¸ä¼¼å³å¯ï¼Œå“ç‰Œå·®å¼‚å¤§'
        },
        'å®¶å±…ç”¨å“': {
            'similarity_min': 0.40,  # ğŸ”§ æé«˜ä¸‹é™ 0.30â†’0.40
            'similarity_max': 0.62,
            'price_tolerance': 0.40,
            'description': 'åŠŸèƒ½å¯¼å‘ï¼Œä»·æ ¼å·®å¼‚å¤§'
        },
        'æ¸…æ´ç”¨å“': {
            'similarity_min': 0.40,  # ğŸ”§ æé«˜ä¸‹é™ 0.32â†’0.40
            'similarity_max': 0.62,
            'price_tolerance': 0.45,
            'description': 'åŠŸèƒ½ä¸»å¯¼ï¼Œå“ç‰Œå¤šæ ·'
        },
        # ç”Ÿé²œç±»ï¼šè§„æ ¼å’Œä»·æ ¼éƒ½å¾ˆæ•æ„Ÿ
        'æ°´æœ': {
            'similarity_min': 0.45,  # ğŸ”§ æé«˜ä¸‹é™ 0.40â†’0.45
            'similarity_max': 0.70,
            'price_tolerance': 0.50,
            'description': 'å­£èŠ‚æ€§å¼ºï¼Œä»·æ ¼æ³¢åŠ¨å¤§'
        },
        'è”¬èœ': {
            'similarity_min': 0.45,  # ğŸ”§ æé«˜ä¸‹é™ 0.40â†’0.45
            'similarity_max': 0.70,
            'price_tolerance': 0.50,
            'description': 'å­£èŠ‚æ€§å¼ºï¼Œä»·æ ¼æ³¢åŠ¨å¤§'
        },
        'è‚‰ç¦½è›‹': {
            'similarity_min': 0.42,  # ğŸ”§ æé«˜ä¸‹é™ 0.38â†’0.42
            'similarity_max': 0.68,
            'price_tolerance': 0.45,
            'description': 'å“ç±»æ˜ç¡®ï¼Œä»·æ ¼æ•æ„Ÿ'
        },
        'æµ·é²œæ°´äº§': {
            'similarity_min': 0.42,  # ğŸ”§ æé«˜ä¸‹é™ 0.35â†’0.42
            'similarity_max': 0.65,
            'price_tolerance': 0.50,
            'description': 'è§„æ ¼å·®å¼‚å¤§ï¼Œä»·æ ¼æ³¢åŠ¨'
        },
        # é»˜è®¤é…ç½®
        'default': {
            'similarity_min': 0.40,  # ğŸ”§ æé«˜ä¸‹é™ 0.32â†’0.40
            'similarity_max': 0.65,
            'price_tolerance': 0.40,
            'description': 'æœªåˆ†ç±»å•†å“é»˜è®¤ç­–ç•¥'
        }
    }
    
    @classmethod
    def get_config(cls, category):
        """
        è·å–å“ç±»é…ç½®
        æ”¯æŒæ¨¡ç³ŠåŒ¹é…ï¼šå¦‚æœç²¾ç¡®åŒ¹é…å¤±è´¥ï¼Œå°è¯•åŒ…å«åŒ¹é…
        """
        # ç²¾ç¡®åŒ¹é…
        if category in cls.CATEGORY_WEIGHTS:
            return cls.CATEGORY_WEIGHTS[category]
        
        # æ¨¡ç³ŠåŒ¹é…ï¼ˆåŒ…å«å…³ç³»ï¼‰
        for key, config in cls.CATEGORY_WEIGHTS.items():
            if key in str(category) or str(category) in key:
                return config
        
        # è¿”å›é»˜è®¤é…ç½®
        return cls.CATEGORY_WEIGHTS['default']
    
    @classmethod
    def get_config_info(cls, category):
        """è·å–é…ç½®è¯´æ˜"""
        config = cls.get_config(category)
        return f"ç›¸ä¼¼åº¦[{config['similarity_min']}-{config['similarity_max']}], ä»·æ ¼Â±{int(config['price_tolerance']*100)}%"

def deduplicate_unique_products(df, store_name):
    """
    å¯¹ç‹¬æœ‰å•†å“æŒ‰å•†å“åç§°å»é‡
    
    Args:
        df: ç‹¬æœ‰å•†å“DataFrame
        store_name: åº—é“ºåç§°
    
    Returns:
        å»é‡åçš„DataFrameï¼ŒåŒ…å«SKUæ•°é‡ç»Ÿè®¡
    """
    if df.empty:
        return df
    
    # æŒ‰å•†å“åç§°åˆ†ç»„ç»Ÿè®¡
    agg_dict = {
        'å”®ä»·': 'first',  # ä¿ç•™ç¬¬ä¸€æ¡è®°å½•çš„å”®ä»·
        'åŸä»·': 'first',
        'ç¾å›¢ä¸€çº§åˆ†ç±»': 'first',
        'ç¾å›¢ä¸‰çº§åˆ†ç±»': 'first',
        'æ¡ç ': lambda x: ', '.join([str(b) for b in x.dropna().unique() if str(b) != 'nan']),  # åˆå¹¶æ¡ç 
        'åº“å­˜': 'sum',  # åº“å­˜æ±‚å’Œ
        'æœˆå”®': 'sum',  # æœˆå”®æ±‚å’Œ
    }
    
    # â­å…³é”®ï¼šä¿ç•™vectoråˆ—ä¾›å·®å¼‚å“åˆ†æä½¿ç”¨
    if 'vector' in df.columns:
        agg_dict['vector'] = lambda x: x.iloc[0]  # ä¿ç•™ç¬¬ä¸€æ¡è®°å½•çš„å‘é‡ï¼ˆä¿æŒåŸæ ¼å¼ï¼‰
    
    grouped = df.groupby('å•†å“åç§°', as_index=False).agg(agg_dict)
    
    # æ·»åŠ SKUæ•°é‡åˆ—
    sku_counts = df.groupby('å•†å“åç§°').size().reset_index(name='SKUæ•°é‡')
    grouped = grouped.merge(sku_counts, on='å•†å“åç§°', how='left')
    
    # é‡æ–°æ’åºåˆ—
    cols_order = ['å•†å“åç§°', 'SKUæ•°é‡', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'å”®ä»·', 'åŸä»·', 'åº“å­˜', 'æœˆå”®', 'æ¡ç ']
    cols_order = [c for c in cols_order if c in grouped.columns]
    other_cols = [c for c in grouped.columns if c not in cols_order]
    grouped = grouped[cols_order + other_cols]
    
    # æŒ‰åˆ†ç±»å’Œå”®ä»·æ’åº
    if 'ç¾å›¢ä¸€çº§åˆ†ç±»' in grouped.columns:
        grouped = grouped.sort_values(['ç¾å›¢ä¸€çº§åˆ†ç±»', 'å”®ä»·'], ascending=[True, False])
    
    print(f"   å»é‡å‰: {len(df)} æ¡ï¼Œå»é‡å: {len(grouped)} æ¡ç‹¬æœ‰å•†å“")
    return grouped

def find_differential_products(df_a_unique, df_b_unique, name_a, name_b, cfg=None):
    """
    å·®å¼‚å“åˆ†æï¼šåœ¨ç‹¬æœ‰å•†å“ä¸­æ‰¾åŒåˆ†ç±»ã€ä»·æ ¼ç›¸ä¼¼ä½†ä¸å®Œå…¨ç›¸åŒçš„å•†å“
    
    Args:
        df_a_unique: æœ¬åº—ç‹¬æœ‰å•†å“
        df_b_unique: ç«å¯¹ç‹¬æœ‰å•†å“
        name_a: æœ¬åº—åç§°
        name_b: ç«å¯¹åç§°
        cfg: é…ç½®å¯¹è±¡ï¼ˆç”¨äºè·å–å‘é‡æ¨¡å‹ï¼‰
    
    Returns:
        å·®å¼‚å“å¯¹æ¯”DataFrame
    """
    if df_a_unique.empty or df_b_unique.empty:
        return pd.DataFrame()
    
    print(f"\nğŸ” å¼€å§‹å·®å¼‚å“åˆ†æ...")
    print(f"   æœ¬åº—ç‹¬æœ‰: {len(df_a_unique)} æ¡ï¼Œç«å¯¹ç‹¬æœ‰: {len(df_b_unique)} æ¡")
    print(f"   åŒ¹é…æ¨¡å¼: {'âœ…ä¸€å¯¹ä¸€æœ€ä½³åŒ¹é…' if True else 'å¤šå¯¹å¤š'} | ä¸‰çº§åˆ†ç±»: {'âš ï¸å¼ºåˆ¶ä¸€è‡´' if DifferentialMatchConfig.REQUIRE_CAT3_MATCH else 'âœ…å…è®¸ä¸åŒ(æ ‡è®°è­¦å‘Š)'}")
    
    differential_matches = []
    
    # ç¡®ä¿å¿…è¦çš„åˆ—å­˜åœ¨
    required_cols = ['å•†å“åç§°', 'å”®ä»·', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'vector']
    for col in required_cols:
        if col not in df_a_unique.columns or col not in df_b_unique.columns:
            print(f"   âš ï¸ ç¼ºå°‘å¿…è¦åˆ— '{col}'ï¼Œè·³è¿‡å·®å¼‚å“åˆ†æ")
            return pd.DataFrame()
    
    # è¯Šæ–­ä¿¡æ¯ï¼šæ£€æŸ¥å…±åŒåˆ†ç±»
    cats_a = set(df_a_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].dropna().unique())
    cats_b = set(df_b_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].dropna().unique())
    common_cats = cats_a & cats_b
    print(f"   ä¸€çº§åˆ†ç±»: Aåº—{len(cats_a)}ä¸ª, Båº—{len(cats_b)}ä¸ª, å…±åŒ{len(common_cats)}ä¸ª")
    
    # æ£€æŸ¥ä¸‰çº§åˆ†ç±»è¦†ç›–
    if 'ç¾å›¢ä¸‰çº§åˆ†ç±»' in df_a_unique.columns and 'ç¾å›¢ä¸‰çº§åˆ†ç±»' in df_b_unique.columns:
        cats3_a = set(df_a_unique['ç¾å›¢ä¸‰çº§åˆ†ç±»'].dropna().unique())
        cats3_b = set(df_b_unique['ç¾å›¢ä¸‰çº§åˆ†ç±»'].dropna().unique())
        common_cats3 = cats3_a & cats3_b
        print(f"   ä¸‰çº§åˆ†ç±»: Aåº—{len(cats3_a)}ä¸ª, Båº—{len(cats3_b)}ä¸ª, å…±åŒ{len(common_cats3)}ä¸ª")
    
    if not common_cats:
        print(f"   âš ï¸ æ²¡æœ‰å…±åŒçš„ä¸€çº§åˆ†ç±»ï¼Œæ— æ³•åŒ¹é…å·®å¼‚å“")
        return pd.DataFrame()
    
    # æ™ºèƒ½ä»·æ ¼é€‰æ‹©ï¼šä¼˜å…ˆä½¿ç”¨åŸä»·ï¼ŒåŸä»·æ— æ•ˆåˆ™ä½¿ç”¨å”®ä»·
    df_a_unique = df_a_unique.copy()
    df_b_unique = df_b_unique.copy()
    
    # è½¬æ¢åŸä»·å’Œå”®ä»·ä¸ºæ•°å€¼ï¼ˆä½¿ç”¨.get()å®‰å…¨è·å–åˆ—ï¼Œé¿å…KeyErrorï¼‰
    if 'åŸä»·' in df_a_unique.columns:
        df_a_unique['åŸä»·_numeric'] = pd.to_numeric(df_a_unique['åŸä»·'], errors='coerce')
    else:
        df_a_unique['åŸä»·_numeric'] = pd.NA
    df_a_unique['å”®ä»·_numeric'] = pd.to_numeric(df_a_unique['å”®ä»·'], errors='coerce')
    
    if 'åŸä»·' in df_b_unique.columns:
        df_b_unique['åŸä»·_numeric'] = pd.to_numeric(df_b_unique['åŸä»·'], errors='coerce')
    else:
        df_b_unique['åŸä»·_numeric'] = pd.NA
    df_b_unique['å”®ä»·_numeric'] = pd.to_numeric(df_b_unique['å”®ä»·'], errors='coerce')
    
    # æ™ºèƒ½ä»·æ ¼é€‰æ‹©ï¼šåŸä»· > 0 ä¼˜å…ˆï¼Œå¦åˆ™ç”¨å”®ä»·
    df_a_unique['å¯¹æ¯”ä»·æ ¼'] = df_a_unique.apply(
        lambda row: row['åŸä»·_numeric'] if (pd.notna(row['åŸä»·_numeric']) and row['åŸä»·_numeric'] > 0) else row['å”®ä»·_numeric'],
        axis=1
    )
    df_a_unique['ä»·æ ¼æ¥æº'] = df_a_unique.apply(
        lambda row: 'åŸä»·' if (pd.notna(row['åŸä»·_numeric']) and row['åŸä»·_numeric'] > 0) else 'å”®ä»·',
        axis=1
    )
    
    df_b_unique['å¯¹æ¯”ä»·æ ¼'] = df_b_unique.apply(
        lambda row: row['åŸä»·_numeric'] if (pd.notna(row['åŸä»·_numeric']) and row['åŸä»·_numeric'] > 0) else row['å”®ä»·_numeric'],
        axis=1
    )
    df_b_unique['ä»·æ ¼æ¥æº'] = df_b_unique.apply(
        lambda row: 'åŸä»·' if (pd.notna(row['åŸä»·_numeric']) and row['åŸä»·_numeric'] > 0) else 'å”®ä»·',
        axis=1
    )
    
    # æŒ‰ä¸€çº§åˆ†ç±»åˆ†ç»„åŒ¹é…
    categories_a = df_a_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].unique()
    matched_count = 0
    
    # è°ƒè¯•ï¼šæ£€æŸ¥ä»·æ ¼æ•°æ®
    valid_price_a = df_a_unique['å¯¹æ¯”ä»·æ ¼'].notna() & (df_a_unique['å¯¹æ¯”ä»·æ ¼'] > 0)
    valid_price_b = df_b_unique['å¯¹æ¯”ä»·æ ¼'].notna() & (df_b_unique['å¯¹æ¯”ä»·æ ¼'] > 0)
    orig_count_a = (df_a_unique['ä»·æ ¼æ¥æº'] == 'åŸä»·').sum()
    orig_count_b = (df_b_unique['ä»·æ ¼æ¥æº'] == 'åŸä»·').sum()
    print(f"   ğŸ’° ä»·æ ¼æ£€æŸ¥: Aåº—æœ‰æ•ˆä»·æ ¼ {valid_price_a.sum()}/{len(df_a_unique)} (åŸä»·{orig_count_a}, å”®ä»·{valid_price_a.sum()-orig_count_a})")
    print(f"   ğŸ’° ä»·æ ¼æ£€æŸ¥: Båº—æœ‰æ•ˆä»·æ ¼ {valid_price_b.sum()}/{len(df_b_unique)} (åŸä»·{orig_count_b}, å”®ä»·{valid_price_b.sum()-orig_count_b})")
    
    print(f"   å¼€å§‹åˆ†ç±»åŒ¹é…ï¼ˆå…± {len(common_cats)} ä¸ªå…±åŒåˆ†ç±»ï¼‰...")
    
    # å¯¼å…¥è¿›åº¦æ¡
    from tqdm import tqdm
    
    # ä½¿ç”¨è¿›åº¦æ¡éå†åˆ†ç±»
    for idx, category in enumerate(tqdm(categories_a, desc="   Differential Analysis", ncols=100, ascii=True), 1):
        # è·å–è¯¥åˆ†ç±»çš„åŠ¨æ€æƒé‡é…ç½®
        config = DifferentialMatchConfig.get_config(category)
        config_info = DifferentialMatchConfig.get_config_info(category)
        
        # ğŸ”§ è°ƒè¯•ï¼šè¾“å‡ºå‰3ä¸ªåˆ†ç±»çš„é…ç½®ä¿¡æ¯
        if idx <= 3:
            tqdm.write(f"   ğŸ“‹ [{category}] é…ç½®: {config_info} (ç›¸ä¼¼åº¦èŒƒå›´: {config['similarity_min']:.2f}-{config['similarity_max']:.2f})")
        
        # ç­›é€‰åŒåˆ†ç±»å•†å“
        df_a_cat = df_a_unique[df_a_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'] == category].copy()
        df_b_cat = df_b_unique[df_b_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'] == category].copy()
        
        # è°ƒè¯•ï¼šæ£€æŸ¥å¯¹æ¯”ä»·æ ¼åˆ—æ˜¯å¦å­˜åœ¨
        if idx <= 3 and ('å¯¹æ¯”ä»·æ ¼' not in df_a_cat.columns or 'å¯¹æ¯”ä»·æ ¼' not in df_b_cat.columns):
            tqdm.write(f"   âš ï¸ [{category}] ç¼ºå°‘å¯¹æ¯”ä»·æ ¼åˆ—!")
            tqdm.write(f"       Aåˆ—: {[c for c in df_a_cat.columns if 'ä»·æ ¼' in c or 'ä»·' in c]}")
            tqdm.write(f"       Båˆ—: {[c for c in df_b_cat.columns if 'ä»·æ ¼' in c or 'ä»·' in c]}")
        
        if df_a_cat.empty or df_b_cat.empty:
            continue
        
        # è®¡ç®—å‘é‡ç›¸ä¼¼åº¦
        try:
            from sklearn.metrics.pairwise import cosine_similarity
            import numpy as np
            
            # è¯Šæ–­ï¼šæ£€æŸ¥vectoræ ¼å¼ï¼ˆä»…å‰3ä¸ªåˆ†ç±»ï¼‰
            if idx <= 3:
                sample_vec_a = df_a_cat['vector'].iloc[0] if len(df_a_cat) > 0 else None
                sample_vec_b = df_b_cat['vector'].iloc[0] if len(df_b_cat) > 0 else None
                tqdm.write(f"       ğŸ” Vectoræ ¼å¼: Aç±»å‹={type(sample_vec_a).__name__}, Bç±»å‹={type(sample_vec_b).__name__}")
                if sample_vec_a is not None:
                    if isinstance(sample_vec_a, (list, np.ndarray)):
                        tqdm.write(f"       ğŸ” Vectoré•¿åº¦: A={len(sample_vec_a)}, é¦–5å€¼={sample_vec_a[:5] if len(sample_vec_a)>=5 else sample_vec_a}")
            
            vectors_a = np.array(df_a_cat['vector'].tolist())
            vectors_b = np.array(df_b_cat['vector'].tolist())
            sim_matrix = cosine_similarity(vectors_a, vectors_b)
        except Exception as e:
            if idx <= 3:
                import traceback
                tqdm.write(f"   âš ï¸ [{category}] è®¡ç®—ç›¸ä¼¼åº¦å¤±è´¥: {e}")
                tqdm.write(f"       è¯¦ç»†é”™è¯¯: {traceback.format_exc()[:200]}")
            continue
        
        category_matches = 0
        debug_info = {
            'total_a': len(df_a_cat),
            'total_b': len(df_b_cat),
            'valid_price_a': 0,
            'valid_price_b': 0,
            'similarity_in_range': 0,
            'price_in_range': 0,
            'cat3_mismatch': 0,
            'final_matches': 0
        }
        
        # ç”¨äºè®°å½•æ¯ä¸ªå•†å“çš„æœ€ä½³åŒ¹é…ï¼ˆä¸€å¯¹ä¸€ï¼‰
        best_matches_a = {}  # {idx_a: (idx_b, similarity, match_record)}
        
        # éå†æœ¬åº—å•†å“ï¼Œæ‰¾å·®å¼‚å“
        for i, row_a in df_a_cat.iterrows():
            price_a = row_a['å¯¹æ¯”ä»·æ ¼']
            if pd.isna(price_a) or price_a <= 0:
                continue
            
            debug_info['valid_price_a'] += 1
            
            # ä½¿ç”¨åŠ¨æ€ä»·æ ¼èŒƒå›´
            price_min = price_a * (1 - config['price_tolerance'])
            price_max = price_a * (1 + config['price_tolerance'])
            
            # è·å–è¯¥å•†å“åœ¨ç›¸ä¼¼åº¦çŸ©é˜µä¸­çš„ç´¢å¼•
            idx_a = df_a_cat.index.get_loc(i)
            similarities = sim_matrix[idx_a]
            
            # æ‰¾åˆ°ç›¸ä¼¼åº¦åœ¨åŠ¨æ€èŒƒå›´å†…ä¸”ä»·æ ¼ç›¸ä¼¼çš„å•†å“
            for j, row_b in df_b_cat.iterrows():
                idx_b = df_b_cat.index.get_loc(j)
                similarity = similarities[idx_b]
                
                # ä½¿ç”¨åŠ¨æ€ç›¸ä¼¼åº¦èŒƒå›´æ£€æŸ¥
                if similarity < config['similarity_min'] or similarity > config['similarity_max']:
                    continue
                
                debug_info['similarity_in_range'] += 1
                
                # è°ƒè¯•ï¼šæ£€æŸ¥Båº—ä»·æ ¼æ•°æ®
                price_b = row_b.get('å¯¹æ¯”ä»·æ ¼', None)
                if price_b is None or pd.isna(price_b) or price_b <= 0:
                    # ç¬¬ä¸€æ¬¡é‡åˆ°æ—¶æ‰“å°è°ƒè¯•ä¿¡æ¯
                    if debug_info['similarity_in_range'] == 1 and idx <= 3:
                        tqdm.write(f"       âš ï¸ è°ƒè¯•: Båº—rowç¼ºå°‘æœ‰æ•ˆä»·æ ¼ - å¯¹æ¯”ä»·æ ¼={price_b}, åŸä»·={row_b.get('åŸä»·_numeric')}, å”®ä»·={row_b.get('å”®ä»·_numeric')}")
                    continue
                
                debug_info['valid_price_b'] += 1
                
                # ä»·æ ¼èŒƒå›´æ£€æŸ¥
                if price_b < price_min or price_b > price_max:
                    continue
                
                debug_info['price_in_range'] += 1
                
                # ä¸‰çº§åˆ†ç±»æ£€æŸ¥ï¼šä¼˜å…ˆåŒ¹é…ç›¸åŒä¸‰çº§åˆ†ç±»
                cat3_a = row_a.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', '')
                cat3_b = row_b.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', '')
                cat3_match = False
                cat3_warning = ''
                
                if cat3_a and cat3_b and str(cat3_a) != 'nan' and str(cat3_b) != 'nan':
                    if cat3_a == cat3_b:
                        cat3_match = True
                        cat3_warning = ''
                    else:
                        # ä¸‰çº§åˆ†ç±»ä¸åŒï¼Œä½†å…è®¸åŒ¹é…ï¼ˆå¯èƒ½æ˜¯åˆ†ç±»é”™è¯¯ï¼‰
                        cat3_match = False
                        cat3_warning = f'âš ï¸ä¸‰çº§åˆ†ç±»ä¸åŒ({cat3_a}â‰ {cat3_b})'
                        debug_info['cat3_mismatch'] += 1
                        # âš ï¸ å¦‚æœå¯ç”¨ä¸¥æ ¼ä¸‰çº§åˆ†ç±»åŒ¹é…ï¼Œè·³è¿‡ä¸ä¸€è‡´çš„å•†å“
                        if DifferentialMatchConfig.REQUIRE_CAT3_MATCH:
                            continue  # è·³è¿‡ä¸‰çº§åˆ†ç±»ä¸ä¸€è‡´çš„å•†å“
                
                # ğŸ†• å“ç‰Œæ£€æŸ¥ï¼šæ’é™¤åŒå“ç‰Œå•†å“ï¼ˆé˜²æ­¢"å¯å£å¯ä¹330ml" vs "å¯å£å¯ä¹500ml"è¢«åˆ¤ä¸ºå·®å¼‚å“ï¼‰
                brand_a = row_a.get('standardized_brand', '').strip().lower()
                brand_b = row_b.get('standardized_brand', '').strip().lower()
                
                # å¦‚æœä¸¤ä¸ªå•†å“å“ç‰Œç›¸åŒä¸”éƒ½ä¸ä¸ºç©ºï¼Œè·³è¿‡ï¼ˆä¸æ˜¯çœŸæ­£çš„å·®å¼‚å“ï¼‰
                if brand_a and brand_b and brand_a == brand_b:
                    debug_info['same_brand_skipped'] = debug_info.get('same_brand_skipped', 0) + 1
                    continue  # è·³è¿‡åŒå“ç‰Œå•†å“
                
                # æ„å»ºå·®å¼‚å“åŒ¹é…è®°å½• - å®Œæ•´çš„ABABæ ¼å¼
                price_diff_pct = ((price_b - price_a) / price_a) * 100
                
                # åŸºç¡€å­—æ®µ - ABABæ ¼å¼
                match_record = {
                    f'å•†å“åç§°_{name_a}': row_a['å•†å“åç§°'],
                    f'å•†å“åç§°_{name_b}': row_b['å•†å“åç§°'],
                    f'ç¾å›¢ä¸€çº§åˆ†ç±»_{name_a}': category,
                    f'ç¾å›¢ä¸€çº§åˆ†ç±»_{name_b}': category,
                }
                
                # ä¸‰çº§åˆ†ç±» - ABABæ ¼å¼
                if 'ç¾å›¢ä¸‰çº§åˆ†ç±»' in row_a.index and 'ç¾å›¢ä¸‰çº§åˆ†ç±»' in row_b.index:
                    match_record[f'ç¾å›¢ä¸‰çº§åˆ†ç±»_{name_a}'] = row_a.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', '')
                    match_record[f'ç¾å›¢ä¸‰çº§åˆ†ç±»_{name_b}'] = row_b.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', '')
                
                # åŸä»· - ABABæ ¼å¼
                match_record[f'åŸä»·_{name_a}'] = row_a.get('åŸä»·_numeric', '')
                match_record[f'åŸä»·_{name_b}'] = row_b.get('åŸä»·_numeric', '')
                
                # å”®ä»· - ABABæ ¼å¼
                match_record[f'å”®ä»·_{name_a}'] = row_a.get('å”®ä»·_numeric', '')
                match_record[f'å”®ä»·_{name_b}'] = row_b.get('å”®ä»·_numeric', '')
                
                # æœˆå”® - ABABæ ¼å¼
                if 'æœˆå”®' in row_a.index and 'æœˆå”®' in row_b.index:
                    match_record[f'æœˆå”®_{name_a}'] = row_a.get('æœˆå”®', 0)
                    match_record[f'æœˆå”®_{name_b}'] = row_b.get('æœˆå”®', 0)
                
                # åº“å­˜ - ABABæ ¼å¼
                if 'åº“å­˜' in row_a.index and 'åº“å­˜' in row_b.index:
                    match_record[f'åº“å­˜_{name_a}'] = row_a.get('åº“å­˜', 0)
                    match_record[f'åº“å­˜_{name_b}'] = row_b.get('åº“å­˜', 0)
                
                # åˆ†æå­—æ®µï¼ˆæ”¾åœ¨æœ€åï¼‰
                match_record[f'å¯¹æ¯”ä»·æ ¼æ¥æº_{name_a}'] = row_a['ä»·æ ¼æ¥æº']
                match_record[f'å¯¹æ¯”ä»·æ ¼æ¥æº_{name_b}'] = row_b['ä»·æ ¼æ¥æº']
                match_record['price_diff_pct'] = round(price_diff_pct, 1)
                match_record['similarity_score'] = round(similarity, 3)
                match_record['å·®å¼‚åˆ†æ'] = 'åŒç±»æ›¿ä»£å“' if similarity > 0.45 else 'åŒç±»ç›¸å…³å“'
                match_record['åˆ†ç±»ä¸€è‡´æ€§'] = 'ä¸‰çº§åˆ†ç±»ä¸€è‡´' if cat3_match else cat3_warning if cat3_warning else 'æ— ä¸‰çº§åˆ†ç±»'
                
                # â­ ä¸€å¯¹ä¸€æœ€ä½³åŒ¹é…ï¼šåªä¿ç•™æ¯ä¸ªAåº—å•†å“ç›¸ä¼¼åº¦æœ€é«˜çš„é‚£ä¸ªBåº—å•†å“
                if i not in best_matches_a:
                    best_matches_a[i] = (j, similarity, match_record)
                else:
                    # å¦‚æœå·²æœ‰åŒ¹é…ï¼Œæ¯”è¾ƒç›¸ä¼¼åº¦ï¼Œä¿ç•™æ›´å¥½çš„
                    _, prev_sim, _ = best_matches_a[i]
                    if similarity > prev_sim:
                        best_matches_a[i] = (j, similarity, match_record)
        
        # å°†æœ€ä½³åŒ¹é…æ·»åŠ åˆ°ç»“æœä¸­
        for idx_a, (idx_b, sim, match_record) in best_matches_a.items():
            differential_matches.append(match_record)
            category_matches += 1
            debug_info['final_matches'] += 1
        
        # æ¯ä¸ªåˆ†ç±»å¤„ç†åæ˜¾ç¤ºè¿›åº¦
        if category_matches > 0:
            matched_count += category_matches
            cat3_mismatch_pct = (debug_info['cat3_mismatch'] / category_matches * 100) if category_matches > 0 else 0
            same_brand_skipped = debug_info.get('same_brand_skipped', 0)
            tqdm.write(f"   âœ… [{category}] æ‰¾åˆ° {category_matches} å¯¹å·®å¼‚å“ ({config_info}){' | ä¸‰çº§åˆ†ç±»ä¸ä¸€è‡´:'+str(debug_info['cat3_mismatch'])+'å¯¹' if debug_info['cat3_mismatch'] > 0 else ''}{' | åŒå“ç‰Œå·²æ’é™¤:'+str(same_brand_skipped)+'å¯¹' if same_brand_skipped > 0 else ''}")
        elif idx <= 5:  # å‰5ä¸ªæ˜¾ç¤ºè¯¦ç»†è°ƒè¯•ï¼ˆå¢åŠ åˆ°5ä¸ªï¼‰
            tqdm.write(f"   âšª [{category}] 0å¯¹ ({config_info})")
            tqdm.write(f"       ğŸ” æ¼æ–—: Aå•†å“{debug_info['total_a']} â†’ Aæœ‰æ•ˆä»·æ ¼{debug_info['valid_price_a']} â†’ ç›¸ä¼¼åº¦ç¬¦åˆ{debug_info['similarity_in_range']} â†’ Bæœ‰æ•ˆä»·æ ¼{debug_info['valid_price_b']} â†’ ä»·æ ¼ç¬¦åˆ{debug_info['price_in_range']} â†’ ä¸‰çº§åˆ†ç±»æ£€æŸ¥å{debug_info['price_in_range']-debug_info['cat3_mismatch']} â†’ ä¸€å¯¹ä¸€æœ€ä½³åŒ¹é…{debug_info['final_matches']}")
            
            # é¢å¤–è¯Šæ–­ï¼šæ˜¾ç¤ºå®é™…ç›¸ä¼¼åº¦åˆ†å¸ƒï¼ˆä»…å‰3ä¸ªåˆ†ç±»ï¼‰
            if idx <= 3 and len(sim_matrix) > 0:
                sim_flat = sim_matrix.flatten()
                sim_in_range = sim_flat[(sim_flat >= config['similarity_min']) & (sim_flat <= config['similarity_max'])]
                tqdm.write(f"       ğŸ“Š ç›¸ä¼¼åº¦: æœ€å¤§{sim_flat.max():.3f}, å‡å€¼{sim_flat.mean():.3f}, æœ€å°{sim_flat.min():.3f}, åœ¨èŒƒå›´å†…{len(sim_in_range)}/{len(sim_flat)}")
    
    if not differential_matches:
        print(f"   âŒ æœªæ‰¾åˆ°ç¬¦åˆæ¡ä»¶çš„å·®å¼‚å“")
        print(f"\n   ï¿½ è¯¦ç»†è¯Šæ–­:")
        print(f"      â€¢ å…±åŒåˆ†ç±»æ•°: {len(common_cats)}")
        print(f"      â€¢ Aåº—æœ‰æ•ˆä»·æ ¼: {valid_price_a.sum()}/{len(df_a_unique)}")
        print(f"      â€¢ Båº—æœ‰æ•ˆä»·æ ¼: {valid_price_b.sum()}/{len(df_b_unique)}")
        print(f"      â€¢ vectoråˆ—å­˜åœ¨: A={('vector' in df_a_unique.columns)}, B={('vector' in df_b_unique.columns)}")
        print(f"\n   ï¿½ğŸ’¡ å¯èƒ½åŸå› :")
        print(f"      1. â­å»é‡åç¼ºå°‘vectoråˆ—ï¼ˆå·²ä¿®å¤ï¼Œè¯·é‡æ–°è¿è¡Œï¼‰")
        print(f"      2. ä»·æ ¼å·®å¼‚è¶…å‡ºå„å“ç±»åŠ¨æ€å®¹å·®èŒƒå›´ï¼ˆé¥®æ–™Â±35%, ä¼‘é—²é£Ÿå“Â±40%, ç”Ÿé²œÂ±50%ç­‰ï¼‰")
        print(f"      3. ç›¸ä¼¼åº¦ä¸åœ¨å„å“ç±»åŠ¨æ€èŒƒå›´å†…ï¼ˆå¦‚é¥®æ–™0.30-0.60, ä¼‘é—²é£Ÿå“0.25-0.60ç­‰ï¼‰")
        print(f"      4. ä¸€çº§åˆ†ç±»ä¸åŒ¹é…ï¼ˆéœ€è¦ä¸¤åº—æœ‰å…±åŒçš„åˆ†ç±»ï¼‰")
        print(f"      5. å•†å“ä»·æ ¼ç¼ºå¤±æˆ–ä¸º0")
        print(f"\n   ğŸ”§ å»ºè®®æ“ä½œ:")
        print(f"      â†’ é‡æ–°è¿è¡Œå®Œæ•´æ¯”ä»·åˆ†æï¼ˆå·²ä¿®å¤vectoråˆ—ä¿ç•™é—®é¢˜ï¼‰")
        print(f"      â†’ å¦‚ä»ç„¶0åŒ¹é…ï¼Œå¯ä¸´æ—¶æ”¾å®½é˜ˆå€¼æµ‹è¯•")
        return pd.DataFrame()
    
    df_differential = pd.DataFrame(differential_matches)
    
    # ç»Ÿè®¡ä¸‰çº§åˆ†ç±»åŒ¹é…æƒ…å†µ
    cat3_mismatch = df_differential[df_differential['åˆ†ç±»ä¸€è‡´æ€§'].str.contains('âš ï¸', na=False)]
    if len(cat3_mismatch) > 0:
        print(f"   âš ï¸  å‘ç° {len(cat3_mismatch)} å¯¹å•†å“ä¸‰çº§åˆ†ç±»ä¸ä¸€è‡´ï¼ˆå¯èƒ½å­˜åœ¨åˆ†ç±»é”™è¯¯ï¼‰")
    
    # æŒ‰ç›¸ä¼¼åº¦é™åºæ’åº
    df_differential = df_differential.sort_values('similarity_score', ascending=False)
    
    print(f"   âœ… æ‰¾åˆ° {len(df_differential)} å¯¹å·®å¼‚å“åŒ¹é…")
    return df_differential

def analyze_category_gaps(df_a_unique, df_b_unique, name_a, name_b):
    """
    å“ç±»ç¼ºå£åˆ†æï¼šæ‰¾å‡ºç«å¯¹æœ‰ä½†æœ¬åº—æ²¡æœ‰çš„ç»†åˆ†å“ç±»ï¼ˆå•†å“æ˜ç»†å±•å¼€ï¼‰
    
    Returns:
        å“ç±»ç¼ºå£åˆ†æDataFrameï¼ˆæ¯ä¸ªå•†å“ä¸€è¡Œï¼‰
    """
    if df_a_unique.empty or df_b_unique.empty:
        return pd.DataFrame()
    
    print(f"\nğŸ“Š å¼€å§‹å“ç±»ç¼ºå£åˆ†æ...")
    
    # ç»Ÿè®¡å„åº—çš„åˆ†ç±»ç»„åˆ
    if 'ç¾å›¢ä¸‰çº§åˆ†ç±»' in df_a_unique.columns and 'ç¾å›¢ä¸‰çº§åˆ†ç±»' in df_b_unique.columns:
        # æŒ‰ä¸€çº§+ä¸‰çº§ç»„åˆåˆ†æ
        df_a_unique['åˆ†ç±»ç»„åˆ'] = df_a_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].astype(str) + ' > ' + df_a_unique['ç¾å›¢ä¸‰çº§åˆ†ç±»'].astype(str)
        df_b_unique['åˆ†ç±»ç»„åˆ'] = df_b_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].astype(str) + ' > ' + df_b_unique['ç¾å›¢ä¸‰çº§åˆ†ç±»'].astype(str)
    else:
        # åªæŒ‰ä¸€çº§åˆ†ç±»åˆ†æ
        df_a_unique['åˆ†ç±»ç»„åˆ'] = df_a_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].astype(str)
        df_b_unique['åˆ†ç±»ç»„åˆ'] = df_b_unique['ç¾å›¢ä¸€çº§åˆ†ç±»'].astype(str)
    
    # æ‰¾å‡ºç«å¯¹ç‹¬æœ‰çš„åˆ†ç±»
    categories_a = set(df_a_unique['åˆ†ç±»ç»„åˆ'].unique())
    categories_b = set(df_b_unique['åˆ†ç±»ç»„åˆ'].unique())
    gap_categories = categories_b - categories_a
    
    if not gap_categories:
        print(f"   æœ¬åº—å“ç±»è¦†ç›–å®Œæ•´ï¼Œæ— æ˜æ˜¾ç¼ºå£")
        return pd.DataFrame()
    
    # ğŸ”§ æ–¹æ¡ˆAï¼šå±•å¼€æ‰€æœ‰å•†å“æ˜ç»†
    gap_products = []
    total_gap_products = 0
    
    for category in sorted(gap_categories):
        cat_products = df_b_unique[df_b_unique['åˆ†ç±»ç»„åˆ'] == category].copy()
        
        # è½¬æ¢æ•°å€¼åˆ—
        cat_products['å”®ä»·_numeric'] = pd.to_numeric(cat_products['å”®ä»·'], errors='coerce')
        cat_products['åŸä»·_numeric'] = pd.to_numeric(cat_products.get('åŸä»·', 0), errors='coerce')
        cat_products['æœˆå”®_numeric'] = pd.to_numeric(cat_products.get('æœˆå”®', 0), errors='coerce')
        cat_products['åº“å­˜_numeric'] = pd.to_numeric(cat_products.get('åº“å­˜', 0), errors='coerce')
        
        # æŒ‰æœˆå”®é™åºæ’åºï¼ˆé”€é‡é«˜çš„åœ¨å‰ï¼‰
        cat_products = cat_products.sort_values('æœˆå”®_numeric', ascending=False)
        
        # æ„å»ºæ¯ä¸ªå•†å“çš„è®°å½•
        for _, product in cat_products.iterrows():
            gap_products.append({
                'ç¼ºå¤±å“ç±»': category,
                'å•†å“åç§°': product.get('å•†å“åç§°', ''),
                'ç¾å›¢ä¸€çº§åˆ†ç±»': product.get('ç¾å›¢ä¸€çº§åˆ†ç±»', ''),
                'ç¾å›¢ä¸‰çº§åˆ†ç±»': product.get('ç¾å›¢ä¸‰çº§åˆ†ç±»', ''),
                'å”®ä»·': product.get('å”®ä»·_numeric', ''),
                'åŸä»·': product.get('åŸä»·_numeric', ''),
                'æœˆå”®': product.get('æœˆå”®_numeric', ''),
                'åº“å­˜': product.get('åº“å­˜_numeric', ''),
                'æ¡ç ': product.get('æ¡ç ', ''),
                f'åº—é“º_{name_b}': name_b,
                'å»ºè®®': 'è€ƒè™‘å¼•è¿›',
            })
            total_gap_products += 1
    
    if not gap_products:
        return pd.DataFrame()
    
    df_gaps = pd.DataFrame(gap_products)
    
    # è°ƒæ•´åˆ—é¡ºåºï¼ŒæŠŠå…³é”®ä¿¡æ¯æ”¾å‰é¢
    cols_order = ['ç¼ºå¤±å“ç±»', 'å•†å“åç§°', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'å”®ä»·', 'åŸä»·', 
                  'æœˆå”®', 'åº“å­˜', 'æ¡ç ', f'åº—é“º_{name_b}', 'å»ºè®®']
    # ä¿ç•™å®é™…å­˜åœ¨çš„åˆ—
    cols_order = [col for col in cols_order if col in df_gaps.columns]
    df_gaps = df_gaps[cols_order]
    
    print(f"   âœ… å‘ç° {len(gap_categories)} ä¸ªå“ç±»ç¼ºå£ï¼Œå…± {total_gap_products} ä¸ªå•†å“")
    return df_gaps

# ========================================
# ğŸ†• æˆæœ¬é¢„æµ‹æ¨¡å— (ç¬¬ä¸€é˜¶æ®µï¼šåŠ ä»·ç‡æ³• + å”®ä»·åŠ æƒä¼˜åŒ–)
# ========================================

def calculate_markup_rate(df, cost_col='æˆæœ¬', price_col='åŸä»·', markup_col_suffix='', use_weights=True):
    """
    è®¡ç®—åŠ ä»·ç‡ = ä»·æ ¼ / æˆæœ¬
    
    ã€å¤šä»·æ ¼æ”¯æŒã€‘å¯è®¡ç®—åŸä»·åŠ ä»·ç‡æˆ–å”®ä»·åŠ ä»·ç‡ï¼š
    - åŸä»·åŠ ä»·ç‡ï¼šåæ˜ å•†å“æ­£å¸¸å®šä»·ç­–ç•¥ï¼ˆç¨³å®šï¼‰
    - å”®ä»·åŠ ä»·ç‡ï¼šåæ˜ å®é™…åˆ©æ¶¦ç©ºé—´ï¼ˆè€ƒè™‘ä¿ƒé”€ï¼‰
    
    ã€æ–¹æ¡ˆA: åŠ æƒåŠ ä»·ç‡ä¼˜åŒ–ã€‘ğŸ†•
    - æŒ‰é”€é‡åŠ æƒï¼šæœˆå”®é«˜çš„å•†å“æƒé‡å¤§
    - æ™ºèƒ½å›é€€ï¼šæ— æœˆå”®æ•°æ®æ—¶ä½¿ç”¨ç®€å•å¹³å‡
    - å‘åå…¼å®¹ï¼šuse_weights=False æ—¶ä½¿ç”¨åŸé€»è¾‘
    
    Args:
        df: å•†å“æ•°æ®
        cost_col: æˆæœ¬åˆ—å
        price_col: ä»·æ ¼åˆ—åï¼ˆ'åŸä»·' æˆ– 'å”®ä»·'ï¼‰
        markup_col_suffix: åŠ ä»·ç‡åˆ—åç¼€ï¼ˆåŒºåˆ†åŸä»·å’Œå”®ä»·åŠ ä»·ç‡ï¼‰
        use_weights: æ˜¯å¦ä½¿ç”¨é”€é‡åŠ æƒï¼ˆé»˜è®¤Trueï¼‰
    
    Returns:
        df: æ·»åŠ äº† markup_rate åˆ—å’Œå¯é€‰çš„ sample_weight åˆ—çš„æ•°æ®
    """
    if cost_col not in df.columns or price_col not in df.columns:
        return df
    
    # è®¡ç®—åŠ ä»·ç‡ï¼Œé¿å…é™¤é›¶
    df = df.copy()
    markup_col = f'markup_rate{markup_col_suffix}' if markup_col_suffix else 'markup_rate'
    
    df[markup_col] = df.apply(
        lambda row: row[price_col] / row[cost_col] if pd.notna(row[cost_col]) and row[cost_col] > 0 else None,
        axis=1
    )
    
    # è¿‡æ»¤å¼‚å¸¸å€¼ï¼ˆåŠ ä»·ç‡ < 1.0 æˆ– > 10.0ï¼‰
    df.loc[(df[markup_col] < 1.0) | (df[markup_col] > 10.0), markup_col] = None
    
    # ğŸ†• é—®é¢˜ä¿®å¤1: è¿‡æ»¤æç«¯æŠ˜æ‰£å•†å“çš„å”®ä»·åŠ ä»·ç‡ï¼ˆé˜²æ­¢ä¿ƒé”€å“æ±¡æŸ“ç»Ÿè®¡ï¼‰
    if price_col == 'å”®ä»·' and 'åŸä»·' in df.columns:
        # è®¡ç®—æŠ˜æ‰£ç‡
        discount_rates = df['å”®ä»·'] / df['åŸä»·']
        # æç«¯æŠ˜æ‰£å•†å“ï¼ˆæŠ˜æ‰£ç‡<50%ï¼‰çš„å”®ä»·åŠ ä»·ç‡ä¸å‚ä¸ç»Ÿè®¡
        extreme_discount_mask = (discount_rates < 0.50) & df[markup_col].notna()
        df.loc[extreme_discount_mask, markup_col] = None
        
        filtered_count = extreme_discount_mask.sum()
        if filtered_count > 0:
            print(f"      ğŸ›¡ï¸ è¿‡æ»¤æç«¯æŠ˜æ‰£å•†å“: {filtered_count}ä¸ªï¼ˆå”®ä»·åŠ ä»·ç‡ä¸å‚ä¸ç»Ÿè®¡ï¼‰")
    
    # ğŸ†• æ–¹æ¡ˆA: è®¡ç®—æ ·æœ¬æƒé‡ï¼ˆé”€é‡åŠ æƒï¼‰
    if use_weights and 'æœˆå”®' in df.columns:
        weight_col = f'sample_weight{markup_col_suffix}' if markup_col_suffix else 'sample_weight'
        
        # é”€é‡æƒé‡ï¼šæœˆå”®è¶Šé«˜ï¼Œæƒé‡è¶Šå¤§ï¼ˆå¯¹æ•°ç¼©æ”¾ï¼Œé¿å…æç«¯å€¼ä¸»å¯¼ï¼‰
        df[weight_col] = df['æœˆå”®'].fillna(1).apply(lambda x: np.log1p(x) + 1)  # log1p(x) = log(1+x)
        
        # æ ‡å‡†åŒ–æƒé‡ï¼ˆå¯é€‰ï¼Œä¾¿äºè°ƒè¯•ï¼‰
        # df[weight_col] = df[weight_col] / df[weight_col].sum()
    
    return df


def validate_cost_prediction(predicted_cost, row, store_a_df=None, cfg=None):
    """
    ã€æ–¹æ¡ˆC: æˆæœ¬é¢„æµ‹å¼‚å¸¸æ£€æµ‹ã€‘ğŸ†•
    
    æ£€æµ‹å¹¶ä¿®æ­£ä¸åˆç†çš„æˆæœ¬é¢„æµ‹ï¼Œå‡å°‘æç«¯é”™è¯¯ã€‚
    
    ä¸‰å¤§æ£€æµ‹è§„åˆ™ï¼š
    1. æˆæœ¬ä¸èƒ½è¶…è¿‡å”®ä»·çš„80%ï¼ˆé˜²æ­¢äºæœ¬é¢„æµ‹ï¼‰
    2. åŠ ä»·ç‡ä¸èƒ½ä½äº1.2ï¼ˆè¡Œä¸šåº•çº¿ï¼‰
    3. å“ç‰ŒåŠ ä»·ç‡ä¸€è‡´æ€§æ£€æŸ¥ï¼ˆåŒå“ç‰Œåº”æ¥è¿‘ï¼‰
    
    Args:
        predicted_cost: é¢„æµ‹çš„æˆæœ¬
        row: å½“å‰å•†å“æ•°æ®ï¼ˆå¿…é¡»åŒ…å«åŸä»·_Bã€å”®ä»·_Bç­‰å­—æ®µï¼‰
        store_a_df: æœ¬åº—æ•°æ®ï¼ˆç”¨äºå“ç‰ŒåŠ ä»·ç‡æŸ¥è¯¢ï¼Œå¯é€‰ï¼‰
        cfg: é…ç½®å¯¹è±¡
    
    Returns:
        tuple: (adjusted_cost, adjusted_confidence, validation_flag)
            - adjusted_cost: è°ƒæ•´åçš„æˆæœ¬
            - adjusted_confidence: è°ƒæ•´åçš„ç½®ä¿¡åº¦
            - validation_flag: éªŒè¯æ ‡è®°ï¼ˆ'æ­£å¸¸', 'è°ƒæ•´:æˆæœ¬è¿‡é«˜', etc.ï¼‰
    """
    if cfg is None:
        cfg = Config()
    
    if pd.isna(predicted_cost) or predicted_cost <= 0:
        return predicted_cost, 0.0, 'æ— æ•ˆé¢„æµ‹'
    
    # è·å–å•†å“ä»·æ ¼ä¿¡æ¯ï¼ˆå…¼å®¹å¤šç§åˆ—åæ ¼å¼ï¼‰
    sale_price_b = None
    orig_price_b = None
    
    for col_suffix in ['_B', f'_{cfg.STORE_B_NAME}', '']:
        if f'å”®ä»·{col_suffix}' in row.index and pd.notna(row.get(f'å”®ä»·{col_suffix}')):
            sale_price_b = row[f'å”®ä»·{col_suffix}']
            break
    
    for col_suffix in ['_B', f'_{cfg.STORE_B_NAME}', '']:
        if f'åŸä»·{col_suffix}' in row.index and pd.notna(row.get(f'åŸä»·{col_suffix}')):
            orig_price_b = row[f'åŸä»·{col_suffix}']
            break
    
    if pd.isna(orig_price_b) or orig_price_b <= 0:
        return predicted_cost, 0.0, 'ç¼ºå°‘ä»·æ ¼æ•°æ®'
    
    # ä½¿ç”¨å”®ä»·ï¼ˆå¦‚æœæœ‰ï¼‰ï¼Œå¦åˆ™ä½¿ç”¨åŸä»·
    reference_price = sale_price_b if pd.notna(sale_price_b) and sale_price_b > 0 else orig_price_b
    
    # === è§„åˆ™0: æç«¯æŠ˜æ‰£äºæœ¬é”€å”®æ£€æµ‹ï¼ˆä¼˜å…ˆçº§æœ€é«˜ï¼‰===
    if pd.notna(sale_price_b) and sale_price_b > 0 and pd.notna(orig_price_b) and orig_price_b > 0:
        discount_rate = sale_price_b / orig_price_b
        
        # æç«¯æŠ˜æ‰£åœºæ™¯ï¼ˆæŠ˜æ‰£ç‡<30%ï¼Œå³æ‰“3æŠ˜ä»¥ä¸‹ï¼‰
        if discount_rate < 0.30:
            min_cost_ratio = 0.85  # æˆæœ¬è‡³å°‘æ˜¯å”®ä»·çš„85%
            min_allowed_cost = sale_price_b * min_cost_ratio
            
            if predicted_cost < min_allowed_cost:
                # æç«¯ä¿ƒé”€å“ï¼Œæˆæœ¬åº”æ¥è¿‘å”®ä»·ï¼ˆäºæœ¬æˆ–å¾®åˆ©é”€å”®ï¼‰
                adjusted_cost = sale_price_b * 0.90  # è°ƒæ•´ä¸ºå”®ä»·çš„90%
                adjusted_confidence = 0.55
                return adjusted_cost, adjusted_confidence, f'è°ƒæ•´:æç«¯æŠ˜æ‰£({discount_rate:.0%})äºæœ¬é¢„æµ‹'
    
    # === è§„åˆ™1: æˆæœ¬ä¸èƒ½è¶…è¿‡å”®ä»·çš„80% ===
    max_cost_ratio = 0.80  # æœ€å¤§æˆæœ¬å æ¯”
    max_allowed_cost = reference_price * max_cost_ratio
    
    if predicted_cost > max_allowed_cost:
        adjusted_cost = reference_price * 0.70  # è°ƒæ•´ä¸º70%ï¼ˆæ›´ä¿å®ˆï¼‰
        adjusted_confidence = 0.40
        return adjusted_cost, adjusted_confidence, f'è°ƒæ•´:æˆæœ¬è¿‡é«˜(>{max_cost_ratio:.0%}å”®ä»·)'
    
    # === è§„åˆ™2: å”®ä»·åŠ ä»·ç‡ä¸èƒ½ä½äº1.2ï¼ˆè¡Œä¸šåº•çº¿ï¼‰===
    min_markup = 1.20  # æœ€ä½åŠ ä»·ç‡
    # ğŸ†• é—®é¢˜ä¿®å¤2: ä½¿ç”¨å”®ä»·è®¡ç®—åŠ ä»·ç‡ï¼ˆæ›´èƒ½åæ˜ å®é™…åˆ©æ¶¦ç©ºé—´ï¼‰
    current_markup = reference_price / predicted_cost
    
    if current_markup < min_markup:
        # è°ƒæ•´ä¸ºæœ€ä½1.5å€å”®ä»·åŠ ä»·ç‡
        adjusted_cost = reference_price / 1.50  # ğŸ†• ä½¿ç”¨å”®ä»·è€ŒéåŸä»·
        adjusted_confidence = 0.50
        return adjusted_cost, adjusted_confidence, f'è°ƒæ•´:å”®ä»·åŠ ä»·ç‡è¿‡ä½(<{min_markup})'
    
    # === è§„åˆ™3: å“ç‰ŒåŠ ä»·ç‡ä¸€è‡´æ€§æ£€æŸ¥ ===
    if store_a_df is not None and len(store_a_df) > 0:
        # æå–å“ç‰Œï¼ˆå…¼å®¹å¤šç§åˆ—åï¼‰
        brand = None
        for brand_col in ['å“ç‰Œ', 'å“ç‰Œ_A', f'å“ç‰Œ_{cfg.STORE_A_NAME}']:
            if brand_col in row.index and pd.notna(row.get(brand_col)):
                brand = row[brand_col]
                break
        
        if brand and cfg.COST_COLUMN_NAME in store_a_df.columns and 'åŸä»·' in store_a_df.columns:
            # æŸ¥æ‰¾æœ¬åº—åŒå“ç‰Œå•†å“çš„å¹³å‡åŠ ä»·ç‡
            brand_col_in_df = None
            for col in ['å“ç‰Œ', 'standardized_brand']:
                if col in store_a_df.columns:
                    brand_col_in_df = col
                    break
            
            if brand_col_in_df:
                brand_products = store_a_df[store_a_df[brand_col_in_df] == brand]
                
                if len(brand_products) >= 3:  # è‡³å°‘3ä¸ªæ ·æœ¬
                    # è®¡ç®—å“ç‰Œå¹³å‡åŠ ä»·ç‡
                    brand_markups = []
                    for _, prod in brand_products.iterrows():
                        if (pd.notna(prod.get('åŸä»·')) and prod['åŸä»·'] > 0 and
                            pd.notna(prod.get(cfg.COST_COLUMN_NAME)) and prod[cfg.COST_COLUMN_NAME] > 0):
                            brand_markups.append(prod['åŸä»·'] / prod[cfg.COST_COLUMN_NAME])
                    
                    if len(brand_markups) >= 3:
                        brand_avg_markup = np.median(brand_markups)  # ä½¿ç”¨ä¸­ä½æ•°ï¼ˆæ›´ç¨³å¥ï¼‰
                        markup_diff = abs(current_markup - brand_avg_markup)
                        
                        # å¦‚æœå·®å¼‚>0.5ï¼ˆå¦‚å“ç‰Œå¹³å‡2.0ï¼Œå½“å‰é¢„æµ‹1.3æˆ–2.7ï¼‰ï¼Œè°ƒæ•´
                        if markup_diff > 0.5:
                            adjusted_cost = orig_price_b / brand_avg_markup
                            adjusted_confidence = 0.65
                            return adjusted_cost, adjusted_confidence, f'è°ƒæ•´:å“ç‰ŒåŠ ä»·ç‡({brand}å¹³å‡{brand_avg_markup:.2f})'
    
    # æ‰€æœ‰æ£€æŸ¥é€šè¿‡ï¼Œé¢„æµ‹åˆç†
    return predicted_cost, None, 'æ­£å¸¸'


def predict_competitor_cost(matched_df, store_a_df, cfg=None):
    """
    é¢„æµ‹ç«å¯¹æˆæœ¬ï¼ˆåŸºäºæœ¬åº—å“ç±»åŠ ä»·ç‡ï¼‰
    
    ç­–ç•¥ï¼š
    1. æ¡å½¢ç ç²¾ç¡®åŒ¹é… â†’ ç›´æ¥ä½¿ç”¨æœ¬åº—æˆæœ¬ï¼ˆç½®ä¿¡åº¦ 0.95ï¼‰
    2. ä¸‰çº§åˆ†ç±»åŠ ä»·ç‡ â†’ ç«å¯¹ä»·æ ¼ / ä¸‰çº§åˆ†ç±»å¹³å‡åŠ ä»·ç‡ï¼ˆç½®ä¿¡åº¦æ ¹æ®æ ·æœ¬é‡ï¼‰
    3. ä¸€çº§åˆ†ç±»åŠ ä»·ç‡ â†’ å…œåº•æ–¹æ¡ˆï¼ˆç½®ä¿¡åº¦è¾ƒä½ï¼‰
    
    ã€ğŸ†• å”®ä»·åŠ æƒä¼˜åŒ–ã€‘ï¼š
    - ä¸»é¢„æµ‹ï¼šåŸºäºåŸä»·åŠ ä»·ç‡ï¼ˆç¨³å®šï¼Œåæ˜ å®šä»·ç­–ç•¥ï¼‰
    - è¾…åŠ©é¢„æµ‹ï¼šåŸºäºå”®ä»·åŠ ä»·ç‡ï¼ˆåæ˜ å®é™…åˆ©æ¶¦ç©ºé—´ï¼‰
    - åŠ æƒèåˆï¼šåŸä»·æƒé‡70% + å”®ä»·æƒé‡30%
    - ç½®ä¿¡åº¦è°ƒæ•´ï¼šåŸä»·/å”®ä»·ä¸€è‡´æ€§è¶Šé«˜ï¼Œç½®ä¿¡åº¦è¶Šé«˜
    
    Args:
        matched_df: åŒ¹é…ç»“æœ DataFrame
        store_a_df: æœ¬åº—åŸå§‹æ•°æ®ï¼ˆå«æˆæœ¬ï¼‰
        cfg: é…ç½®å¯¹è±¡
    
    Returns:
        matched_df: æ·»åŠ äº†æˆæœ¬é¢„æµ‹åˆ—çš„ DataFrame
    """
    if cfg is None:
        cfg = Config()
    
    cost_col = cfg.COST_COLUMN_NAME
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æˆæœ¬æ•°æ®
    if cost_col not in store_a_df.columns:
        print("   âš ï¸  æœ¬åº—æ•°æ®ä¸­æœªæ‰¾åˆ°æˆæœ¬åˆ—ï¼Œè·³è¿‡æˆæœ¬é¢„æµ‹")
        return matched_df
    
    print("\n" + "="*60)
    print("ğŸ§® ç«å¯¹æˆæœ¬é¢„æµ‹åˆ†æ")
    print("="*60)
    
    # è®¡ç®—æœ¬åº—åŸä»·åŠ ä»·ç‡å’Œå”®ä»·åŠ ä»·ç‡
    store_a_with_markup = calculate_markup_rate(store_a_df.copy(), cost_col, 'åŸä»·', '_åŸä»·', use_weights=True)
    if 'å”®ä»·' in store_a_df.columns and cfg.USE_SALE_PRICE_WEIGHT:
        store_a_with_markup = calculate_markup_rate(store_a_with_markup, cost_col, 'å”®ä»·', '_å”®ä»·', use_weights=True)
    
    # ğŸ†• æ–¹æ¡ˆA: ä½¿ç”¨åŠ æƒå¹³å‡è®¡ç®—å“ç±»åŠ ä»·ç‡ï¼ˆæŒ‰é”€é‡åŠ æƒï¼‰
    def weighted_agg(df, value_col, weight_col):
        """åŠ æƒèšåˆå‡½æ•°"""
        if weight_col not in df.columns or df[weight_col].isna().all():
            # å›é€€ï¼šæ— æƒé‡æ—¶ä½¿ç”¨ç®€å•å¹³å‡
            return df[value_col].agg(['mean', 'std', 'count'])
        
        # è¿‡æ»¤æœ‰æ•ˆæ•°æ®
        valid_mask = df[value_col].notna() & df[weight_col].notna()
        valid_df = df[valid_mask]
        
        if len(valid_df) == 0:
            return pd.Series({'mean': None, 'std': None, 'count': 0})
        
        weights = valid_df[weight_col]
        values = valid_df[value_col]
        
        # åŠ æƒå¹³å‡
        weighted_mean = np.average(values, weights=weights)
        
        # åŠ æƒæ ‡å‡†å·®
        weighted_variance = np.average((values - weighted_mean) ** 2, weights=weights)
        weighted_std = np.sqrt(weighted_variance)
        
        return pd.Series({
            'mean': weighted_mean,
            'std': weighted_std,
            'count': len(valid_df)
        })
    
    # æŒ‰å“ç±»ç»Ÿè®¡åŸä»·åŠ ä»·ç‡ï¼ˆåŠ æƒï¼‰
    if 'sample_weight_åŸä»·' in store_a_with_markup.columns:
        category_markup_orig_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').apply(
            lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·')
        ).dropna()
        
        category_markup_orig_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').apply(
            lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·')
        ).dropna()
    else:
        # å›é€€ï¼šæ— æƒé‡æ—¶ä½¿ç”¨åŸé€»è¾‘
        category_markup_orig_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').agg({
            'markup_rate_åŸä»·': ['mean', 'std', 'count']
        }).dropna()
        
        category_markup_orig_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').agg({
            'markup_rate_åŸä»·': ['mean', 'std', 'count']
        }).dropna()
    
    # æŒ‰å“ç±»ç»Ÿè®¡å”®ä»·åŠ ä»·ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼ŒåŒæ ·åŠ æƒï¼‰
    category_markup_sale_level3 = pd.DataFrame()
    category_markup_sale_level1 = pd.DataFrame()
    if 'markup_rate_å”®ä»·' in store_a_with_markup.columns and cfg.USE_SALE_PRICE_WEIGHT:
        if 'sample_weight_å”®ä»·' in store_a_with_markup.columns:
            category_markup_sale_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').apply(
                lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·')
            ).dropna()
            
            category_markup_sale_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').apply(
                lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·')
            ).dropna()
        else:
            # å›é€€ï¼šæ— æƒé‡æ—¶ä½¿ç”¨åŸé€»è¾‘
            category_markup_sale_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').agg({
                'markup_rate_å”®ä»·': ['mean', 'std', 'count']
            }).dropna()
            
            category_markup_sale_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').agg({
                'markup_rate_å”®ä»·': ['mean', 'std', 'count']
            }).dropna()
    
    # ğŸ†• æ–¹æ¡ˆB: è®¡ç®—å“ç‰Œ+åˆ†ç±»ç»„åˆåŠ ä»·ç‡ï¼ˆå¤šç»´åº¦åˆ†å±‚ï¼‰
    brand_cat3_markup_orig = pd.DataFrame()
    brand_cat1_markup_orig = pd.DataFrame()
    brand_cat3_markup_sale = pd.DataFrame()
    brand_cat1_markup_sale = pd.DataFrame()
    
    MIN_BRAND_CATEGORY_SAMPLES = 3  # å“ç‰Œ+åˆ†ç±»æœ€å°æ ·æœ¬æ•°
    
    if 'å“ç‰Œ' in store_a_with_markup.columns:
        # å“ç‰Œ+ä¸‰çº§åˆ†ç±»ï¼ˆåŸä»·åŠ ä»·ç‡ï¼‰
        if 'sample_weight_åŸä»·' in store_a_with_markup.columns:
            brand_cat3_markup_orig = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸‰çº§åˆ†ç±»']).apply(
                lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
            ).dropna()
            
            brand_cat1_markup_orig = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸€çº§åˆ†ç±»']).apply(
                lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
            ).dropna()
        
        # å“ç‰Œ+åˆ†ç±»ï¼ˆå”®ä»·åŠ ä»·ç‡ï¼‰
        if 'markup_rate_å”®ä»·' in store_a_with_markup.columns and cfg.USE_SALE_PRICE_WEIGHT:
            if 'sample_weight_å”®ä»·' in store_a_with_markup.columns:
                brand_cat3_markup_sale = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸‰çº§åˆ†ç±»']).apply(
                    lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
                ).dropna()
                
                brand_cat1_markup_sale = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸€çº§åˆ†ç±»']).apply(
                    lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
                ).dropna()
    
    # ğŸ†• æ–¹æ¡ˆB: è®¡ç®—ä»·æ ¼åŒºé—´åŠ ä»·ç‡
    def get_price_range(price):
        """å°†ä»·æ ¼åˆ†é…åˆ°åŒºé—´"""
        if pd.isna(price) or price <= 0:
            return None
        if price < 10:
            return '0-10å…ƒ'
        elif price < 30:
            return '10-30å…ƒ'
        elif price < 50:
            return '30-50å…ƒ'
        elif price < 100:
            return '50-100å…ƒ'
        else:
            return '100å…ƒä»¥ä¸Š'
    
    price_range_markup_orig = pd.DataFrame()
    price_range_markup_sale = pd.DataFrame()
    
    store_a_with_markup['ä»·æ ¼åŒºé—´'] = store_a_with_markup['åŸä»·'].apply(get_price_range)
    
    if 'sample_weight_åŸä»·' in store_a_with_markup.columns:
        price_range_markup_orig = store_a_with_markup.groupby('ä»·æ ¼åŒºé—´').apply(
            lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·')
        ).dropna()
    
    if 'markup_rate_å”®ä»·' in store_a_with_markup.columns and cfg.USE_SALE_PRICE_WEIGHT:
        if 'sample_weight_å”®ä»·' in store_a_with_markup.columns:
            price_range_markup_sale = store_a_with_markup.groupby('ä»·æ ¼åŒºé—´').apply(
                lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·')
            ).dropna()
    
    print(f"   ğŸ“Š æœ¬åº—åŠ ä»·ç‡ç»Ÿè®¡ï¼ˆæ–¹æ¡ˆA+Bï¼šé”€é‡åŠ æƒ + å¤šç»´åˆ†å±‚ï¼‰ï¼š")
    print(f"      ä¸‰çº§åˆ†ç±»: {len(category_markup_orig_level3)}ä¸ª")
    print(f"      ä¸€çº§åˆ†ç±»: {len(category_markup_orig_level1)}ä¸ª")
    if not brand_cat3_markup_orig.empty or not brand_cat1_markup_orig.empty:
        print(f"      ğŸ†• å“ç‰Œ+ä¸‰çº§åˆ†ç±»: {len(brand_cat3_markup_orig)}ä¸ª")
        print(f"      ğŸ†• å“ç‰Œ+ä¸€çº§åˆ†ç±»: {len(brand_cat1_markup_orig)}ä¸ª")
    if not price_range_markup_orig.empty:
        print(f"      ğŸ†• ä»·æ ¼åŒºé—´: {len(price_range_markup_orig)}ä¸ª")
    if cfg.USE_SALE_PRICE_WEIGHT and not category_markup_sale_level3.empty:
        print(f"      å”®ä»·åŠ æƒæ¨¡å¼: å¯ç”¨ï¼ˆåŸä»·æƒé‡{cfg.ORIGINAL_PRICE_WEIGHT*100}% + å”®ä»·æƒé‡{cfg.SALE_PRICE_WEIGHT*100}%ï¼‰")
    
    # åˆå§‹åŒ–é¢„æµ‹åˆ—
    matched_df = matched_df.copy()
    matched_df['é¢„æµ‹æˆæœ¬_B'] = None
    matched_df['é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = None  # ğŸ†• ä¿ç•™åŸä»·åŸºå‡†é¢„æµ‹
    matched_df['é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = None  # ğŸ†• ä¿ç•™å”®ä»·åŸºå‡†é¢„æµ‹
    matched_df['é¢„æµ‹æ–¹æ³•'] = None
    matched_df['ç½®ä¿¡åº¦'] = None
    # ğŸ”§ ä¿®å¤ï¼šä¸è¦åˆå§‹åŒ–æˆæœ¬_Aä¸ºNoneï¼Œç¨åä»åŸå§‹åˆ—å¤åˆ¶
    # matched_df['æˆæœ¬_A'] = None
    matched_df['åŠ ä»·ç‡_A'] = None
    
    # è·å–åˆ—å
    barcode_col_a = f'æ¡ç _{cfg.STORE_A_NAME}' if f'æ¡ç _{cfg.STORE_A_NAME}' in matched_df.columns else 'æ¡ç _A'
    barcode_col_b = f'æ¡ç _{cfg.STORE_B_NAME}' if f'æ¡ç _{cfg.STORE_B_NAME}' in matched_df.columns else 'æ¡ç _B'
    cost_col_a = f'{cost_col}_{cfg.STORE_A_NAME}' if f'{cost_col}_{cfg.STORE_A_NAME}' in matched_df.columns else f'{cost_col}_A'
    orig_price_col_b = f'åŸä»·_{cfg.STORE_B_NAME}' if f'åŸä»·_{cfg.STORE_B_NAME}' in matched_df.columns else 'åŸä»·_B'
    sale_price_col_b = f'å”®ä»·_{cfg.STORE_B_NAME}' if f'å”®ä»·_{cfg.STORE_B_NAME}' in matched_df.columns else 'å”®ä»·_B'
    cat3_col_a = f'ç¾å›¢ä¸‰çº§åˆ†ç±»_{cfg.STORE_A_NAME}' if f'ç¾å›¢ä¸‰çº§åˆ†ç±»_{cfg.STORE_A_NAME}' in matched_df.columns else 'ç¾å›¢ä¸‰çº§åˆ†ç±»_A'
    cat1_col_a = f'ç¾å›¢ä¸€çº§åˆ†ç±»_{cfg.STORE_A_NAME}' if f'ç¾å›¢ä¸€çº§åˆ†ç±»_{cfg.STORE_A_NAME}' in matched_df.columns else 'ç¾å›¢ä¸€çº§åˆ†ç±»_A'
    
    # ğŸ”§ ä¿®å¤ï¼šç»Ÿä¸€æˆæœ¬åˆ—åä¸ºæˆæœ¬_A
    if cost_col_a != 'æˆæœ¬_A':
        if cost_col_a in matched_df.columns:
            matched_df['æˆæœ¬_A'] = matched_df[cost_col_a]
            print(f"   ğŸ”§ å°†æˆæœ¬åˆ— {cost_col_a} å¤åˆ¶åˆ° æˆæœ¬_Aï¼Œéç©ºæ•°é‡: {matched_df['æˆæœ¬_A'].notna().sum()}")
        else:
            matched_df['æˆæœ¬_A'] = None
            print(f"   âš ï¸  æœªæ‰¾åˆ°æˆæœ¬åˆ— {cost_col_a}ï¼Œæˆæœ¬_Aå°†ä¸ºç©º")
    else:
        # cost_col_a å°±æ˜¯ æˆæœ¬_Aï¼Œç¡®ä¿åˆ—å­˜åœ¨
        if 'æˆæœ¬_A' not in matched_df.columns:
            matched_df['æˆæœ¬_A'] = None
            print(f"   âš ï¸  matched_dfä¸­ä¸å­˜åœ¨æˆæœ¬_Aåˆ—ï¼Œå°†ä¸ºç©º")
    
    barcode_match_count = 0
    brand_cat3_match_count = 0  # ğŸ†• æ–¹æ¡ˆB: å“ç‰Œ+ä¸‰çº§åˆ†ç±»è®¡æ•°
    cat3_match_count = 0
    brand_cat1_match_count = 0  # ğŸ†• æ–¹æ¡ˆB: å“ç‰Œ+ä¸€çº§åˆ†ç±»è®¡æ•°
    cat1_match_count = 0
    price_range_match_count = 0  # ğŸ†• æ–¹æ¡ˆB: ä»·æ ¼åŒºé—´è®¡æ•°
    weighted_count = 0  # ğŸ†• å”®ä»·åŠ æƒé¢„æµ‹è®¡æ•°
    
    for idx, row in matched_df.iterrows():
        # ğŸ”§ ä¿®å¤ï¼šæˆæœ¬_Aå·²ç»åœ¨ä¸Šé¢å¤åˆ¶å¥½äº†ï¼Œè¿™é‡Œç›´æ¥ä½¿ç”¨
        # if cost_col_a in matched_df.columns and pd.notna(row.get(cost_col_a)):
        #     matched_df.at[idx, 'æˆæœ¬_A'] = row[cost_col_a]
        
        # è®¡ç®—åŠ ä»·ç‡
        if pd.notna(row.get('æˆæœ¬_A')) and pd.notna(row.get(orig_price_col_b)) and row[orig_price_col_b] > 0 and row['æˆæœ¬_A'] > 0:
            matched_df.at[idx, 'åŠ ä»·ç‡_A'] = row[orig_price_col_b] / row['æˆæœ¬_A']
        
        # ç­–ç•¥1: æ¡å½¢ç åŒ¹é…
        # ğŸ”§ ä¿®å¤ï¼šæ¡ç ç›¸åŒä¸ä»£è¡¨æˆæœ¬ç›¸åŒï¼Œéœ€è¦åŸºäºåŠ ä»·ç‡é¢„æµ‹ç«å¯¹æˆæœ¬
        # æ ‡è®°æ¡ç åŒ¹é…çŠ¶æ€ï¼Œåç»­ä½¿ç”¨æ›´é«˜çš„ç½®ä¿¡åº¦
        is_barcode_match = (barcode_col_a in matched_df.columns and barcode_col_b in matched_df.columns and
            pd.notna(row.get(barcode_col_a)) and pd.notna(row.get(barcode_col_b)) and
            str(row[barcode_col_a]) == str(row[barcode_col_b]))
        
        # ä¸å†ç›´æ¥ä½¿ç”¨æœ¬åº—æˆæœ¬ï¼Œç»§ç»­ä½¿ç”¨åŠ ä»·ç‡é¢„æµ‹
        
        # è·å–ç«å¯¹ä»·æ ¼
        orig_price_b = row.get(orig_price_col_b)
        sale_price_b = row.get(sale_price_col_b)
        
        if pd.isna(orig_price_b) or orig_price_b <= 0:
            continue
        
        # ğŸ†• è·å–å“ç‰Œå’Œåˆ†ç±»ä¿¡æ¯ï¼ˆæ–¹æ¡ˆBï¼‰
        # æ³¨æ„ï¼šå“ç‰Œä½¿ç”¨ç«å¯¹çš„ï¼ˆbrand_bï¼‰ï¼Œåˆ†ç±»ä½¿ç”¨æœ¬åº—çš„ï¼ˆcat3/cat1ï¼‰ï¼Œå› ä¸ºè¦åŒ¹é…æœ¬åº—çš„åŠ ä»·ç‡è¡¨
        brand_col_b = f'å“ç‰Œ_{cfg.STORE_B_NAME}' if f'å“ç‰Œ_{cfg.STORE_B_NAME}' in matched_df.columns else 'å“ç‰Œ_B'
        brand_b = row.get(brand_col_b, '')
        
        # ğŸ†• æ–¹æ¡ˆB ä¼˜å…ˆçº§1: å“ç‰Œ+ä¸‰çº§åˆ†ç±»åŠ ä»·ç‡ï¼ˆæœ€ç²¾å‡†ï¼‰
        # ä½¿ç”¨ç«å¯¹å“ç‰Œ + æœ¬åº—ä¸‰çº§åˆ†ç±»åŒ¹é…æœ¬åº—çš„å“ç‰Œ+åˆ†ç±»åŠ ä»·ç‡
        cat3_a = row.get(cat3_col_a)
        if (pd.notna(brand_b) and brand_b != '' and 
            pd.notna(cat3_a) and 
            not brand_cat3_markup_orig.empty and 
            (brand_b, cat3_a) in brand_cat3_markup_orig.index):
            
            stats_orig = brand_cat3_markup_orig.loc[(brand_b, cat3_a)]
            mean_markup_orig = stats_orig['mean']
            count_orig = stats_orig['count']
            
            if count_orig >= MIN_BRAND_CATEGORY_SAMPLES and pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                # åŸä»·åŸºå‡†é¢„æµ‹
                cost_pred_orig = orig_price_b / mean_markup_orig
                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                
                # å”®ä»·åŠ æƒé¢„æµ‹ï¼ˆå¦‚æœå¯ç”¨ï¼‰
                use_sale_price = False
                sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT
                
                if (cfg.USE_SALE_PRICE_WEIGHT and 
                    pd.notna(sale_price_b) and sale_price_b > 0 and
                    not brand_cat3_markup_sale.empty and
                    (brand_b, cat3_a) in brand_cat3_markup_sale.index):
                    
                    discount_rate = sale_price_b / orig_price_b
                    if cfg.MIN_DISCOUNT_RATE <= discount_rate <= cfg.MAX_DISCOUNT_RATE:
                        stats_sale = brand_cat3_markup_sale.loc[(brand_b, cat3_a)]
                        mean_markup_sale = stats_sale['mean']
                        
                        if pd.notna(mean_markup_sale) and mean_markup_sale > 1.0:
                            cost_pred_sale = sale_price_b / mean_markup_sale
                            matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = cost_pred_sale
                            
                            prediction_diff_ratio = abs(cost_pred_orig - cost_pred_sale) / cost_pred_orig
                            if prediction_diff_ratio < 0.5:
                                use_sale_price = True
                                
                                if discount_rate < cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD:
                                    decay_factor = (discount_rate - cfg.MIN_DISCOUNT_RATE) / (cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD - cfg.MIN_DISCOUNT_RATE)
                                    sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT * decay_factor
                                    orig_price_weight_adjusted = 1 - sale_price_weight_adjusted
                                else:
                                    orig_price_weight_adjusted = cfg.ORIGINAL_PRICE_WEIGHT
                                
                                cost_pred_weighted = (cost_pred_orig * orig_price_weight_adjusted + 
                                                    cost_pred_sale * sale_price_weight_adjusted)
                                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_weighted
                                matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'å“ç‰Œ+ä¸‰çº§åˆ†ç±»({brand_b})(å”®ä»·åŠ æƒ{sale_price_weight_adjusted:.0%})'
                                
                                consistency = 1 - abs(cost_pred_orig - cost_pred_sale) / max(cost_pred_orig, cost_pred_sale)
                                base_confidence = 0.90  # ğŸ†• å“ç‰Œ+åˆ†ç±»æœ€é«˜ç½®ä¿¡åº¦
                                confidence = min(0.95, base_confidence * (0.8 + 0.2 * consistency))
                                
                                if is_barcode_match:
                                    confidence = min(0.95, confidence + 0.1)
                                    barcode_match_count += 1
                                
                                matched_df.at[idx, 'ç½®ä¿¡åº¦'] = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence)
                                weighted_count += 1
                                
                                # ğŸ†• å¼‚å¸¸æ£€æµ‹
                                current_cost = matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B']
                                adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                                    current_cost, row, store_a_df, cfg
                                )
                                if validation_flag != 'æ­£å¸¸':
                                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = adjusted_cost
                                    if adjusted_confidence is not None:
                                        matched_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                                    current_method = matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                                    matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                                
                                brand_cat3_match_count += 1  # ğŸ†• ç»Ÿè®¡å“ç‰Œ+ä¸‰çº§åˆ†ç±»å‘½ä¸­æ•°
                                continue  # ğŸ†• æ‰¾åˆ°å“ç‰Œ+åˆ†ç±»åŠ ä»·ç‡ï¼Œè·³è¿‡åç»­åˆ¤æ–­
                
                # æœªä½¿ç”¨å”®ä»·åŠ æƒï¼Œä»…ç”¨åŸä»·
                if not use_sale_price:
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_orig
                    matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'å“ç‰Œ+ä¸‰çº§åˆ†ç±»({brand_b})(åŸä»·)'
                    confidence = 0.90  # ğŸ†• å“ç‰Œ+åˆ†ç±»é«˜ç½®ä¿¡åº¦
                    
                    if is_barcode_match:
                        confidence = min(0.95, confidence + 0.1)
                        barcode_match_count += 1
                    
                    matched_df.at[idx, 'ç½®ä¿¡åº¦'] = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence)
                    
                    # ğŸ†• å¼‚å¸¸æ£€æµ‹
                    current_cost = matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B']
                    adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                        current_cost, row, store_a_df, cfg
                    )
                    if validation_flag != 'æ­£å¸¸':
                        matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = adjusted_cost
                        if adjusted_confidence is not None:
                            matched_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                        current_method = matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                        matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                    
                    brand_cat3_match_count += 1  # ğŸ†• ç»Ÿè®¡å“ç‰Œ+ä¸‰çº§åˆ†ç±»å‘½ä¸­æ•°
                    continue  # ğŸ†• æ‰¾åˆ°å“ç‰Œ+åˆ†ç±»åŠ ä»·ç‡ï¼Œè·³è¿‡åç»­åˆ¤æ–­
        
        # ç­–ç•¥2: ä¸‰çº§åˆ†ç±»åŠ ä»·ç‡ï¼ˆå«å”®ä»·åŠ æƒï¼‰
        cat3 = row.get(cat3_col_a)
        if pd.notna(cat3) and cat3 in category_markup_orig_level3.index:
            stats_orig = category_markup_orig_level3.loc[cat3]
            # ğŸ†• å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„ï¼ˆMultiIndex vs å•å±‚Indexï¼‰
            if isinstance(stats_orig, pd.DataFrame):
                # MultiIndex ç»“æ„ï¼ˆæ—§é€»è¾‘ï¼‰
                mean_markup_orig = stats_orig[('markup_rate_åŸä»·', 'mean')]
                std_markup_orig = stats_orig[('markup_rate_åŸä»·', 'std')]
                count_orig = stats_orig[('markup_rate_åŸä»·', 'count')]
            else:
                # å•å±‚ Index ç»“æ„ï¼ˆåŠ æƒèšåˆåï¼‰
                mean_markup_orig = stats_orig['mean']
                std_markup_orig = stats_orig['std']
                count_orig = stats_orig['count']
            
            # æ£€æŸ¥åŠ ä»·ç‡æœ‰æ•ˆæ€§
            if count_orig >= cfg.COST_PREDICTION_MIN_SAMPLES and pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                # åŸä»·åŸºå‡†é¢„æµ‹
                cost_pred_orig = orig_price_b / mean_markup_orig
                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                
                # ğŸ†• å”®ä»·åŠ æƒé¢„æµ‹ï¼ˆå«æç«¯æŠ˜æ‰£ä¿æŠ¤ï¼‰
                use_sale_price = False  # æ˜¯å¦ä½¿ç”¨å”®ä»·åŠ æƒ
                discount_rate = None
                sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT  # åŠ¨æ€è°ƒæ•´çš„å”®ä»·æƒé‡
                
                if (cfg.USE_SALE_PRICE_WEIGHT and 
                    pd.notna(sale_price_b) and sale_price_b > 0 and
                    cat3 in category_markup_sale_level3.index):
                    
                    # ğŸ›¡ï¸ æç«¯æŠ˜æ‰£æ£€æµ‹
                    discount_rate = sale_price_b / orig_price_b
                    
                    # æ£€æŸ¥1: æŠ˜æ‰£ç‡æ˜¯å¦åœ¨åˆç†èŒƒå›´å†…
                    if cfg.MIN_DISCOUNT_RATE <= discount_rate <= cfg.MAX_DISCOUNT_RATE:
                        stats_sale = category_markup_sale_level3.loc[cat3]
                        # ğŸ†• å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„
                        if isinstance(stats_sale, pd.DataFrame):
                            mean_markup_sale = stats_sale[('markup_rate_å”®ä»·', 'mean')]
                        else:
                            mean_markup_sale = stats_sale['mean']
                        
                        if pd.notna(mean_markup_sale) and mean_markup_sale > 1.0:
                            cost_pred_sale = sale_price_b / mean_markup_sale
                            matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = cost_pred_sale
                            
                            # æ£€æŸ¥2: åŸä»·/å”®ä»·é¢„æµ‹å·®å¼‚æ˜¯å¦è¿‡å¤§ï¼ˆé˜²æ­¢å”®ä»·æç«¯æ³¢åŠ¨ï¼‰
                            prediction_diff_ratio = abs(cost_pred_orig - cost_pred_sale) / cost_pred_orig
                            
                            if prediction_diff_ratio < 0.5:  # å·®å¼‚<50%ï¼Œå¯ä»¥ä½¿ç”¨å”®ä»·
                                use_sale_price = True
                                
                                # ğŸ”§ æŠ˜æ‰£ç‡åŠ¨æ€æƒé‡è°ƒæ•´
                                # æŠ˜æ‰£è¶Šæ·±ï¼ˆå¼•æµå“ï¼‰ï¼Œå”®ä»·æƒé‡è¶Šä½
                                if discount_rate < cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD:
                                    # æŠ˜æ‰£ç‡50%-70%æ—¶ï¼Œå”®ä»·æƒé‡çº¿æ€§è¡°å‡
                                    decay_factor = (discount_rate - cfg.MIN_DISCOUNT_RATE) / (cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD - cfg.MIN_DISCOUNT_RATE)
                                    sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT * decay_factor
                                    orig_price_weight_adjusted = 1 - sale_price_weight_adjusted
                                else:
                                    # æŠ˜æ‰£ç‡>=70%ï¼Œæ­£å¸¸æƒé‡
                                    orig_price_weight_adjusted = cfg.ORIGINAL_PRICE_WEIGHT
                                
                                # åŠ æƒå¹³å‡
                                cost_pred_weighted = (cost_pred_orig * orig_price_weight_adjusted + 
                                                    cost_pred_sale * sale_price_weight_adjusted)
                                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_weighted
                                matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸‰çº§åˆ†ç±»(å”®ä»·åŠ æƒ{sale_price_weight_adjusted:.0%})'
                                
                                # ç½®ä¿¡åº¦è°ƒæ•´ï¼šåŸä»·/å”®ä»·é¢„æµ‹ä¸€è‡´æ€§
                                consistency = 1 - abs(cost_pred_orig - cost_pred_sale) / max(cost_pred_orig, cost_pred_sale)
                                base_confidence = 0.5 + (count_orig / 50) * 0.2
                                if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                                    base_confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                                confidence = min(0.90, base_confidence * (0.8 + 0.2 * consistency))
                                # ğŸ”§ æ¡ç åŒ¹é…æå‡ç½®ä¿¡åº¦
                                if is_barcode_match:
                                    confidence = min(0.95, confidence + 0.1)
                                    barcode_match_count += 1
                                matched_df.at[idx, 'ç½®ä¿¡åº¦'] = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence)
                                weighted_count += 1
                
                # å¦‚æœä¸ä½¿ç”¨å”®ä»·åŠ æƒï¼ˆæŠ˜æ‰£å¼‚å¸¸æˆ–é…ç½®å…³é—­ï¼‰
                if not use_sale_price:
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_orig
                    
                    # æ ‡æ³¨åŸå› 
                    if discount_rate is not None:
                        if discount_rate < cfg.MIN_DISCOUNT_RATE:
                            matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸‰çº§åˆ†ç±»(åŸä»·) [å”®ä»·å¼‚å¸¸ä½{discount_rate:.0%}]'
                        elif discount_rate > cfg.MAX_DISCOUNT_RATE:
                            matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸‰çº§åˆ†ç±»(åŸä»·) [å”®ä»·é«˜äºåŸä»·]'
                        else:
                            matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸‰çº§åˆ†ç±»(åŸä»·) [å”®ä»·é¢„æµ‹å·®å¼‚å¤§]'
                    else:
                        matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸‰çº§åˆ†ç±»(åŸä»·)'
                    
                    confidence = 0.5 + (count_orig / 50) * 0.2
                    if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                        confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                    # ğŸ”§ æ¡ç åŒ¹é…æå‡ç½®ä¿¡åº¦
                    if is_barcode_match:
                        confidence = min(0.95, confidence + 0.1)
                        barcode_match_count += 1
                    matched_df.at[idx, 'ç½®ä¿¡åº¦'] = min(0.85, max(cfg.COST_CONFIDENCE_THRESHOLD, confidence))
                
                # ğŸ†• æ–¹æ¡ˆC: å¼‚å¸¸æ£€æµ‹éªŒè¯ï¼ˆä¸‰çº§åˆ†ç±»é¢„æµ‹ï¼‰
                current_cost = matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B']
                adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                    current_cost, row, store_a_df, cfg
                )
                
                if validation_flag != 'æ­£å¸¸':
                    # å¼‚å¸¸æ£€æµ‹è§¦å‘ï¼Œè°ƒæ•´é¢„æµ‹
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = adjusted_cost
                    if adjusted_confidence is not None:
                        matched_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                    
                    # æ›´æ–°é¢„æµ‹æ–¹æ³•æ ‡è®°
                    current_method = matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                    matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                
                cat3_match_count += 1
                continue
        
        # ğŸ†• æ–¹æ¡ˆB ä¼˜å…ˆçº§3: å“ç‰Œ+ä¸€çº§åˆ†ç±»åŠ ä»·ç‡
        # ä½¿ç”¨ç«å¯¹å“ç‰Œ + æœ¬åº—ä¸€çº§åˆ†ç±»åŒ¹é…æœ¬åº—çš„å“ç‰Œ+åˆ†ç±»åŠ ä»·ç‡
        cat1_a = row.get(cat1_col_a)
        if (pd.notna(brand_b) and brand_b != '' and 
            pd.notna(cat1_a) and 
            not brand_cat1_markup_orig.empty and 
            (brand_b, cat1_a) in brand_cat1_markup_orig.index):
            
            stats_orig = brand_cat1_markup_orig.loc[(brand_b, cat1_a)]
            mean_markup_orig = stats_orig['mean']
            count_orig = stats_orig['count']
            
            if count_orig >= MIN_BRAND_CATEGORY_SAMPLES and pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                cost_pred_orig = orig_price_b / mean_markup_orig
                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_orig
                matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'å“ç‰Œ+ä¸€çº§åˆ†ç±»({brand_b})(åŸä»·)'
                confidence = 0.75  # ğŸ†• å“ç‰Œ+ä¸€çº§åˆ†ç±»ä¸­ç­‰ç½®ä¿¡åº¦
                
                if is_barcode_match:
                    confidence = min(0.95, confidence + 0.1)
                    barcode_match_count += 1
                
                matched_df.at[idx, 'ç½®ä¿¡åº¦'] = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence)
                
                # ğŸ†• å¼‚å¸¸æ£€æµ‹
                current_cost = matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B']
                adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                    current_cost, row, store_a_df, cfg
                )
                if validation_flag != 'æ­£å¸¸':
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = adjusted_cost
                    if adjusted_confidence is not None:
                        matched_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                    current_method = matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                    matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                
                brand_cat1_match_count += 1  # ğŸ†• ç»Ÿè®¡å“ç‰Œ+ä¸€çº§åˆ†ç±»å‘½ä¸­æ•°
                continue  # ğŸ†• æ‰¾åˆ°å“ç‰Œ+ä¸€çº§åˆ†ç±»åŠ ä»·ç‡ï¼Œè·³è¿‡åç»­åˆ¤æ–­
        
        # ç­–ç•¥3: ä¸€çº§åˆ†ç±»åŠ ä»·ç‡ï¼ˆå…œåº•ï¼ŒåŒæ ·æ”¯æŒå”®ä»·åŠ æƒï¼‰
        cat1 = row.get(cat1_col_a)
        if pd.notna(cat1) and cat1 in category_markup_orig_level1.index:
            stats_orig = category_markup_orig_level1.loc[cat1]
            # ğŸ†• å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„
            if isinstance(stats_orig, pd.DataFrame):
                mean_markup_orig = stats_orig[('markup_rate_åŸä»·', 'mean')]
                std_markup_orig = stats_orig[('markup_rate_åŸä»·', 'std')]
                count_orig = stats_orig[('markup_rate_åŸä»·', 'count')]
            else:
                mean_markup_orig = stats_orig['mean']
                std_markup_orig = stats_orig['std']
                count_orig = stats_orig['count']
            
            if pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                cost_pred_orig = orig_price_b / mean_markup_orig
                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                
                # å”®ä»·åŠ æƒï¼ˆä¸€çº§åˆ†ç±»ï¼Œå«æç«¯æŠ˜æ‰£ä¿æŠ¤ï¼‰
                use_sale_price = False
                discount_rate = None
                sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT
                
                if (cfg.USE_SALE_PRICE_WEIGHT and 
                    pd.notna(sale_price_b) and sale_price_b > 0 and
                    cat1 in category_markup_sale_level1.index):
                    
                    # ğŸ›¡ï¸ æç«¯æŠ˜æ‰£æ£€æµ‹
                    discount_rate = sale_price_b / orig_price_b
                    
                    if cfg.MIN_DISCOUNT_RATE <= discount_rate <= cfg.MAX_DISCOUNT_RATE:
                        stats_sale = category_markup_sale_level1.loc[cat1]
                        # ğŸ†• å…¼å®¹ä¸¤ç§æ•°æ®ç»“æ„
                        if isinstance(stats_sale, pd.DataFrame):
                            mean_markup_sale = stats_sale[('markup_rate_å”®ä»·', 'mean')]
                        else:
                            mean_markup_sale = stats_sale['mean']
                        
                        if pd.notna(mean_markup_sale) and mean_markup_sale > 1.0:
                            cost_pred_sale = sale_price_b / mean_markup_sale
                            matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = cost_pred_sale
                            
                            prediction_diff_ratio = abs(cost_pred_orig - cost_pred_sale) / cost_pred_orig
                            
                            if prediction_diff_ratio < 0.5:
                                use_sale_price = True
                                
                                # æŠ˜æ‰£ç‡åŠ¨æ€æƒé‡è°ƒæ•´
                                if discount_rate < cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD:
                                    decay_factor = (discount_rate - cfg.MIN_DISCOUNT_RATE) / (cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD - cfg.MIN_DISCOUNT_RATE)
                                    sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT * decay_factor
                                    orig_price_weight_adjusted = 1 - sale_price_weight_adjusted
                                else:
                                    orig_price_weight_adjusted = cfg.ORIGINAL_PRICE_WEIGHT
                                
                                cost_pred_weighted = (cost_pred_orig * orig_price_weight_adjusted + 
                                                    cost_pred_sale * sale_price_weight_adjusted)
                                matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_weighted
                                matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸€çº§åˆ†ç±»(å”®ä»·åŠ æƒ{sale_price_weight_adjusted:.0%})'
                                
                                consistency = 1 - abs(cost_pred_orig - cost_pred_sale) / max(cost_pred_orig, cost_pred_sale)
                                base_confidence = 0.4 + (count_orig / 100) * 0.2
                                if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                                    base_confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                                confidence = min(0.75, base_confidence * (0.8 + 0.2 * consistency))
                                matched_df.at[idx, 'ç½®ä¿¡åº¦'] = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence)
                                weighted_count += 1
                
                if not use_sale_price:
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_orig
                    
                    if discount_rate is not None:
                        if discount_rate < cfg.MIN_DISCOUNT_RATE:
                            matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸€çº§åˆ†ç±»(åŸä»·) [å”®ä»·å¼‚å¸¸ä½{discount_rate:.0%}]'
                        elif discount_rate > cfg.MAX_DISCOUNT_RATE:
                            matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸€çº§åˆ†ç±»(åŸä»·) [å”®ä»·é«˜äºåŸä»·]'
                        else:
                            matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸€çº§åˆ†ç±»(åŸä»·) [å”®ä»·é¢„æµ‹å·®å¼‚å¤§]'
                    else:
                        matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸€çº§åˆ†ç±»(åŸä»·)'
                    
                    confidence = 0.4 + (count_orig / 100) * 0.2
                    if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                        confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                    matched_df.at[idx, 'ç½®ä¿¡åº¦'] = min(0.70, max(cfg.COST_CONFIDENCE_THRESHOLD, confidence))
                
                # ğŸ†• æ–¹æ¡ˆC: å¼‚å¸¸æ£€æµ‹éªŒè¯ï¼ˆä¸€çº§åˆ†ç±»é¢„æµ‹ï¼‰
                current_cost = matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B']
                adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                    current_cost, row, store_a_df, cfg
                )
                
                if validation_flag != 'æ­£å¸¸':
                    # å¼‚å¸¸æ£€æµ‹è§¦å‘ï¼Œè°ƒæ•´é¢„æµ‹
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = adjusted_cost
                    if adjusted_confidence is not None:
                        matched_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                    
                    # æ›´æ–°é¢„æµ‹æ–¹æ³•æ ‡è®°
                    current_method = matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                    matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                
                cat1_match_count += 1
                continue  # ğŸ†• æ‰¾åˆ°ä¸€çº§åˆ†ç±»ï¼Œè·³è¿‡ä»·æ ¼åŒºé—´å…œåº•
        
        # ğŸ†• æ–¹æ¡ˆB ä¼˜å…ˆçº§5: ä»·æ ¼åŒºé—´åŠ ä»·ç‡ï¼ˆæœ€åå…œåº•ï¼‰
        if not price_range_markup_orig.empty:
            price_range = get_price_range(orig_price_b)
            if price_range and price_range in price_range_markup_orig.index:
                stats_orig = price_range_markup_orig.loc[price_range]
                mean_markup_orig = stats_orig['mean']
                count_orig = stats_orig['count']
                
                if pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                    cost_pred_orig = orig_price_b / mean_markup_orig
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                    matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = cost_pred_orig
                    matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä»·æ ¼åŒºé—´({price_range})'
                    confidence = 0.50  # ğŸ†• ä»·æ ¼åŒºé—´ä½ç½®ä¿¡åº¦
                    
                    if is_barcode_match:
                        confidence = min(0.95, confidence + 0.1)
                        barcode_match_count += 1
                    
                    matched_df.at[idx, 'ç½®ä¿¡åº¦'] = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence)
                    
                    # ğŸ†• å¼‚å¸¸æ£€æµ‹
                    current_cost = matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B']
                    adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                        current_cost, row, store_a_df, cfg
                    )
                    if validation_flag != 'æ­£å¸¸':
                        matched_df.at[idx, 'é¢„æµ‹æˆæœ¬_B'] = adjusted_cost
                        if adjusted_confidence is not None:
                            matched_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                        current_method = matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                        matched_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                    
                    price_range_match_count += 1  # ğŸ†• ç»Ÿè®¡ä»·æ ¼åŒºé—´å‘½ä¸­æ•°
    
    predicted_count = matched_df['é¢„æµ‹æˆæœ¬_B'].notna().sum()
    print(f"\n   âœ… æˆæœ¬é¢„æµ‹å®Œæˆ:")
    print(f"      æ¡ç ç²¾ç¡®åŒ¹é…: {barcode_match_count} ä¸ª")
    if brand_cat3_match_count > 0:  # ğŸ†• æ–¹æ¡ˆBç»Ÿè®¡
        print(f"      ğŸ†• å“ç‰Œ+ä¸‰çº§åˆ†ç±»: {brand_cat3_match_count} ä¸ª (ç½®ä¿¡åº¦0.90-0.95)")
    print(f"      ä¸‰çº§åˆ†ç±»é¢„æµ‹: {cat3_match_count} ä¸ª")
    if brand_cat1_match_count > 0:  # ğŸ†• æ–¹æ¡ˆBç»Ÿè®¡
        print(f"      ğŸ†• å“ç‰Œ+ä¸€çº§åˆ†ç±»: {brand_cat1_match_count} ä¸ª (ç½®ä¿¡åº¦0.75)")
    print(f"      ä¸€çº§åˆ†ç±»é¢„æµ‹: {cat1_match_count} ä¸ª")
    if price_range_match_count > 0:  # ğŸ†• æ–¹æ¡ˆBç»Ÿè®¡
        print(f"      ğŸ†• ä»·æ ¼åŒºé—´å…œåº•: {price_range_match_count} ä¸ª (ç½®ä¿¡åº¦0.50)")
    if weighted_count > 0:
        print(f"      å”®ä»·åŠ æƒä¼˜åŒ–: {weighted_count} ä¸ª")
    print(f"      æ€»é¢„æµ‹æ•°é‡: {predicted_count} / {len(matched_df)}")
    
    return matched_df


def predict_all_competitor_products_cost(store_b_df, store_a_df, cfg=None):
    """
    ğŸ†• å¯¹ç«å¯¹æ‰€æœ‰å•†å“ï¼ˆåŒ…æ‹¬ç‹¬æœ‰å•†å“ï¼‰è¿›è¡Œæˆæœ¬å€’æ¨
    
    ç­–ç•¥ï¼š
    1. åŸºäºå•†å“è‡ªèº«çš„åˆ†ç±»åŒ¹é…æœ¬åº—çš„å“ç±»åŠ ä»·ç‡
    2. ä¼˜å…ˆä½¿ç”¨ä¸‰çº§åˆ†ç±»ï¼Œé™çº§åˆ°ä¸€çº§åˆ†ç±»
    3. åŒæ ·æ”¯æŒå”®ä»·åŠ æƒé¢„æµ‹
    
    Args:
        store_b_df: ç«å¯¹æ‰€æœ‰å•†å“æ•°æ®
        store_a_df: æœ¬åº—åŸå§‹æ•°æ®ï¼ˆå«æˆæœ¬ï¼Œç”¨äºè®¡ç®—åŠ ä»·ç‡ï¼‰
        cfg: é…ç½®å¯¹è±¡
    
    Returns:
        DataFrame: æ·»åŠ äº†é¢„æµ‹æˆæœ¬åˆ—çš„ç«å¯¹å•†å“æ•°æ®
    """
    if cfg is None:
        cfg = Config()
    
    cost_col = cfg.COST_COLUMN_NAME
    
    # æ£€æŸ¥æ˜¯å¦æœ‰æˆæœ¬æ•°æ®
    if cost_col not in store_a_df.columns:
        print("   âš ï¸  æœ¬åº—æ•°æ®ä¸­æœªæ‰¾åˆ°æˆæœ¬åˆ—ï¼Œè·³è¿‡å…¨å•†å“æˆæœ¬é¢„æµ‹")
        return store_b_df
    
    print("\n" + "="*60)
    print("ğŸ§® ç«å¯¹å…¨å•†å“æˆæœ¬é¢„æµ‹åˆ†æ")
    print("="*60)
    
    # è®¡ç®—æœ¬åº—åŸä»·åŠ ä»·ç‡å’Œå”®ä»·åŠ ä»·ç‡ï¼ˆğŸ†• æ–¹æ¡ˆAï¼šé”€é‡åŠ æƒï¼‰
    store_a_with_markup = calculate_markup_rate(store_a_df.copy(), cost_col, 'åŸä»·', '_åŸä»·', use_weights=True)
    if 'å”®ä»·' in store_a_df.columns and cfg.USE_SALE_PRICE_WEIGHT:
        store_a_with_markup = calculate_markup_rate(store_a_with_markup, cost_col, 'å”®ä»·', '_å”®ä»·', use_weights=True)
    
    # ğŸ†• æ–¹æ¡ˆA: åŠ æƒèšåˆå‡½æ•°ï¼ˆä¸predict_competitor_costä¿æŒä¸€è‡´ï¼‰
    def weighted_agg(df, value_col, weight_col):
        """åŠ æƒèšåˆå‡½æ•°"""
        if weight_col not in df.columns or df[weight_col].isna().all():
            return df[value_col].agg(['mean', 'std', 'count'])
        
        valid_mask = df[value_col].notna() & df[weight_col].notna()
        valid_df = df[valid_mask]
        
        if len(valid_df) == 0:
            return pd.Series({'mean': None, 'std': None, 'count': 0})
        
        weights = valid_df[weight_col]
        values = valid_df[value_col]
        
        weighted_mean = np.average(values, weights=weights)
        weighted_variance = np.average((values - weighted_mean) ** 2, weights=weights)
        weighted_std = np.sqrt(weighted_variance)
        
        return pd.Series({'mean': weighted_mean, 'std': weighted_std, 'count': len(valid_df)})
    
    # æŒ‰å“ç±»ç»Ÿè®¡åŸä»·åŠ ä»·ç‡ï¼ˆğŸ†• ä½¿ç”¨åŠ æƒï¼‰
    if 'sample_weight_åŸä»·' in store_a_with_markup.columns:
        category_markup_orig_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').apply(
            lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·')
        ).dropna()
        
        category_markup_orig_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').apply(
            lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·')
        ).dropna()
    else:
        category_markup_orig_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').agg({
            'markup_rate_åŸä»·': ['mean', 'std', 'count']
        }).dropna()
        
        category_markup_orig_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').agg({
            'markup_rate_åŸä»·': ['mean', 'std', 'count']
        }).dropna()
    
    # æŒ‰å“ç±»ç»Ÿè®¡å”®ä»·åŠ ä»·ç‡ï¼ˆå¦‚æœå¯ç”¨ï¼ŒğŸ†• åŒæ ·åŠ æƒï¼‰
    category_markup_sale_level3 = pd.DataFrame()
    category_markup_sale_level1 = pd.DataFrame()
    if 'markup_rate_å”®ä»·' in store_a_with_markup.columns and cfg.USE_SALE_PRICE_WEIGHT:
        if 'sample_weight_å”®ä»·' in store_a_with_markup.columns:
            category_markup_sale_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').apply(
                lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·')
            ).dropna()
            
            category_markup_sale_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').apply(
                lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·')
            ).dropna()
        else:
            category_markup_sale_level3 = store_a_with_markup.groupby('ç¾å›¢ä¸‰çº§åˆ†ç±»').agg({
                'markup_rate_å”®ä»·': ['mean', 'std', 'count']
            }).dropna()
            
            category_markup_sale_level1 = store_a_with_markup.groupby('ç¾å›¢ä¸€çº§åˆ†ç±»').agg({
                'markup_rate_å”®ä»·': ['mean', 'std', 'count']
            }).dropna()
    
    # ğŸ†• æ–¹æ¡ˆB: è®¡ç®—å“ç‰Œ+åˆ†ç±»ç»„åˆåŠ ä»·ç‡
    brand_cat3_markup_orig = pd.DataFrame()
    brand_cat1_markup_orig = pd.DataFrame()
    brand_cat3_markup_sale = pd.DataFrame()
    brand_cat1_markup_sale = pd.DataFrame()
    
    MIN_BRAND_CATEGORY_SAMPLES = 3
    
    if 'å“ç‰Œ' in store_a_with_markup.columns:
        if 'sample_weight_åŸä»·' in store_a_with_markup.columns:
            brand_cat3_markup_orig = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸‰çº§åˆ†ç±»']).apply(
                lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
            ).dropna()
            
            brand_cat1_markup_orig = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸€çº§åˆ†ç±»']).apply(
                lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
            ).dropna()
        
        if 'markup_rate_å”®ä»·' in store_a_with_markup.columns and cfg.USE_SALE_PRICE_WEIGHT:
            if 'sample_weight_å”®ä»·' in store_a_with_markup.columns:
                brand_cat3_markup_sale = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸‰çº§åˆ†ç±»']).apply(
                    lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
                ).dropna()
                
                brand_cat1_markup_sale = store_a_with_markup.groupby(['å“ç‰Œ', 'ç¾å›¢ä¸€çº§åˆ†ç±»']).apply(
                    lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·') if len(x) >= MIN_BRAND_CATEGORY_SAMPLES else pd.Series({'mean': None, 'std': None, 'count': 0})
                ).dropna()
    
    # ğŸ†• æ–¹æ¡ˆB: è®¡ç®—ä»·æ ¼åŒºé—´åŠ ä»·ç‡
    def get_price_range(price):
        if pd.isna(price) or price <= 0:
            return None
        if price < 10:
            return '0-10å…ƒ'
        elif price < 30:
            return '10-30å…ƒ'
        elif price < 50:
            return '30-50å…ƒ'
        elif price < 100:
            return '50-100å…ƒ'
        else:
            return '100å…ƒä»¥ä¸Š'
    
    price_range_markup_orig = pd.DataFrame()
    price_range_markup_sale = pd.DataFrame()
    
    store_a_with_markup['ä»·æ ¼åŒºé—´'] = store_a_with_markup['åŸä»·'].apply(get_price_range)
    
    if 'sample_weight_åŸä»·' in store_a_with_markup.columns:
        price_range_markup_orig = store_a_with_markup.groupby('ä»·æ ¼åŒºé—´').apply(
            lambda x: weighted_agg(x, 'markup_rate_åŸä»·', 'sample_weight_åŸä»·')
        ).dropna()
    
    if 'markup_rate_å”®ä»·' in store_a_with_markup.columns and cfg.USE_SALE_PRICE_WEIGHT:
        if 'sample_weight_å”®ä»·' in store_a_with_markup.columns:
            price_range_markup_sale = store_a_with_markup.groupby('ä»·æ ¼åŒºé—´').apply(
                lambda x: weighted_agg(x, 'markup_rate_å”®ä»·', 'sample_weight_å”®ä»·')
            ).dropna()
    
    print(f"   ğŸ“Š æœ¬åº—åŠ ä»·ç‡ç»Ÿè®¡ï¼ˆæ–¹æ¡ˆA+Bï¼šé”€é‡åŠ æƒ + å¤šç»´åˆ†å±‚ï¼‰ï¼š")
    print(f"      ä¸‰çº§åˆ†ç±»: {len(category_markup_orig_level3)}ä¸ª")
    print(f"      ä¸€çº§åˆ†ç±»: {len(category_markup_orig_level1)}ä¸ª")
    if not brand_cat3_markup_orig.empty or not brand_cat1_markup_orig.empty:
        print(f"      ğŸ†• å“ç‰Œ+ä¸‰çº§åˆ†ç±»: {len(brand_cat3_markup_orig)}ä¸ª")
        print(f"      ğŸ†• å“ç‰Œ+ä¸€çº§åˆ†ç±»: {len(brand_cat1_markup_orig)}ä¸ª")
    if not price_range_markup_orig.empty:
        print(f"      ğŸ†• ä»·æ ¼åŒºé—´: {len(price_range_markup_orig)}ä¸ª")

    
    # åˆå§‹åŒ–é¢„æµ‹åˆ—
    result_df = store_b_df.copy()
    result_df['é¢„æµ‹æˆæœ¬'] = None
    result_df['é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = None
    result_df['é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = None
    result_df['é¢„æµ‹æ–¹æ³•'] = None
    result_df['ç½®ä¿¡åº¦'] = None
    
    cat3_match_count = 0
    cat1_match_count = 0
    weighted_count = 0
    
    for idx, row in result_df.iterrows():
        # è·å–å•†å“ä»·æ ¼
        orig_price = row.get('åŸä»·')
        sale_price = row.get('å”®ä»·')
        
        if pd.isna(orig_price) or orig_price <= 0:
            continue
        
        # ç­–ç•¥1: ä¸‰çº§åˆ†ç±»åŠ ä»·ç‡ï¼ˆå«å”®ä»·åŠ æƒï¼‰
        cat3 = row.get('ç¾å›¢ä¸‰çº§åˆ†ç±»')
        if pd.notna(cat3) and cat3 in category_markup_orig_level3.index:
            stats_orig = category_markup_orig_level3.loc[cat3]
            # ğŸ†• å…¼å®¹åŠ æƒèšåˆåçš„å•å±‚Indexç»“æ„
            mean_markup_orig = stats_orig['mean']
            std_markup_orig = stats_orig['std']
            count_orig = stats_orig['count']
            
            if count_orig >= cfg.COST_PREDICTION_MIN_SAMPLES and pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                cost_pred_orig = orig_price / mean_markup_orig
                result_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                
                # å”®ä»·åŠ æƒé¢„æµ‹ï¼ˆå«æç«¯æŠ˜æ‰£ä¿æŠ¤ï¼‰
                use_sale_price = False
                discount_rate = None
                sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT
                
                if (cfg.USE_SALE_PRICE_WEIGHT and 
                    pd.notna(sale_price) and sale_price > 0 and
                    cat3 in category_markup_sale_level3.index):
                    
                    # ğŸ›¡ï¸ æç«¯æŠ˜æ‰£æ£€æµ‹
                    discount_rate = sale_price / orig_price
                    
                    if cfg.MIN_DISCOUNT_RATE <= discount_rate <= cfg.MAX_DISCOUNT_RATE:
                        stats_sale = category_markup_sale_level3.loc[cat3]
                        # ğŸ†• å…¼å®¹åŠ æƒèšåˆåçš„å•å±‚Indexç»“æ„
                        mean_markup_sale = stats_sale['mean']
                        
                        if pd.notna(mean_markup_sale) and mean_markup_sale > 1.0:
                            cost_pred_sale = sale_price / mean_markup_sale
                            result_df.at[idx, 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = cost_pred_sale
                            
                            prediction_diff_ratio = abs(cost_pred_orig - cost_pred_sale) / cost_pred_orig
                            
                            if prediction_diff_ratio < 0.5:
                                use_sale_price = True
                                
                                # æŠ˜æ‰£ç‡åŠ¨æ€æƒé‡è°ƒæ•´
                                if discount_rate < cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD:
                                    decay_factor = (discount_rate - cfg.MIN_DISCOUNT_RATE) / (cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD - cfg.MIN_DISCOUNT_RATE)
                                    sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT * decay_factor
                                    orig_price_weight_adjusted = 1 - sale_price_weight_adjusted
                                else:
                                    orig_price_weight_adjusted = cfg.ORIGINAL_PRICE_WEIGHT
                                
                                cost_pred_weighted = (cost_pred_orig * orig_price_weight_adjusted + 
                                                    cost_pred_sale * sale_price_weight_adjusted)
                                result_df.at[idx, 'é¢„æµ‹æˆæœ¬'] = cost_pred_weighted
                                result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸‰çº§åˆ†ç±»(å”®ä»·åŠ æƒ{sale_price_weight_adjusted:.0%})'
                                
                                consistency = 1 - abs(cost_pred_orig - cost_pred_sale) / max(cost_pred_orig, cost_pred_sale)
                                base_confidence = 0.5 + (count_orig / 50) * 0.2
                                if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                                    base_confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                                confidence = min(0.90, base_confidence * (0.8 + 0.2 * consistency))
                                
                                # ğŸ¯ éåŒ¹é…å•†å“ç½®ä¿¡åº¦æƒ©ç½š
                                confidence = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence - cfg.NON_MATCHED_CONFIDENCE_PENALTY)
                                result_df.at[idx, 'ç½®ä¿¡åº¦'] = confidence
                                weighted_count += 1
                
                if not use_sale_price:
                    result_df.at[idx, 'é¢„æµ‹æˆæœ¬'] = cost_pred_orig
                    
                    if discount_rate is not None:
                        if discount_rate < cfg.MIN_DISCOUNT_RATE:
                            result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸‰çº§åˆ†ç±»(åŸä»·) [å¼•æµå“{discount_rate:.0%}]'
                        elif discount_rate > cfg.MAX_DISCOUNT_RATE:
                            result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸‰çº§åˆ†ç±»(åŸä»·) [å”®ä»·é«˜äºåŸä»·]'
                        else:
                            result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸‰çº§åˆ†ç±»(åŸä»·) [å”®ä»·é¢„æµ‹å·®å¼‚å¤§]'
                    else:
                        result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸‰çº§åˆ†ç±»(åŸä»·)'
                    
                    confidence = 0.5 + (count_orig / 50) * 0.2
                    if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                        confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                    
                    # ğŸ¯ éåŒ¹é…å•†å“ç½®ä¿¡åº¦æƒ©ç½š
                    confidence = max(cfg.COST_CONFIDENCE_THRESHOLD, min(0.85, confidence) - cfg.NON_MATCHED_CONFIDENCE_PENALTY)
                    result_df.at[idx, 'ç½®ä¿¡åº¦'] = confidence
                
                # ğŸ†• æ–¹æ¡ˆC: å¼‚å¸¸æ£€æµ‹éªŒè¯ï¼ˆä¸‰çº§åˆ†ç±»ï¼Œå…¨å•†å“é¢„æµ‹ï¼‰
                current_cost = result_df.at[idx, 'é¢„æµ‹æˆæœ¬']
                adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                    current_cost, row, store_a_df, cfg
                )
                
                if validation_flag != 'æ­£å¸¸':
                    # å¼‚å¸¸æ£€æµ‹è§¦å‘ï¼Œè°ƒæ•´é¢„æµ‹
                    result_df.at[idx, 'é¢„æµ‹æˆæœ¬'] = adjusted_cost
                    if adjusted_confidence is not None:
                        result_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                    
                    # æ›´æ–°é¢„æµ‹æ–¹æ³•æ ‡è®°
                    current_method = result_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                    result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                
                cat3_match_count += 1
                continue
        
        # ç­–ç•¥2: ä¸€çº§åˆ†ç±»åŠ ä»·ç‡ï¼ˆå…œåº•ï¼‰
        cat1 = row.get('ç¾å›¢ä¸€çº§åˆ†ç±»')
        if pd.notna(cat1) and cat1 in category_markup_orig_level1.index:
            stats_orig = category_markup_orig_level1.loc[cat1]
            # ğŸ†• å…¼å®¹åŠ æƒèšåˆåçš„å•å±‚Indexç»“æ„
            mean_markup_orig = stats_orig['mean']
            std_markup_orig = stats_orig['std']
            count_orig = stats_orig['count']
            
            if pd.notna(mean_markup_orig) and mean_markup_orig > 1.0:
                cost_pred_orig = orig_price / mean_markup_orig
                result_df.at[idx, 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†'] = cost_pred_orig
                
                # å”®ä»·åŠ æƒï¼ˆä¸€çº§åˆ†ç±»ï¼Œå«æç«¯æŠ˜æ‰£ä¿æŠ¤ï¼‰
                use_sale_price = False
                discount_rate = None
                sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT
                
                if (cfg.USE_SALE_PRICE_WEIGHT and 
                    pd.notna(sale_price) and sale_price > 0 and
                    cat1 in category_markup_sale_level1.index):
                    
                    discount_rate = sale_price / orig_price
                    
                    if cfg.MIN_DISCOUNT_RATE <= discount_rate <= cfg.MAX_DISCOUNT_RATE:
                        stats_sale = category_markup_sale_level1.loc[cat1]
                        # ğŸ†• å…¼å®¹åŠ æƒèšåˆåçš„å•å±‚Indexç»“æ„
                        mean_markup_sale = stats_sale['mean']
                        
                        if pd.notna(mean_markup_sale) and mean_markup_sale > 1.0:
                            cost_pred_sale = sale_price / mean_markup_sale
                            result_df.at[idx, 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'] = cost_pred_sale
                            
                            prediction_diff_ratio = abs(cost_pred_orig - cost_pred_sale) / cost_pred_orig
                            
                            if prediction_diff_ratio < 0.5:
                                use_sale_price = True
                                
                                if discount_rate < cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD:
                                    decay_factor = (discount_rate - cfg.MIN_DISCOUNT_RATE) / (cfg.SALE_PRICE_WEIGHT_DECAY_THRESHOLD - cfg.MIN_DISCOUNT_RATE)
                                    sale_price_weight_adjusted = cfg.SALE_PRICE_WEIGHT * decay_factor
                                    orig_price_weight_adjusted = 1 - sale_price_weight_adjusted
                                else:
                                    orig_price_weight_adjusted = cfg.ORIGINAL_PRICE_WEIGHT
                                
                                cost_pred_weighted = (cost_pred_orig * orig_price_weight_adjusted + 
                                                    cost_pred_sale * sale_price_weight_adjusted)
                                result_df.at[idx, 'é¢„æµ‹æˆæœ¬'] = cost_pred_weighted
                                result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸€çº§åˆ†ç±»(å”®ä»·åŠ æƒ{sale_price_weight_adjusted:.0%})'
                                
                                consistency = 1 - abs(cost_pred_orig - cost_pred_sale) / max(cost_pred_orig, cost_pred_sale)
                                base_confidence = 0.4 + (count_orig / 100) * 0.2
                                if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                                    base_confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                                confidence = min(0.75, base_confidence * (0.8 + 0.2 * consistency))
                                
                                # ğŸ¯ éåŒ¹é…å•†å“ç½®ä¿¡åº¦æƒ©ç½š
                                confidence = max(cfg.COST_CONFIDENCE_THRESHOLD, confidence - cfg.NON_MATCHED_CONFIDENCE_PENALTY)
                                result_df.at[idx, 'ç½®ä¿¡åº¦'] = confidence
                                weighted_count += 1
                
                if not use_sale_price:
                    result_df.at[idx, 'é¢„æµ‹æˆæœ¬'] = cost_pred_orig
                    
                    if discount_rate is not None:
                        if discount_rate < cfg.MIN_DISCOUNT_RATE:
                            result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸€çº§åˆ†ç±»(åŸä»·) [å¼•æµå“{discount_rate:.0%}]'
                        elif discount_rate > cfg.MAX_DISCOUNT_RATE:
                            result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f'ä¸€çº§åˆ†ç±»(åŸä»·) [å”®ä»·é«˜äºåŸä»·]'
                        else:
                            result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸€çº§åˆ†ç±»(åŸä»·) [å”®ä»·é¢„æµ‹å·®å¼‚å¤§]'
                    else:
                        result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = 'ä¸€çº§åˆ†ç±»(åŸä»·)'
                    
                    confidence = 0.4 + (count_orig / 100) * 0.2
                    if pd.notna(std_markup_orig) and mean_markup_orig > 0:
                        confidence -= (std_markup_orig / mean_markup_orig) * 0.3
                    
                    # ğŸ¯ éåŒ¹é…å•†å“ç½®ä¿¡åº¦æƒ©ç½š
                    confidence = max(cfg.COST_CONFIDENCE_THRESHOLD, min(0.70, confidence) - cfg.NON_MATCHED_CONFIDENCE_PENALTY)
                    result_df.at[idx, 'ç½®ä¿¡åº¦'] = confidence
                
                # ğŸ†• æ–¹æ¡ˆC: å¼‚å¸¸æ£€æµ‹éªŒè¯ï¼ˆä¸€çº§åˆ†ç±»ï¼Œå…¨å•†å“é¢„æµ‹ï¼‰
                current_cost = result_df.at[idx, 'é¢„æµ‹æˆæœ¬']
                adjusted_cost, adjusted_confidence, validation_flag = validate_cost_prediction(
                    current_cost, row, store_a_df, cfg
                )
                
                if validation_flag != 'æ­£å¸¸':
                    # å¼‚å¸¸æ£€æµ‹è§¦å‘ï¼Œè°ƒæ•´é¢„æµ‹
                    result_df.at[idx, 'é¢„æµ‹æˆæœ¬'] = adjusted_cost
                    if adjusted_confidence is not None:
                        result_df.at[idx, 'ç½®ä¿¡åº¦'] = adjusted_confidence
                    
                    # æ›´æ–°é¢„æµ‹æ–¹æ³•æ ‡è®°
                    current_method = result_df.at[idx, 'é¢„æµ‹æ–¹æ³•']
                    result_df.at[idx, 'é¢„æµ‹æ–¹æ³•'] = f"{current_method} [{validation_flag}]"
                
                cat1_match_count += 1
    
    predicted_count = result_df['é¢„æµ‹æˆæœ¬'].notna().sum()
    print(f"\n   âœ… å…¨å•†å“æˆæœ¬é¢„æµ‹å®Œæˆ:")
    print(f"      ä¸‰çº§åˆ†ç±»é¢„æµ‹: {cat3_match_count} ä¸ª")
    print(f"      ä¸€çº§åˆ†ç±»é¢„æµ‹: {cat1_match_count} ä¸ª")
    if weighted_count > 0:
        print(f"      å”®ä»·åŠ æƒä¼˜åŒ–: {weighted_count} ä¸ª")
    print(f"      æ€»é¢„æµ‹æ•°é‡: {predicted_count} / {len(result_df)}")
    print(f"      é¢„æµ‹è¦†ç›–ç‡: {predicted_count/len(result_df)*100:.1f}%")
    
    return result_df


def generate_cost_analysis_sheets(matched_df, store_b_all_df=None, cfg=None):
    """
    ç”Ÿæˆæˆæœ¬åˆ†æç›¸å…³çš„ Sheet
    
    Args:
        matched_df: åŒ¹é…å•†å“æ•°æ®ï¼ˆå«é¢„æµ‹æˆæœ¬ï¼‰
        store_b_all_df: ğŸ†• ç«å¯¹æ‰€æœ‰å•†å“æ•°æ®ï¼ˆå«é¢„æµ‹æˆæœ¬ï¼Œå¯é€‰ï¼‰
        cfg: é…ç½®å¯¹è±¡
    
    Returns:
        dict: {sheet_name: dataframe}
    """
    if cfg is None:
        cfg = Config()
    
    sheets = {}
    
    # è¿‡æ»¤æœ‰é¢„æµ‹æˆæœ¬çš„æ•°æ®
    df_with_cost = matched_df[matched_df['é¢„æµ‹æˆæœ¬_B'].notna()].copy()
    
    if df_with_cost.empty:
        print("   âš ï¸  æ— æˆæœ¬é¢„æµ‹æ•°æ®ï¼Œè·³è¿‡æˆæœ¬åˆ†æ Sheet ç”Ÿæˆ")
        return sheets
    
    print("\n" + "="*60)
    print("ğŸ“Š ç”Ÿæˆæˆæœ¬åˆ†ææŠ¥è¡¨")
    print("="*60)
    
    # è·å–åˆ—åï¼ˆå¤„ç†å¸¦åº—é“ºåç¼€çš„æƒ…å†µï¼‰- ä½¿ç”¨å’Œ predict_competitor_cost ç›¸åŒçš„é€»è¾‘
    def find_column(df, base_name, store_name_suffix=''):
        """æ™ºèƒ½æŸ¥æ‰¾åˆ—åï¼Œæ”¯æŒåº—é“ºååç¼€å’Œ _A/_B åç¼€"""
        # ä¼˜å…ˆæŸ¥æ‰¾å¸¦åº—é“ºåçš„åˆ—
        if store_name_suffix:
            col_with_store = f'{base_name}_{store_name_suffix}'
            if col_with_store in df.columns:
                return col_with_store
        # å›é€€åˆ° _A/_B åç¼€
        for suffix in ['_A', '_B']:
            col_with_suffix = f'{base_name}{suffix}'
            if col_with_suffix in df.columns:
                return col_with_suffix
        # æœ€åå°è¯•æ— åç¼€
        if base_name in df.columns:
            return base_name
        return None
    
    name_a_col = find_column(df_with_cost, 'å•†å“åç§°', cfg.STORE_A_NAME if cfg else '')
    name_b_col = find_column(df_with_cost, 'å•†å“åç§°', cfg.STORE_B_NAME if cfg else '')
    price_a_col = find_column(df_with_cost, 'åŸä»·', cfg.STORE_A_NAME if cfg else '')
    price_b_col = find_column(df_with_cost, 'åŸä»·', cfg.STORE_B_NAME if cfg else '')
    
    # æ£€æŸ¥å¿…éœ€åˆ—æ˜¯å¦å­˜åœ¨
    if not all([name_a_col, name_b_col, price_a_col, price_b_col]):
        print(f"   âš ï¸  ç¼ºå°‘å¿…éœ€åˆ—ï¼Œæ— æ³•ç”Ÿæˆæˆæœ¬åˆ†æ:")
        print(f"      å•†å“åç§°_A: {name_a_col}")
        print(f"      å•†å“åç§°_B: {name_b_col}")
        print(f"      åŸä»·_A: {price_a_col}")
        print(f"      åŸä»·_B: {price_b_col}")
        return sheets
    
    # Sheet 1: ç«å¯¹æˆæœ¬é¢„æµ‹æ±‡æ€»
    # ğŸ”§ ä¿®å¤é—®é¢˜3ï¼šæ˜¾ç¤ºä¸¤ç§é¢„æµ‹æˆæœ¬é€»è¾‘
    # ğŸ”§ ä¿®å¤é—®é¢˜2ï¼šç¡®ä¿æˆæœ¬_Aï¼ˆæˆæœ¬_æœ¬åº—ï¼‰æœ‰æ•°æ®
    
    # æŸ¥æ‰¾æœ¬åº—æˆæœ¬åˆ—ï¼ˆæ™ºèƒ½æŸ¥æ‰¾ï¼‰
    # ğŸ”§ ä¿®å¤ï¼šä¼˜å…ˆæŸ¥æ‰¾åŸå§‹æˆæœ¬åˆ—ï¼ˆä¸å¸¦_Aåç¼€ï¼‰
    cost_col_name_a = None
    if cfg and cfg.COST_COLUMN_NAME:
        # 1. å°è¯•å¸¦åº—é“ºåçš„åˆ—
        store_name_col = f'{cfg.COST_COLUMN_NAME}_{cfg.STORE_A_NAME}'
        if store_name_col in df_with_cost.columns:
            cost_col_name_a = store_name_col
        # 2. å°è¯•ä¸å¸¦åç¼€çš„åŸå§‹åˆ—
        elif cfg.COST_COLUMN_NAME in df_with_cost.columns:
            cost_col_name_a = cfg.COST_COLUMN_NAME
    
    # å¦‚æœæˆæœ¬_Aåˆ—ä¸å­˜åœ¨æˆ–ä¸ºç©ºï¼Œä»åŸå§‹åˆ—å¤åˆ¶æ•°æ®
    if 'æˆæœ¬_A' not in df_with_cost.columns or df_with_cost['æˆæœ¬_A'].isna().all():
        print(f"   ğŸ”§ ä¿®å¤ï¼šæˆæœ¬_Aåˆ—ç¼ºå¤±æˆ–ä¸ºç©ºï¼Œä» {cost_col_name_a} å¤åˆ¶æ•°æ®")
        if cost_col_name_a and cost_col_name_a in df_with_cost.columns and cost_col_name_a != 'æˆæœ¬_A':
            df_with_cost['æˆæœ¬_A'] = df_with_cost[cost_col_name_a]
            print(f"   âœ… å·²ä» {cost_col_name_a} å¤åˆ¶ {df_with_cost['æˆæœ¬_A'].notna().sum()} æ¡æˆæœ¬æ•°æ®")
        else:
            print(f"   âš ï¸  è­¦å‘Šï¼šæœªæ‰¾åˆ°æœ¬åº—æˆæœ¬åˆ—ï¼ˆæŸ¥æ‰¾ï¼š{cost_col_name_a}ï¼‰ï¼Œæˆæœ¬_Aå°†ä¸ºç©º")
            df_with_cost['æˆæœ¬_A'] = None
    
    cost_prediction_cols = [
        name_a_col, name_b_col,
        # ABABæ¨¡å¼ï¼šæœ¬åº—ä»·æ ¼å’Œæˆæœ¬
        price_a_col, 'æˆæœ¬_A',
        # ABABæ¨¡å¼ï¼šç«å¯¹ä»·æ ¼å’Œä¸¤ç§é¢„æµ‹æˆæœ¬
        price_b_col, 
        'é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰',  # ğŸ†• é‡å‘½åä»¥æ˜ç¡®
        'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†',      # ğŸ†• æ˜¾ç¤ºåŸä»·åŸºå‡†é¢„æµ‹
        'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†',      # ğŸ†• æ˜¾ç¤ºå”®ä»·åŸºå‡†é¢„æµ‹ï¼ˆå¦‚æœæœ‰ï¼‰
        'é¢„æµ‹æ–¹æ³•', 'ç½®ä¿¡åº¦'
    ]
    
    # ğŸ†• å°†'é¢„æµ‹æˆæœ¬_B'é‡å‘½åä¸º'é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰'ä»¥æ˜ç¡®å…¶å«ä¹‰
    if 'é¢„æµ‹æˆæœ¬_B' in df_with_cost.columns:
        df_with_cost['é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_with_cost['é¢„æµ‹æˆæœ¬_B']
    
    # æ·»åŠ å¯é€‰åˆ—ï¼ˆæ™ºèƒ½æŸ¥æ‰¾ï¼‰
    optional_base_cols = ['ç¾å›¢ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'æ¡ç ', 'æœˆå”®']
    for base_col in optional_base_cols:
        col_a = find_column(df_with_cost, base_col, cfg.STORE_A_NAME if cfg else '')
        col_b = find_column(df_with_cost, base_col, cfg.STORE_B_NAME if cfg else '')
        if col_a:
            cost_prediction_cols.append(col_a)
        if col_b:
            cost_prediction_cols.append(col_b)
    
    # è®¡ç®—æˆæœ¬å·®å’Œä¼˜åŠ¿ï¼ˆåŸºäºå”®ä»·åŠ æƒç‰ˆï¼‰
    df_with_cost['æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_with_cost['æˆæœ¬_A'] - df_with_cost['é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰']
    
    # ğŸ†• è®¡ç®—åŸä»·åŸºå‡†çš„æˆæœ¬å·®
    if 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†' in df_with_cost.columns:
        df_with_cost['æˆæœ¬å·®ï¼ˆçº¯åŸä»·ï¼‰'] = df_with_cost['æˆæœ¬_A'] - df_with_cost['é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†']
    
    df_with_cost['æˆæœ¬ä¼˜åŠ¿'] = df_with_cost['æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'].apply(
        lambda x: 'æœ¬åº—æˆæœ¬ä½' if pd.notna(x) and x < -1 else ('ç«å¯¹æˆæœ¬ä½' if pd.notna(x) and x > 1 else 'æˆæœ¬ç›¸è¿‘')
    )
    cost_prediction_cols.extend(['æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰', 'æˆæœ¬å·®ï¼ˆçº¯åŸä»·ï¼‰', 'æˆæœ¬ä¼˜åŠ¿'])
    
    cost_prediction_cols = [col for col in cost_prediction_cols if col in df_with_cost.columns]
    sheets['ç«å¯¹æˆæœ¬é¢„æµ‹'] = df_with_cost[cost_prediction_cols].copy()
    
    # Sheet 2: åˆ©æ¶¦ç©ºé—´å¯¹æ¯”ï¼ˆåŒè§†è§’ï¼‰
    df_profit = df_with_cost.copy()
    
    # ğŸ”§ ä¿®å¤é—®é¢˜2ï¼šç¡®ä¿æˆæœ¬_Aæœ‰æ•°æ®
    # ï¼ˆå·²ç»åœ¨Sheet 1ä¸­ä¿®å¤ï¼Œè¿™é‡Œdf_profitç»§æ‰¿df_with_costçš„ä¿®å¤ï¼‰
    
    # === æœ¬åº—ï¼ˆAåº—ï¼‰æ¯›åˆ©è®¡ç®— ===
    df_profit['æ¯›åˆ©_A'] = df_profit[price_a_col] - df_profit['æˆæœ¬_A']
    df_profit['æ¯›åˆ©ç‡_A'] = df_profit.apply(
        lambda row: (row['æ¯›åˆ©_A'] / row[price_a_col] * 100) if pd.notna(row['æ¯›åˆ©_A']) and pd.notna(row[price_a_col]) and row[price_a_col] > 0 and pd.notna(row['æˆæœ¬_A']) else None,
        axis=1
    )
    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ¯›åˆ©ç‡_Aæ˜¯æ•°å€¼ç±»å‹å†round
    if pd.api.types.is_numeric_dtype(df_profit['æ¯›åˆ©ç‡_A']):
        df_profit['æ¯›åˆ©ç‡_A'] = df_profit['æ¯›åˆ©ç‡_A'].round(2)
    else:
        df_profit['æ¯›åˆ©ç‡_A'] = pd.to_numeric(df_profit['æ¯›åˆ©ç‡_A'], errors='coerce').round(2)
    
    # === ç«å¯¹ï¼ˆBåº—ï¼‰å”®ä»·åŠ æƒç‰ˆæ¯›åˆ© ===
    # ğŸ”§ ä½¿ç”¨ç»Ÿä¸€çš„åˆ—å'é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰'
    if 'é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰' not in df_profit.columns and 'é¢„æµ‹æˆæœ¬_B' in df_profit.columns:
        df_profit['é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_profit['é¢„æµ‹æˆæœ¬_B']
    
    df_profit['é¢„æµ‹æ¯›åˆ©_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_profit[price_b_col] - df_profit['é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰']
    df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_profit.apply(
        lambda row: (row['é¢„æµ‹æ¯›åˆ©_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] / row[price_b_col] * 100) if pd.notna(row.get('é¢„æµ‹æ¯›åˆ©_Bï¼ˆå”®ä»·åŠ æƒï¼‰')) and pd.notna(row[price_b_col]) and row[price_b_col] > 0 else None,
        axis=1
    )
    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°å€¼ç±»å‹å†round
    if pd.api.types.is_numeric_dtype(df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰']):
        df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰'].round(2)
    else:
        df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰'] = pd.to_numeric(df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰'], errors='coerce').round(2)
    
    # === ç«å¯¹ï¼ˆBåº—ï¼‰çº¯åŸä»·ç‰ˆæ¯›åˆ© ===
    # ä½¿ç”¨é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†' in df_profit.columns:
        df_profit['é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰'] = df_profit['é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†']
    else:
        df_profit['é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰'] = df_profit['é¢„æµ‹æˆæœ¬_B']  # å›é€€æ–¹æ¡ˆ
    
    df_profit['é¢„æµ‹æ¯›åˆ©_Bï¼ˆçº¯åŸä»·ï¼‰'] = df_profit[price_b_col] - df_profit['é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰']
    df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'] = df_profit.apply(
        lambda row: (row['é¢„æµ‹æ¯›åˆ©_Bï¼ˆçº¯åŸä»·ï¼‰'] / row[price_b_col] * 100) if pd.notna(row.get('é¢„æµ‹æ¯›åˆ©_Bï¼ˆçº¯åŸä»·ï¼‰')) and pd.notna(row[price_b_col]) and row[price_b_col] > 0 else None,
        axis=1
    )
    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°å€¼ç±»å‹å†round
    if pd.api.types.is_numeric_dtype(df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰']):
        df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'] = df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'].round(2)
    else:
        df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'] = pd.to_numeric(df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'], errors='coerce').round(2)
    
    # === æ¯›åˆ©ç‡å¯¹æ¯”åˆ†æ ===
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨fillnaç¡®ä¿æ•°å€¼ç±»å‹
    df_profit['æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'] = (df_profit['æ¯›åˆ©ç‡_A'].fillna(0) - df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰'].fillna(0))
    df_profit['æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'] = pd.to_numeric(df_profit['æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'], errors='coerce').round(2)
    
    df_profit['æ¯›åˆ©ç‡å·®ï¼ˆçº¯åŸä»·ï¼‰'] = (df_profit['æ¯›åˆ©ç‡_A'].fillna(0) - df_profit['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'].fillna(0))
    df_profit['æ¯›åˆ©ç‡å·®ï¼ˆçº¯åŸä»·ï¼‰'] = pd.to_numeric(df_profit['æ¯›åˆ©ç‡å·®ï¼ˆçº¯åŸä»·ï¼‰'], errors='coerce').round(2)
    
    # ç«å¯¹ä¿ƒé”€å½±å“
    df_profit['ç«å¯¹ä¿ƒé”€å½±å“'] = df_profit.apply(
        lambda row: (row['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰'] - row['é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰']) if pd.notna(row.get('é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰')) and pd.notna(row.get('é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰')) else None,
        axis=1
    )
    # ğŸ”§ ä¿®å¤ï¼šç¡®ä¿æ•°å€¼ç±»å‹å†round
    df_profit['ç«å¯¹ä¿ƒé”€å½±å“'] = pd.to_numeric(df_profit['ç«å¯¹ä¿ƒé”€å½±å“'], errors='coerce').round(2)
    
    # ç«äº‰ä¼˜åŠ¿åˆ†æï¼ˆåŸºäºå”®ä»·åŠ æƒç‰ˆï¼‰
    def analyze_advantage(row):
        if pd.isna(row.get('æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰')):
            return 'æ•°æ®ä¸è¶³'
        if row['æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'] > 10:
            return 'æœ¬åº—é«˜æ¯›åˆ©'
        elif row['æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'] < -10:
            return 'ç«å¯¹é«˜æ¯›åˆ©'
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨'æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'
        elif row.get('æˆæœ¬ä¼˜åŠ¿') == 'æœ¬åº—æˆæœ¬ä½' and row.get(price_a_col, 999999) <= row.get(price_b_col, 0):
            return 'æˆæœ¬+ä»·æ ¼åŒä¼˜åŠ¿'
        elif row.get('æˆæœ¬ä¼˜åŠ¿') == 'æœ¬åº—æˆæœ¬ä½':
            return 'æˆæœ¬ä¼˜åŠ¿'
        elif row.get(price_a_col, 999999) < row.get(price_b_col, 0):
            return 'ä»·æ ¼ä¼˜åŠ¿'
        else:
            return 'ç«äº‰å‡è¡¡'
    
    df_profit['ç«äº‰ä¼˜åŠ¿'] = df_profit.apply(analyze_advantage, axis=1)
    
    # å®šä¹‰åˆ—é¡ºåºï¼ˆABABå¯¹æ¯”æ¨¡å¼ï¼‰
    profit_cols = [
        name_a_col, name_b_col,
        # æœ¬åº—æ•°æ®
        price_a_col, 'æˆæœ¬_A', 'æ¯›åˆ©_A', 'æ¯›åˆ©ç‡_A',
        # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨ç»Ÿä¸€çš„åˆ—å
        # ç«å¯¹å”®ä»·åŠ æƒç‰ˆ
        price_b_col, 'é¢„æµ‹æˆæœ¬_Bï¼ˆå”®ä»·åŠ æƒï¼‰', 'é¢„æµ‹æ¯›åˆ©_Bï¼ˆå”®ä»·åŠ æƒï¼‰', 'é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰',
        # ç«å¯¹çº¯åŸä»·ç‰ˆ
        'é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰', 'é¢„æµ‹æ¯›åˆ©_Bï¼ˆçº¯åŸä»·ï¼‰', 'é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰',
        # å¯¹æ¯”åˆ†æ
        'æ¯›åˆ©ç‡å·®ï¼ˆå”®ä»·åŠ æƒï¼‰', 'æ¯›åˆ©ç‡å·®ï¼ˆçº¯åŸä»·ï¼‰', 'ç«å¯¹ä¿ƒé”€å½±å“', 'ç«äº‰ä¼˜åŠ¿', 'ç½®ä¿¡åº¦'
    ]
    
    # æ·»åŠ å¯é€‰åˆ—ï¼ˆæ™ºèƒ½æŸ¥æ‰¾ï¼‰
    cat1_col_a = find_column(df_profit, 'ç¾å›¢ä¸€çº§åˆ†ç±»', cfg.STORE_A_NAME if cfg else '')
    monthly_a = find_column(df_profit, 'æœˆå”®', cfg.STORE_A_NAME if cfg else '')
    monthly_b = find_column(df_profit, 'æœˆå”®', cfg.STORE_B_NAME if cfg else '')
    if cat1_col_a:
        profit_cols.append(cat1_col_a)
    if monthly_a:
        profit_cols.append(monthly_a)
    if monthly_b:
        profit_cols.append(monthly_b)
    
    profit_cols = [col for col in profit_cols if col in df_profit.columns]
    sheets['åˆ©æ¶¦ç©ºé—´å¯¹æ¯”'] = df_profit[profit_cols].copy()
    
    # Sheet 3: æˆæœ¬ä¼˜åŠ¿å•†å“ï¼ˆåŒè§†è§’å¯¹æ¯”ï¼‰
    # ğŸ”§ ä¿®å¤ï¼šä½¿ç”¨'æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'ä»£æ›¿'æˆæœ¬å·®'
    # åŸºäºå”®ä»·åŠ æƒç‰ˆç­›é€‰æˆæœ¬ä¼˜åŠ¿å•†å“
    if 'æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰' in df_profit.columns:
        df_advantage = df_profit[
            (df_profit['æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰'] < -1) &  # æœ¬åº—æˆæœ¬ä½
            (df_profit[price_a_col] <= df_profit[price_b_col] * 1.05) &  # ä»·æ ¼ç›¸è¿‘æˆ–æ›´ä½
            (df_profit['ç½®ä¿¡åº¦'] >= 0.6)  # ä¸­ç­‰ä»¥ä¸Šç½®ä¿¡åº¦
        ].copy()
    else:
        # å›é€€æ–¹æ¡ˆï¼ˆå¦‚æœæ–°åˆ—åä¸å­˜åœ¨ï¼‰
        df_advantage = pd.DataFrame()
    
    if not df_advantage.empty:
        df_advantage = df_advantage.sort_values('æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰', ascending=True)
        
        # === æˆæœ¬å¯¹æ¯”ï¼ˆåŒè§†è§’ï¼‰===
        # ï¼ˆå·²ç»åœ¨df_profitä¸­è®¡ç®—è¿‡ï¼‰
        
        # === çº¯åŸä»·ç‰ˆæˆæœ¬å·®ï¼ˆå¦‚æœä¸å­˜åœ¨åˆ™è®¡ç®—ï¼‰===
        if 'æˆæœ¬å·®ï¼ˆçº¯åŸä»·ï¼‰' not in df_advantage.columns and 'é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰' in df_advantage.columns:
            df_advantage['æˆæœ¬å·®ï¼ˆçº¯åŸä»·ï¼‰'] = df_advantage['æˆæœ¬_A'] - df_advantage['é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰']
        
        # è®¡ç®—æ½œåœ¨è°ƒä»·ç©ºé—´ï¼ˆåŸºäºå”®ä»·åŠ æƒç‰ˆï¼‰
        df_advantage['æ½œåœ¨æä»·ç©ºé—´'] = (df_advantage[price_b_col] - df_advantage[price_a_col]).round(2)
        
        # === ä¿ƒé”€å½±å“è¯„ä¼° ===
        def assess_competitor_promotion(row):
            promo_impact = row.get('ç«å¯¹ä¿ƒé”€å½±å“', 0)
            if pd.isna(promo_impact):
                return 'æ— ä¿ƒé”€æ•°æ®'
            if promo_impact >= 10:
                return 'ç«å¯¹æ·±åº¦ä¿ƒé”€'
            elif promo_impact >= 5:
                return 'ç«å¯¹æ¸©å’Œä¿ƒé”€'
            elif promo_impact >= 1:
                return 'ç«å¯¹è½»å¾®ä¿ƒé”€'
            else:
                return 'ç«å¯¹æ— ä¿ƒé”€'
        
        df_advantage['ç«å¯¹ä¿ƒé”€çŠ¶æ€'] = df_advantage.apply(assess_competitor_promotion, axis=1)
        
        # === æ™ºèƒ½å»ºè®®ï¼ˆè€ƒè™‘ä¿ƒé”€å½±å“ï¼‰===
        def generate_smart_suggestion(row):
            promo_state = row.get('ç«å¯¹ä¿ƒé”€çŠ¶æ€', 'æ— ä¿ƒé”€æ•°æ®')
            price_gap = row.get('æ½œåœ¨æä»·ç©ºé—´', 0)
            cost_diff_weighted = row.get('æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰', 0)
            cost_diff_orig = row.get('æˆæœ¬å·®ï¼ˆçº¯åŸä»·ï¼‰', 0)
            
            if pd.isna(price_gap) or price_gap <= 0:
                return "ä»·æ ¼å·²è¾¾ç«å¯¹æ°´å¹³ï¼Œæš‚ä¸å»ºè®®è°ƒä»·"
            
            if 'æ·±åº¦ä¿ƒé”€' in promo_state:
                return f"âš ï¸ ç«å¯¹æ·±åº¦ä¿ƒé”€ä¸­ï¼Œå»ºè®®è§‚æœ›ã€‚æ­£å¸¸æœŸå¯è€ƒè™‘æä»·è‡³{row[price_b_col]:.2f}å…ƒ"
            elif 'æ¸©å’Œä¿ƒé”€' in promo_state:
                return f"ç«å¯¹ä¿ƒé”€æœŸï¼Œå¯è€ƒè™‘å°å¹…æä»·è‡³{row[price_a_col] + price_gap/2:.2f}å…ƒï¼Œä¿ƒé”€ç»“æŸåæè‡³{row[price_b_col]:.2f}å…ƒ"
            else:
                return f"å¯è€ƒè™‘æä»·è‡³{row[price_b_col]:.2f}å…ƒï¼Œå¢åŠ æ¯›åˆ©{price_gap:.2f}å…ƒ"
        
        df_advantage['æ™ºèƒ½å»ºè®®'] = df_advantage.apply(generate_smart_suggestion, axis=1)
        
        # å®šä¹‰åˆ—é¡ºåº
        advantage_cols = [
            name_a_col, price_a_col, 'æˆæœ¬_A', 'æ¯›åˆ©ç‡_A',
            price_b_col, 
            # å”®ä»·åŠ æƒç‰ˆ
            'é¢„æµ‹æˆæœ¬_B', 'é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆå”®ä»·åŠ æƒï¼‰', 'æˆæœ¬å·®ï¼ˆå”®ä»·åŠ æƒï¼‰',
            # çº¯åŸä»·ç‰ˆ
            'é¢„æµ‹æˆæœ¬_Bï¼ˆçº¯åŸä»·ï¼‰', 'é¢„æµ‹æ¯›åˆ©ç‡_Bï¼ˆçº¯åŸä»·ï¼‰', 'æˆæœ¬å·®ï¼ˆçº¯åŸä»·ï¼‰',
            # å†³ç­–æ”¯æŒ
            'ç«å¯¹ä¿ƒé”€çŠ¶æ€', 'ç«å¯¹ä¿ƒé”€å½±å“', 'æ½œåœ¨æä»·ç©ºé—´', 'æ™ºèƒ½å»ºè®®', 'ç½®ä¿¡åº¦'
        ]
        advantage_cols = [col for col in advantage_cols if col in df_advantage.columns]
        sheets['æˆæœ¬ä¼˜åŠ¿å•†å“'] = df_advantage[advantage_cols].copy()
        
        print(f"   âœ… è¯†åˆ«å‡º {len(df_advantage)} ä¸ªæˆæœ¬ä¼˜åŠ¿å•†å“")
    
    # ğŸ†• Sheet 4: ç«å¯¹å…¨å•†å“æˆæœ¬å€’æ¨ï¼ˆåŒè§†è§’å¯¹æ¯”ï¼‰
    # ğŸ”§ ä¿®å¤ï¼šé¿å…é‡å¤å•†å“ï¼ŒåŸºäºå•†å“åç§°+è§„æ ¼å»é‡
    if store_b_all_df is not None:
        # 1. ä»matched_dfä¸­æå–Båº—æ•°æ®ï¼ˆåŒ…å«æˆæœ¬é¢„æµ‹ï¼‰
        matched_b_data = None
        matched_products_set = set()  # ç”¨äºå»é‡ï¼š(å•†å“åç§°, è§„æ ¼)
        
        if matched_df is not None and not matched_df.empty:
            # matched_dfåŒ…å« _A å’Œ _B åç¼€çš„åˆ—ï¼Œä»¥åŠæˆæœ¬é¢„æµ‹åˆ—
            # æå–æ‰€æœ‰Båº—ç›¸å…³åˆ—ï¼ˆ_Båç¼€ï¼‰+ æˆæœ¬é¢„æµ‹åˆ—
            b_cols = []
            rename_map = {}
            
            for col in matched_df.columns:
                if col.endswith('_B'):
                    # Båº—åŸºç¡€åˆ—ï¼šå»æ‰_Båç¼€
                    b_cols.append(col)
                    rename_map[col] = col[:-2]
                elif col in ['é¢„æµ‹æˆæœ¬_B', 'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†', 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†', 'é¢„æµ‹æ–¹æ³•', 'ç½®ä¿¡åº¦']:
                    # æˆæœ¬é¢„æµ‹åˆ—ä¿ç•™
                    b_cols.append(col)
                    rename_map[col] = col
            
            if b_cols:
                matched_b_data = matched_df[b_cols].copy()
                matched_b_data = matched_b_data.rename(columns=rename_map)
                
                # é‡å‘½åæˆæœ¬åˆ—ï¼šé¢„æµ‹æˆæœ¬_B â†’ é¢„æµ‹æˆæœ¬
                if 'é¢„æµ‹æˆæœ¬_B' in matched_b_data.columns:
                    matched_b_data = matched_b_data.rename(columns={'é¢„æµ‹æˆæœ¬_B': 'é¢„æµ‹æˆæœ¬'})
                
                # ğŸ†• è®°å½•å·²åŒ¹é…å•†å“ï¼ˆç”¨äºå»é‡ï¼‰
                for idx, row in matched_b_data.iterrows():
                    product_name = row.get('å•†å“åç§°', '')
                    spec = row.get('è§„æ ¼', '')  # è§„æ ¼å­—æ®µ
                    if product_name:
                        matched_products_set.add((product_name, spec if pd.notna(spec) else ''))
                
                print(f"   ğŸ“Š åŒ¹é…å•†å“æˆæœ¬æ•°æ®: {len(matched_b_data)} ä¸ªï¼ˆæå–{len(b_cols)}ä¸ªBåº—åˆ—ï¼‰")
            else:
                print(f"   âš ï¸  matched_dfä¸­æ— Båº—åˆ—ï¼Œè·³è¿‡åŒ¹é…å•†å“")
        
        # 2. ç‹¬æœ‰å•†å“çš„æˆæœ¬æ•°æ®ï¼ˆå·²åœ¨predict_all_competitor_products_costä¸­å€’æ¨ï¼‰
        # ğŸ”§ ä¿®å¤ï¼šä»store_b_all_dfä¸­æ’é™¤å·²åŒ¹é…çš„å•†å“
        df_unmatched_with_cost_list = []
        for idx, row in store_b_all_df.iterrows():
            if pd.isna(row.get('é¢„æµ‹æˆæœ¬')):
                continue  # è·³è¿‡æ²¡æœ‰é¢„æµ‹æˆæœ¬çš„å•†å“
            
            product_name = row.get('å•†å“åç§°', '')
            spec = row.get('è§„æ ¼', '')
            product_key = (product_name, spec if pd.notna(spec) else '')
            
            # ğŸ†• åªæ·»åŠ æœªåŒ¹é…çš„å•†å“ï¼ˆé¿å…é‡å¤ï¼‰
            if product_key not in matched_products_set:
                df_unmatched_with_cost_list.append(row)
        
        df_unmatched_with_cost = pd.DataFrame(df_unmatched_with_cost_list) if df_unmatched_with_cost_list else pd.DataFrame()
        print(f"   ğŸ“Š ç‹¬æœ‰å•†å“æˆæœ¬æ•°æ®: {len(df_unmatched_with_cost)} ä¸ªï¼ˆå·²æ’é™¤{len(matched_products_set)}ä¸ªå·²åŒ¹é…å•†å“ï¼‰")
        
        # 3. åˆå¹¶ä¸¤éƒ¨åˆ†æ•°æ®
        if matched_b_data is not None and not matched_b_data.empty:
            # ç¡®ä¿åˆ—åä¸€è‡´ååˆå¹¶
            df_all_with_cost = pd.concat([matched_b_data, df_unmatched_with_cost], ignore_index=True)
            print(f"   âœ… åˆå¹¶åæ€»å•†å“æ•°: {len(df_all_with_cost)} ä¸ªï¼ˆåŒ¹é…{len(matched_b_data)} + ç‹¬æœ‰{len(df_unmatched_with_cost)}ï¼‰")
        else:
            df_all_with_cost = df_unmatched_with_cost
            print(f"   âš ï¸  ä»…ç‹¬æœ‰å•†å“: {len(df_all_with_cost)} ä¸ª")
        
        if not df_all_with_cost.empty:
            # === å”®ä»·åŠ æƒç‰ˆè®¡ç®— ===
            df_all_with_cost['é¢„æµ‹æ¯›åˆ©ï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_all_with_cost['åŸä»·'] - df_all_with_cost['é¢„æµ‹æˆæœ¬']
            df_all_with_cost['é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆå”®ä»·åŠ æƒï¼‰'] = df_all_with_cost.apply(
                lambda row: (row['é¢„æµ‹æ¯›åˆ©ï¼ˆå”®ä»·åŠ æƒï¼‰'] / row['åŸä»·'] * 100) if pd.notna(row['åŸä»·']) and row['åŸä»·'] > 0 else None,
                axis=1
            ).round(2)
            
            # === çº¯åŸä»·ç‰ˆè®¡ç®— ===
            df_all_with_cost['é¢„æµ‹æˆæœ¬ï¼ˆçº¯åŸä»·ï¼‰'] = df_all_with_cost['é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†']
            df_all_with_cost['é¢„æµ‹æ¯›åˆ©ï¼ˆçº¯åŸä»·ï¼‰'] = df_all_with_cost['åŸä»·'] - df_all_with_cost['é¢„æµ‹æˆæœ¬ï¼ˆçº¯åŸä»·ï¼‰']
            df_all_with_cost['é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆçº¯åŸä»·ï¼‰'] = df_all_with_cost.apply(
                lambda row: (row['é¢„æµ‹æ¯›åˆ©ï¼ˆçº¯åŸä»·ï¼‰'] / row['åŸä»·'] * 100) if pd.notna(row['åŸä»·']) and row['åŸä»·'] > 0 else None,
                axis=1
            ).round(2)
            
            # === ä¿ƒé”€å½±å“åˆ†æ ===
            # è®¡ç®—æŠ˜æ‰£ç‡ï¼ˆå¦‚æœæœ‰å”®ä»·ï¼‰
            if 'å”®ä»·' in df_all_with_cost.columns:
                df_all_with_cost['æŠ˜æ‰£ç‡'] = df_all_with_cost.apply(
                    lambda row: (row['å”®ä»·'] / row['åŸä»·'] * 100) if pd.notna(row['åŸä»·']) and row['åŸä»·'] > 0 else None,
                    axis=1
                ).round(2)
            
            # è®¡ç®—ä¿ƒé”€å¯¹æ¯›åˆ©ç‡çš„å½±å“
            df_all_with_cost['ä¿ƒé”€å½±å“'] = df_all_with_cost.apply(
                lambda row: (row['é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆçº¯åŸä»·ï¼‰'] - row['é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆå”®ä»·åŠ æƒï¼‰']) if pd.notna(row['é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆçº¯åŸä»·ï¼‰']) and pd.notna(row['é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆå”®ä»·åŠ æƒï¼‰']) else None,
                axis=1
            ).round(2)
            
            # ä¿ƒé”€å¼ºåº¦æ ‡è®°
            def mark_promotion_intensity(row):
                if pd.isna(row['ä¿ƒé”€å½±å“']):
                    return None
                if row['ä¿ƒé”€å½±å“'] >= 10:
                    return 'æ·±åº¦ä¿ƒé”€'
                elif row['ä¿ƒé”€å½±å“'] >= 5:
                    return 'æ¸©å’Œä¿ƒé”€'
                elif row['ä¿ƒé”€å½±å“'] >= 1:
                    return 'è½»å¾®ä¿ƒé”€'
                else:
                    return 'æ— ä¿ƒé”€'
            
            df_all_with_cost['ä¿ƒé”€å¼ºåº¦'] = df_all_with_cost.apply(mark_promotion_intensity, axis=1)
            
            # å®šä¹‰è¦å±•ç¤ºçš„åˆ—ï¼ˆABABäº¤æ›¿å¯¹æ¯”æ¨¡å¼ï¼‰
            all_product_cols = [
                # å•†å“åŸºç¡€ä¿¡æ¯
                'å•†å“åç§°', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ç¾å›¢äºŒçº§åˆ†ç±»', 'ç¾å›¢ä¸‰çº§åˆ†ç±»',
                'æ¡ç ', 'åŸä»·', 'å”®ä»·', 'åº“å­˜', 'æœˆå”®', 'æŠ˜æ‰£ç‡',
                
                # å”®ä»·åŠ æƒç‰ˆé¢„æµ‹
                'é¢„æµ‹æˆæœ¬', 'é¢„æµ‹æ¯›åˆ©ï¼ˆå”®ä»·åŠ æƒï¼‰', 'é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆå”®ä»·åŠ æƒï¼‰',
                
                # çº¯åŸä»·ç‰ˆé¢„æµ‹
                'é¢„æµ‹æˆæœ¬ï¼ˆçº¯åŸä»·ï¼‰', 'é¢„æµ‹æ¯›åˆ©ï¼ˆçº¯åŸä»·ï¼‰', 'é¢„æµ‹æ¯›åˆ©ç‡ï¼ˆçº¯åŸä»·ï¼‰',
                
                # å¯¹æ¯”åˆ†æ
                'ä¿ƒé”€å½±å“', 'ä¿ƒé”€å¼ºåº¦',
                
                # é¢„æµ‹å…ƒä¿¡æ¯
                'é¢„æµ‹æ–¹æ³•', 'ç½®ä¿¡åº¦',
                
                # å¯é€‰ï¼šåŸºå‡†æ•°æ®ï¼ˆä¾›éªŒè¯ï¼‰
                'é¢„æµ‹æˆæœ¬_åŸä»·åŸºå‡†', 'é¢„æµ‹æˆæœ¬_å”®ä»·åŸºå‡†'
            ]
            
            # æ·»åŠ å…¶ä»–å¯èƒ½å­˜åœ¨çš„å­—æ®µ
            optional_cols = ['åº—å†…ç ', 'å“ç‰Œ', 'è§„æ ¼', 'å•ä½', 'å•†å“ä»‹ç»', 'åº—å†…åˆ†ç±»']
            for col in optional_cols:
                if col in df_all_with_cost.columns and col not in all_product_cols:
                    all_product_cols.append(col)
            
            # è¿‡æ»¤å­˜åœ¨çš„åˆ—
            all_product_cols = [col for col in all_product_cols if col in df_all_with_cost.columns]
            
            # æŒ‰ä¿ƒé”€å¼ºåº¦å’Œç½®ä¿¡åº¦æ’åºï¼ˆæ·±åº¦ä¿ƒé”€ä¼˜å…ˆï¼Œä¾¿äºè¯†åˆ«å…³é”®å•†å“ï¼‰
            sort_keys = []
            sort_ascending = []
            
            if 'ä¿ƒé”€å¼ºåº¦' in df_all_with_cost.columns:
                # å®šä¹‰ä¿ƒé”€å¼ºåº¦æ’åºæƒé‡
                promotion_order = {'æ·±åº¦ä¿ƒé”€': 1, 'æ¸©å’Œä¿ƒé”€': 2, 'è½»å¾®ä¿ƒé”€': 3, 'æ— ä¿ƒé”€': 4}
                df_all_with_cost['_ä¿ƒé”€æ’åº'] = df_all_with_cost['ä¿ƒé”€å¼ºåº¦'].map(promotion_order).fillna(5)
                sort_keys.append('_ä¿ƒé”€æ’åº')
                sort_ascending.append(True)
            
            sort_keys.append('ç½®ä¿¡åº¦')
            sort_ascending.append(False)
            
            df_all_with_cost = df_all_with_cost.sort_values(
                by=sort_keys,
                ascending=sort_ascending
            )
            
            # åˆ é™¤ä¸´æ—¶æ’åºåˆ—
            if '_ä¿ƒé”€æ’åº' in df_all_with_cost.columns:
                df_all_with_cost = df_all_with_cost.drop(columns=['_ä¿ƒé”€æ’åº'])
            
            sheets['ç«å¯¹å…¨å•†å“æˆæœ¬å€’æ¨'] = df_all_with_cost[all_product_cols].copy()
            
            # ç»Ÿè®¡ä¿ƒé”€åˆ†å¸ƒ
            promotion_stats = df_all_with_cost['ä¿ƒé”€å¼ºåº¦'].value_counts().to_dict() if 'ä¿ƒé”€å¼ºåº¦' in df_all_with_cost.columns else {}
            promotion_summary = ', '.join([f"{k}:{v}ä¸ª" for k, v in promotion_stats.items()]) if promotion_stats else "æ— "
            
            print(f"   âœ… ç«å¯¹å…¨å•†å“æˆæœ¬å€’æ¨: {len(df_all_with_cost)} ä¸ªå•†å“ï¼ˆè¦†ç›–ç‡ {len(df_all_with_cost)/len(store_b_all_df)*100:.1f}%ï¼‰")
            print(f"      ä¿ƒé”€åˆ†å¸ƒ: {promotion_summary}")
    
    print(f"   âœ… ç”Ÿæˆ {len(sheets)} ä¸ªæˆæœ¬åˆ†æ Sheet")
    return sheets


def generate_final_reports(df_all_a, df_all_b, barcode_matches, fuzzy_matches, name_a, name_b, cfg=None):
    """
    ç”Ÿæˆæ‰€æœ‰æŠ¥å‘Šæ•°æ®
    
    æ–°å¢è¿”å›ï¼š
    - df_a_unique_dedup: å»é‡åçš„æœ¬åº—ç‹¬æœ‰å•†å“
    - df_b_unique_dedup: å»é‡åçš„ç«å¯¹ç‹¬æœ‰å•†å“
    - df_differential: å·®å¼‚å“å¯¹æ¯”
    - df_category_gaps: å“ç±»ç¼ºå£åˆ†æ
    """
    name_a_col, name_b_col = f'å•†å“åç§°_{name_a}', f'å•†å“åç§°_{name_b}'
    
    matched_names_a = set()
    if not barcode_matches.empty and name_a_col in barcode_matches.columns:
        matched_names_a.update(barcode_matches[name_a_col].dropna().tolist())
    if not fuzzy_matches.empty and name_a_col in fuzzy_matches.columns:
        matched_names_a.update(fuzzy_matches[name_a_col].dropna().tolist())

    matched_names_b = set()
    if not barcode_matches.empty and name_b_col in barcode_matches.columns:
        matched_names_b.update(barcode_matches[name_b_col].dropna().tolist())
    if not fuzzy_matches.empty and name_b_col in fuzzy_matches.columns:
        matched_names_b.update(fuzzy_matches[name_b_col].dropna().tolist())

    df_a_unique = df_all_a[~df_all_a['å•†å“åç§°'].isin(matched_names_a)].copy()
    if not df_a_unique.empty and 'åº—å†…ç ' in df_a_unique.columns:
        df_a_unique.rename(columns={'åº—å†…ç ': f'åº—å†…ç _{name_a}'}, inplace=True)
    
    # è°ƒè¯•ï¼šæ£€æŸ¥vectoråˆ—
    print(f"   ğŸ› è°ƒè¯•: df_a_uniqueåˆ—å={df_a_unique.columns.tolist()[:10]}... (å…±{len(df_a_unique.columns)}åˆ—)")
    print(f"   ğŸ› è°ƒè¯•: 'vector' in df_a_unique.columns = {('vector' in df_a_unique.columns)}")

    df_b_unique = df_all_b[~df_all_b['å•†å“åç§°'].isin(matched_names_b)].copy()
    if not df_b_unique.empty and 'åº—å†…ç ' in df_b_unique.columns:
        df_b_unique.rename(columns={'åº—å†…ç ': f'åº—å†…ç _{name_b}'}, inplace=True)
    
    # è°ƒè¯•ï¼šæ£€æŸ¥vectoråˆ—
    print(f"   ğŸ› è°ƒè¯•: df_b_uniqueåˆ—å={df_b_unique.columns.tolist()[:10]}... (å…±{len(df_b_unique.columns)}åˆ—)")
    print(f"   ğŸ› è°ƒè¯•: 'vector' in df_b_unique.columns = {('vector' in df_b_unique.columns)}")

    all_matches = pd.concat([barcode_matches, fuzzy_matches], ignore_index=True)
    sales_comparison_df = pd.DataFrame()
    discount_filter_df = pd.DataFrame()  # æ–°å¢ï¼šåº“å­˜ä¸æŠ˜æ‰£è”åˆç­›é€‰ç»“æœ
    if not all_matches.empty:
        df = all_matches.copy()
        price_a, price_b = f'å”®ä»·_{name_a}', f'å”®ä»·_{name_b}'
        orig_a, orig_b = f'åŸä»·_{name_a}', f'åŸä»·_{name_b}'
        sales_b = f'æœˆå”®_{name_b}'
        inventory_a, inventory_b = f'åº“å­˜_{name_a}', f'åº“å­˜_{name_b}'

        for col in [price_a, price_b, orig_a, orig_b, sales_b, inventory_a, inventory_b]:
            if col in df.columns:
                df[col] = pd.to_numeric(df[col], errors='coerce')
        
        if orig_a in df.columns and orig_b in df.columns:
            df['æŠ˜æ‰£A'] = df[price_a] / df[orig_a]
            df['æŠ˜æ‰£B'] = df[price_b] / df[orig_b]
            
            sales_comparison_df = df[
                (df[sales_b] > 0) &
                (df[inventory_a] > 0) &
                (df[inventory_b] > 0) &
                (df['æŠ˜æ‰£A'] <= df['æŠ˜æ‰£B'])
            ].sort_values(by=sales_b, ascending=False)

            # æ–°å¢ï¼šç”Ÿæˆâ€œåº“å­˜éƒ½>0ã€Bæœˆå”®>0ã€ä¸”AæŠ˜æ‰£>=BæŠ˜æ‰£ï¼ˆå‡ä¸ä¸ºç©ºï¼‰â€çš„æ•°æ®é›†
            try:
                discount_filter_df = df[
                    (df[inventory_a] > 0) &
                    (df[inventory_b] > 0) &
                    (df[sales_b] > 0) &
                    df['æŠ˜æ‰£A'].notna() & df['æŠ˜æ‰£B'].notna() &
                    (df['æŠ˜æ‰£A'] >= df['æŠ˜æ‰£B'])
                ].sort_values(by=sales_b, ascending=False)
            except Exception:
                discount_filter_df = pd.DataFrame()
    
    # === æ–°å¢åŠŸèƒ½ 2: å·®å¼‚å“åˆ†æï¼ˆåœ¨å»é‡å‰è¿›è¡Œï¼Œéœ€è¦vectoråˆ—ï¼‰===
    df_differential = find_differential_products(df_a_unique, df_b_unique, name_a, name_b, cfg)
    
    # === æ–°å¢åŠŸèƒ½ 1: ç‹¬æœ‰å•†å“å»é‡ ===
    print(f"\nğŸ“¦ ç‹¬æœ‰å•†å“å»é‡å¤„ç†...")
    df_a_unique_dedup = deduplicate_unique_products(df_a_unique, name_a)
    df_b_unique_dedup = deduplicate_unique_products(df_b_unique, name_b)
    
    # === æ–°å¢åŠŸèƒ½ 3: å“ç±»ç¼ºå£åˆ†æï¼ˆä½¿ç”¨å»é‡åçš„æ•°æ®ï¼Œæ›´æ¸…æ™°ï¼‰===
    df_category_gaps = analyze_category_gaps(df_a_unique_dedup, df_b_unique_dedup, name_a, name_b)
    
    # === ğŸ†• ç¬¬ä¸€é˜¶æ®µåŠŸèƒ½: ç«å¯¹æˆæœ¬é¢„æµ‹ ===
    cost_sheets = {}
    
    # è°ƒè¯•ï¼šæ£€æŸ¥æˆæœ¬åˆ—æ˜¯å¦å­˜åœ¨
    print(f"\nğŸ” æˆæœ¬é¢„æµ‹æ£€æŸ¥:")
    print(f"   ENABLE_COST_PREDICTION = {cfg.ENABLE_COST_PREDICTION if cfg else 'None'}")
    print(f"   COST_COLUMN_NAME = {cfg.COST_COLUMN_NAME if cfg else 'None'}")
    print(f"   df_all_a åˆ—ååŒ…å«: {[col for col in df_all_a.columns if 'æˆæœ¬' in col or 'cost' in col.lower()]}")
    print(f"   'æˆæœ¬' in df_all_a.columns = {'æˆæœ¬' in df_all_a.columns}")
    
    if cfg and cfg.ENABLE_COST_PREDICTION and cfg.COST_COLUMN_NAME in df_all_a.columns:
        print(f"   âœ… æˆæœ¬é¢„æµ‹åŠŸèƒ½å·²å¯ç”¨")
        
        # ğŸ”§ ä¿®å¤ï¼šå¯¹æ¡ç åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…éƒ½è¿›è¡Œæˆæœ¬é¢„æµ‹
        if not barcode_matches.empty:
            print(f"   ğŸ“Š å¯¹æ¡ç ç²¾ç¡®åŒ¹é…å•†å“è¿›è¡Œæˆæœ¬é¢„æµ‹...")
            barcode_matches = predict_competitor_cost(barcode_matches, df_all_a, cfg)
        
        if not fuzzy_matches.empty:
            print(f"   ğŸ“Š å¯¹åç§°æ¨¡ç³ŠåŒ¹é…å•†å“è¿›è¡Œæˆæœ¬é¢„æµ‹...")
            fuzzy_matches = predict_competitor_cost(fuzzy_matches, df_all_a, cfg)
        
        # ğŸ”§ ä¿®å¤ï¼šåˆå¹¶æ¡ç åŒ¹é…å’Œæ¨¡ç³ŠåŒ¹é…çš„æˆæœ¬é¢„æµ‹æ•°æ®
        matched_with_cost = []
        if not barcode_matches.empty:
            matched_with_cost.append(barcode_matches)
        if not fuzzy_matches.empty:
            matched_with_cost.append(fuzzy_matches)
        
        all_matched_df = pd.concat(matched_with_cost, ignore_index=True) if matched_with_cost else pd.DataFrame()
        
        # ğŸ”§ ä¿®å¤ï¼šå»é‡ï¼ˆåŸºäºå•†å“åç§°_B+è§„æ ¼_Bï¼Œé¿å…åŒä¸€å•†å“æ—¢åœ¨æ¡ç åŒ¹é…åˆåœ¨æ¨¡ç³ŠåŒ¹é…ä¸­ï¼‰
        if not all_matched_df.empty:
            name_col_b = f'å•†å“åç§°_{cfg.STORE_B_NAME}' if f'å•†å“åç§°_{cfg.STORE_B_NAME}' in all_matched_df.columns else 'å•†å“åç§°_B'
            spec_col_b = f'è§„æ ¼_{cfg.STORE_B_NAME}' if f'è§„æ ¼_{cfg.STORE_B_NAME}' in all_matched_df.columns else 'è§„æ ¼_B'
            
            dedup_cols = [name_col_b]
            if spec_col_b in all_matched_df.columns:
                dedup_cols.append(spec_col_b)
            
            original_count = len(all_matched_df)
            all_matched_df = all_matched_df.drop_duplicates(subset=dedup_cols, keep='first')
            dedup_count = original_count - len(all_matched_df)
            
            if dedup_count > 0:
                print(f"   ğŸ”§ å»é‡ï¼šç§»é™¤{dedup_count}ä¸ªé‡å¤åŒ¹é…å•†å“ï¼ˆåˆå¹¶åæ€»è®¡{len(all_matched_df)}ä¸ªï¼‰")
        
        # ğŸ†• å¯¹ç«å¯¹æ‰€æœ‰å•†å“è¿›è¡Œæˆæœ¬å€’æ¨ï¼ˆåŒ…æ‹¬ç‹¬æœ‰å•†å“ï¼‰
        # ğŸ”§ ä¿®å¤ï¼šåªå€’æ¨æœªåŒ¹é…çš„å•†å“ï¼Œé¿å…é‡å¤è®¡ç®—
        if not all_matched_df.empty:
            # è·å–å·²åŒ¹é…å•†å“çš„æ¡ç åˆ—è¡¨ï¼ˆç”¨äºå‰”é™¤ï¼‰
            matched_barcodes = set()
            barcode_col_b = f'æ¡ç _B'
            if barcode_col_b in all_matched_df.columns:
                matched_barcodes = set(all_matched_df[barcode_col_b].dropna().astype(str))
            
            # å‰”é™¤å·²åŒ¹é…çš„å•†å“ï¼ˆåŸºäºæ¡ç ï¼‰
            if matched_barcodes and 'æ¡ç ' in df_all_b.columns:
                df_b_unmatched = df_all_b[~df_all_b['æ¡ç '].astype(str).isin(matched_barcodes)].copy()
                print(f"   ğŸ“Š ç«å¯¹å•†å“åˆ†ç±»: æ€»{len(df_all_b)}ä¸ª, å·²åŒ¹é…{len(matched_barcodes)}ä¸ª, å¾…å€’æ¨{len(df_b_unmatched)}ä¸ª")
            else:
                df_b_unmatched = df_all_b.copy()
                print(f"   âš ï¸  æ— æ³•åŸºäºæ¡ç å‰”é™¤ï¼Œå°†å¯¹å…¨éƒ¨{len(df_all_b)}ä¸ªå•†å“å€’æ¨")
            
            df_b_with_å…¨å•†å“æˆæœ¬ = predict_all_competitor_products_cost(df_b_unmatched, df_all_a, cfg)
        else:
            # å¦‚æœæ²¡æœ‰åŒ¹é…å•†å“ï¼Œåˆ™å¯¹å…¨éƒ¨ç«å¯¹å•†å“å€’æ¨
            df_b_with_å…¨å•†å“æˆæœ¬ = predict_all_competitor_products_cost(df_all_b, df_all_a, cfg)
        
        # ç”Ÿæˆæˆæœ¬åˆ†æSheetï¼ˆä¼ å…¥åŒ¹é…å•†å“å’Œå…¨å•†å“æ•°æ®ï¼‰
        if not all_matched_df.empty:
            cost_sheets = generate_cost_analysis_sheets(all_matched_df, df_b_with_å…¨å•†å“æˆæœ¬, cfg)
        else:
            cost_sheets = {}
    else:
        print(f"   âš ï¸  æˆæœ¬é¢„æµ‹åŠŸèƒ½æœªå¯ç”¨ï¼ŒåŸå› :")
        if not cfg:
            print(f"      - cfg ä¸º None")
        elif not cfg.ENABLE_COST_PREDICTION:
            print(f"      - ENABLE_COST_PREDICTION = False")
        elif cfg.COST_COLUMN_NAME not in df_all_a.columns:
            print(f"      - åˆ— '{cfg.COST_COLUMN_NAME}' ä¸å­˜åœ¨äº df_all_a ä¸­")
    
    return (df_a_unique, df_b_unique, sales_comparison_df, discount_filter_df,
            df_a_unique_dedup, df_b_unique_dedup, df_differential, df_category_gaps, cost_sheets)

def export_to_excel(writer, df, sheet_name, cfg=None):
    if df is not None and not df.empty:
        # å»æ‰å‘é‡åˆ—
        cols_to_drop = [col for col in df.columns if 'vector' in str(col)]
        
        # ğŸ†• æ­¥éª¤1: åˆ é™¤æ‰€æœ‰ä¸´æ—¶è¾…åŠ©åˆ—ï¼ˆé˜²æ­¢æ³„éœ²åˆ°Excelï¼‰
        auxiliary_cols = [
            col for col in df.columns 
            if any(prefix in str(col) for prefix in [
                'cat3_group_', 'cat1_group_', 'category_id', 
                'cat3_group', 'cat1_group',  # æ— åç¼€ç‰ˆæœ¬
                'index_'  # ç´¢å¼•åˆ—ï¼ˆå¦‚index_A, index_Bï¼‰
            ])
        ]
        cols_to_drop.extend(auxiliary_cols)
        if auxiliary_cols:
            print(f"   ğŸ§¹ æ¸…ç†ä¸´æ—¶è¾…åŠ©åˆ—: {auxiliary_cols}")
        
        
        # å…¨å±€ï¼šéæ¸…æ´—ç±»Sheetä¸€å¾‹ç§»é™¤æ¸…æ´—å‰ç¼€åˆ—ï¼ˆcleaned_/standardized_brand/specsï¼‰ï¼Œé¿å…å¹²æ‰°é˜…è¯»
        is_cleaning_sheet = ('æ¸…æ´—æ•°æ®' in sheet_name) or ('åˆå¹¶æ¸…æ´—æ•°æ®å¯¹æ¯”' in sheet_name)
        if not is_cleaning_sheet:
            prefixed_cols = [col for col in df.columns if str(col).startswith('cleaned_') or str(col).startswith('standardized_brand') or str(col).startswith('specs')]
            cols_to_drop.extend(prefixed_cols)
            # ç»Ÿä¸€éšè—æ ‡å‡†åŒ–åçš„åˆ†ç±»åˆ—ï¼ˆæœ‰åº—é“ºåç¼€çš„å½¢å¼ï¼‰ï¼Œä»…ä¿ç•™â€œç¾å›¢ä¸€çº§/ä¸‰çº§åˆ†ç±»_*â€
            std_category_cols = [col for col in df.columns if str(col).startswith('ä¸€çº§åˆ†ç±»_') or str(col).startswith('ä¸‰çº§åˆ†ç±»_') or str(col).startswith('å•†å®¶åˆ†ç±»_')]
            cols_to_drop.extend(std_category_cols)

        # å¯¹ä¸¤å¼ ä¸»è¦åŒ¹é…ç»“æœSheetï¼Œé¢å¤–ç§»é™¤å¤„ç†åˆ—ä¸æ ‡å‡†åŒ–åˆ†ç±»ï¼Œä¿æŒæœ€ç®€å±•ç¤º
        if any(keyword in sheet_name for keyword in ['æ¡ç ç²¾ç¡®åŒ¹é…', 'åç§°æ¨¡ç³ŠåŒ¹é…']):
            extra_cols = [col for col in df.columns if ('å•†å®¶åˆ†ç±»' in str(col)) or (str(col) in ['ä¸€çº§åˆ†ç±»', 'ä¸‰çº§åˆ†ç±»'])]
            cols_to_drop.extend(extra_cols)
            print(f"ğŸ“‹ {sheet_name}: ä¿ç•™ç¾å›¢åŸå§‹åˆ†ç±»ï¼Œå·²éšè—æ¸…æ´—/å¤„ç†åˆ—")
        
        df = df.drop(columns=cols_to_drop, errors='ignore')
        
        # ğŸ†• æ­¥éª¤2: ç»Ÿä¸€åˆ—ååç¼€ï¼ˆåº—é“ºå â†’ _A/_Bï¼Œç¡®ä¿ABABæ’åˆ—ç”Ÿæ•ˆï¼‰
        # è·å–åº—é“ºåç§°ï¼šä¼˜å…ˆä½¿ç”¨ä¼ å…¥çš„cfgï¼Œå¦åˆ™ä½¿ç”¨Configç±»é»˜è®¤å€¼
        if cfg:
            store_a = cfg.STORE_A_NAME
            store_b = cfg.STORE_B_NAME
        else:
            store_a = Config.STORE_A_NAME
            store_b = Config.STORE_B_NAME
        
        rename_map = {}
        for col in df.columns:
            # åªè½¬æ¢åº—é“ºååç¼€ï¼Œå·²ç»æ˜¯_A/_Bçš„ä¸é‡å¤è½¬æ¢
            if col.endswith(f'_{store_a}') and not col.endswith('_A'):
                new_col = col.replace(f'_{store_a}', '_A')
                rename_map[col] = new_col
            elif col.endswith(f'_{store_b}') and not col.endswith('_B'):
                new_col = col.replace(f'_{store_b}', '_B')
                rename_map[col] = new_col
        
        if rename_map:
            df = df.rename(columns=rename_map)
            print(f"   ğŸ”„ ç»Ÿä¸€åˆ—ååç¼€: {len(rename_map)} åˆ— (åº—é“ºå â†’ _A/_B)")
            if len(rename_map) <= 5:
                print(f"      ç¤ºä¾‹: {list(rename_map.items())}")
            else:
                print(f"      ç¤ºä¾‹: {list(rename_map.items())[:3]} ...")

        # ğŸ” è¯Šæ–­è°ƒè¯•è¾“å‡º
        print(f"\nğŸ” [{sheet_name}] åˆ—æ’åºè¯Šæ–­:")
        print(f"   åº—é“ºA: {store_a}")
        print(f"   åº—é“ºB: {store_b}")
        print(f"   æ€»åˆ—æ•°: {len(df.columns)}")
        
        # æ™ºèƒ½è¯†åˆ« A/B åˆ—ï¼ˆæ”¯æŒåº—é“ºååç¼€å’Œ _A/_B åç¼€ï¼‰
        a_cols = []
        b_cols = []
        for col in df.columns:
            if col.endswith(f'_{store_a}') or col.endswith('_A'):
                a_cols.append(col)
            elif col.endswith(f'_{store_b}') or col.endswith('_B'):
                b_cols.append(col)
        common_cols = [col for col in df.columns if col not in a_cols + b_cols]
        
        print(f"   Aåº—åˆ—æ•°: {len(a_cols)}")
        print(f"   Båº—åˆ—æ•°: {len(b_cols)}")
        print(f"   å…¬å…±åˆ—æ•°: {len(common_cols)}")
        if a_cols:
            print(f"   Aåº—åˆ—ç¤ºä¾‹: {a_cols[0]}")
        if b_cols:
            print(f"   Båº—åˆ—ç¤ºä¾‹: {b_cols[0]}")

        # å®šä¹‰éœ€è¦ABABæ’åˆ—çš„Sheetï¼ˆå¯¹æ¯”ç±»è¡¨æ ¼ï¼‰
        # ğŸ†• ç»Ÿä¸€å¸ƒå±€ï¼šæ¡ç åŒ¹é…ã€åç§°åŒ¹é…ã€å·®å¼‚å“ã€æŠ˜æ‰£ä¼˜åŠ¿éƒ½é‡‡ç”¨ABABæ’åˆ—
        abab_sheets = ['æ¡ç ç²¾ç¡®åŒ¹é…', 'åç§°æ¨¡ç³ŠåŒ¹é…', 'å·®å¼‚å“å¯¹æ¯”', 'åº“å­˜>0&AæŠ˜æ‰£', 'ç«å¯¹æˆæœ¬é¢„æµ‹', 'åˆ©æ¶¦ç©ºé—´å¯¹æ¯”', 'æˆæœ¬ä¼˜åŠ¿å•†å“']
        needs_abab = any(keyword in sheet_name for keyword in abab_sheets)
        print(f"   æ˜¯å¦è§¦å‘ABAB: {needs_abab}")
        
        if needs_abab:
            # ABABåˆ—æ’åˆ—ï¼šæŒ‰å­—æ®µç±»å‹äº¤æ›¿æ’åˆ—Aåº—å’ŒBåº—çš„åˆ—
            print(f"ğŸ“ {sheet_name}: å¯ç”¨ABABåˆ—æ’åˆ— (åº—é“º: {store_a} vs {store_b})")
            
            # å®šä¹‰æ ¸å¿ƒå­—æ®µé¡ºåºï¼ˆæŒ‰ä¸šåŠ¡é‡è¦æ€§ï¼‰
            field_order = [
                'å•†å“åç§°', 'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'æ¡ç ',
                'å”®ä»·', 'åŸä»·', 'æœˆå”®', 'åº“å­˜', 'è§„æ ¼åç§°', 'åº—å†…ç ', 'æŠ˜æ‰£'
            ]
            
            # ğŸ†• è‡ªåŠ¨å‘ç°é¢å¤–å­—æ®µï¼ˆä¸åœ¨é¢„å®šä¹‰åˆ—è¡¨ä¸­çš„A/Båˆ—ï¼‰
            all_a_fields = set()
            all_b_fields = set()
            for col in a_cols:
                # æŒ‰ä¼˜å…ˆçº§é¡ºåºç§»é™¤åç¼€
                field_name = col
                if field_name.endswith(f'_{store_a}'):
                    field_name = field_name[:-len(f'_{store_a}')]
                elif field_name.endswith('_A'):
                    field_name = field_name[:-2]
                elif field_name.endswith('A'):
                    field_name = field_name[:-1]
                all_a_fields.add(field_name)
            for col in b_cols:
                # æŒ‰ä¼˜å…ˆçº§é¡ºåºç§»é™¤åç¼€
                field_name = col
                if field_name.endswith(f'_{store_b}'):
                    field_name = field_name[:-len(f'_{store_b}')]
                elif field_name.endswith('_B'):
                    field_name = field_name[:-2]
                elif field_name.endswith('B'):
                    field_name = field_name[:-1]
                all_b_fields.add(field_name)
            
            # åˆå¹¶æ‰€æœ‰å­—æ®µï¼Œæ·»åŠ åˆ° field_order æœ«å°¾ï¼ˆå»é‡ï¼‰
            extra_fields = (all_a_fields | all_b_fields) - set(field_order)
            if extra_fields:
                print(f"   ğŸ” å‘ç°é¢å¤–å­—æ®µ: {sorted(extra_fields)}")
                field_order.extend(sorted(extra_fields))  # æŒ‰å­—æ¯é¡ºåºæ·»åŠ 
            
            # æ„å»ºABABæ’åˆ—
            abab_cols = []
            for field in field_order:
                # å°è¯•å¤šç§åˆ—åå˜ä½“ï¼ˆç²¾ç¡®åº—é“ºå > A/Båç¼€ï¼‰
                col_a_variants = [f'{field}_{store_a}', f'{field}_A', f'{field}A']
                col_b_variants = [f'{field}_{store_b}', f'{field}_B', f'{field}B']
                
                # æŸ¥æ‰¾Aåº—åˆ—
                found_a = None
                for var in col_a_variants:
                    if var in df.columns:
                        found_a = var
                        break
                
                # æŸ¥æ‰¾Båº—åˆ—
                found_b = None
                for var in col_b_variants:
                    if var in df.columns:
                        found_b = var
                        break
                
                # ABABäº¤æ›¿æ·»åŠ ï¼ˆä¼˜å…ˆæ·»åŠ é…å¯¹çš„A-Båˆ—ï¼‰
                if found_a and found_b:
                    if found_a not in abab_cols:
                        abab_cols.append(found_a)
                    if found_b not in abab_cols:
                        abab_cols.append(found_b)
                elif found_a:
                    if found_a not in abab_cols:
                        abab_cols.append(found_a)
                elif found_b:
                    if found_b not in abab_cols:
                        abab_cols.append(found_b)
            
            print(f"   âœ… ABABæ ¸å¿ƒåˆ—æ•°: {len(abab_cols)}")
            if abab_cols:
                print(f"   å‰10åˆ—: {abab_cols[:10]}")
            
            # æ·»åŠ ç‰¹æ®Šå­—æ®µï¼ˆæŠ˜æ‰£ã€ç›¸ä¼¼åº¦ç­‰ï¼‰åœ¨ABABåˆ—ä¹‹å
            special_cols = []
            # å·®å¼‚å“å¯¹æ¯”ç‰¹æœ‰ï¼šå¯¹æ¯”ä»·æ ¼æ¥æºï¼ˆåŠ¨æ€è¯†åˆ«åº—é“ºåï¼‰
            price_source_a = f'å¯¹æ¯”ä»·æ ¼æ¥æº_{store_a}'
            price_source_b = f'å¯¹æ¯”ä»·æ ¼æ¥æº_{store_b}'
            if price_source_a in df.columns:
                special_cols.extend([price_source_a, price_source_b])
            # æŠ˜æ‰£å­—æ®µï¼ˆå¯èƒ½ä½¿ç”¨A/Bæˆ–å®é™…åº—é“ºåï¼‰
            if 'æŠ˜æ‰£A' in df.columns:
                special_cols.extend(['æŠ˜æ‰£A', 'æŠ˜æ‰£B'])
            elif f'æŠ˜æ‰£{store_a}' in df.columns:
                special_cols.extend([f'æŠ˜æ‰£{store_a}', f'æŠ˜æ‰£{store_b}'])
            # ç›¸ä¼¼åº¦å­—æ®µ
            if 'composite_similarity_score' in df.columns:
                special_cols.append('composite_similarity_score')
            if 'price_diff_pct' in df.columns:  # å·®å¼‚å“å¯¹æ¯”ï¼šä»·å·®%
                special_cols.append('price_diff_pct')
            if 'similarity_score' in df.columns:  # å·®å¼‚å“å¯¹æ¯”ï¼šç›¸ä¼¼åº¦
                special_cols.append('similarity_score')
            if 'å·®å¼‚åˆ†æ' in df.columns:
                special_cols.append('å·®å¼‚åˆ†æ')
            if 'åˆ†ç±»ä¸€è‡´æ€§' in df.columns:  # å·®å¼‚å“å¯¹æ¯”ï¼šåˆ†ç±»ä¸€è‡´æ€§æ£€æŸ¥
                special_cols.append('åˆ†ç±»ä¸€è‡´æ€§')
            
            # å…¶ä½™åˆ—ï¼šæœªåŒ¹é…çš„Aåº—åˆ— -> æœªåŒ¹é…çš„Båº—åˆ— -> å…¬å…±åˆ—
            a_rest = [c for c in a_cols if c not in abab_cols]
            b_rest = [c for c in b_cols if c not in abab_cols]
            common_rest = [c for c in common_cols if c not in abab_cols + special_cols]
            
            # æœ€ç»ˆåˆ—é¡ºåº
            final_cols = abab_cols + special_cols + a_rest + b_rest + common_rest
            df = df[[c for c in final_cols if c in df.columns]]
            
            # ğŸ” ä¿å­˜è°ƒè¯•ä¿¡æ¯åˆ°æ–‡ä»¶
            # æ¸…ç†æ–‡ä»¶åä¸­çš„éæ³•å­—ç¬¦ï¼ˆWindows: < > : " / \ | ? *ï¼‰
            safe_sheet_name = sheet_name.replace('/', '-').replace('\\', '-').replace(':', '-').replace('*', '-').replace('?', '-').replace('<', '-').replace('>', '-').replace('|', '-').replace('"', '')
            debug_file = f"d:/abab_debug_{safe_sheet_name}.txt"
            with open(debug_file, 'w', encoding='utf-8') as f:
                f.write(f"Sheet: {sheet_name}\n")
                f.write(f"åº—é“º: {store_a} vs {store_b}\n\n")
                f.write(f"ABABåˆ—({len(abab_cols)}):\n")
                for i, col in enumerate(abab_cols, 1):
                    f.write(f"  {i}. {col}\n")
                f.write(f"\nSpecialåˆ—({len(special_cols)}):\n")
                for col in special_cols:
                    f.write(f"  - {col}\n")
                f.write(f"\næœ€ç»ˆåˆ—é¡ºåº(å‰20):\n")
                for i, col in enumerate(list(df.columns)[:20], 1):
                    f.write(f"  {i}. {col}\n")
            print(f"   ğŸ’¾ è°ƒè¯•ä¿¡æ¯å·²ä¿å­˜åˆ°: {debug_file}")
        else:
            # éå¯¹æ¯”ç±»è¡¨æ ¼ï¼šé»˜è®¤ Aåˆ— + Båˆ— + å…¶ä»–
            ordered_cols = a_cols + b_cols + common_cols
            df = df[ordered_cols]

        # æ¸…æ´—å¹¶ç¡®ä¿å·¥ä½œè¡¨ååˆæ³•ä¸”å”¯ä¸€
        try:
            existing_names = set(getattr(writer.book, 'sheetnames', []) or [])
        except Exception:
            existing_names = set()
        safe_name = _sanitize_sheet_name(sheet_name, existing_names)
        
        # ğŸ†• æ­¥éª¤4: Excelå±•ç¤ºæ—¶å°† _A/_B è½¬æ¢ä¸ºå®é™…åº—é“ºåç§°ï¼ˆä»…ç”¨äºæ˜¾ç¤ºï¼Œä¸å½±å“æ•°æ®å¤„ç†ï¼‰
        display_df = df.copy()
        display_rename = {}
        for col in display_df.columns:
            if col.endswith('_A'):
                display_rename[col] = col.replace('_A', f'_{store_a}')
            elif col.endswith('_B'):
                display_rename[col] = col.replace('_B', f'_{store_b}')
        
        if display_rename:
            display_df = display_df.rename(columns=display_rename)
            print(f"   ğŸ“Š Excelå±•ç¤º: {len(display_rename)} åˆ—è½¬æ¢ä¸ºåº—é“ºåç§°")
        
        display_df.to_excel(writer, sheet_name=safe_name, index=False)
        logging.info(f"âœ… å·¥ä½œè¡¨ã€Œ{safe_name}ã€å·²å¯¼å‡ºï¼ŒåŒ…å« {len(df)} æ¡è®°å½•ã€‚")
    else:
        try:
            existing_names = set(getattr(writer.book, 'sheetnames', []) or [])
        except Exception:
            existing_names = set()
        safe_name = _sanitize_sheet_name(sheet_name, existing_names)
        pd.DataFrame([{"æç¤º": "æ­¤åˆ†ç±»ä¸‹æ— æ•°æ®"}]).to_excel(writer, sheet_name=safe_name, index=False)
        logging.info(f"âš ï¸ å·¥ä½œè¡¨ã€Œ{safe_name}ã€æ— æ•°æ®ï¼Œå·²å¯¼å‡ºä¸ºç©ºç™½é¡µã€‚")

# ==============================================================================
# 5. ä¸»æ‰§è¡Œæµç¨‹ (Main Workflow)
# ==============================================================================
def main():
    # ä¿®å¤ Windows æ§åˆ¶å°ç¼–ç é—®é¢˜ï¼ˆæ”¯æŒä¸­æ–‡å’Œ emoji è¾“å‡ºï¼‰
    import sys
    import os
    
    # è®¾ç½®ç¯å¢ƒå˜é‡å¼ºåˆ¶UTF-8ï¼ˆå¿…é¡»åœ¨ä»»ä½•è¾“å‡ºå‰è®¾ç½®ï¼‰
    os.environ['PYTHONIOENCODING'] = 'utf-8'
    
    if sys.platform == 'win32':
        try:
            import io
            # é‡æ–°åŒ…è£… stdout/stderr ä¸º UTF-8 æ¨¡å¼ï¼ˆä»…å½“æœ‰æ•ˆæ—¶ï¼‰
            if sys.stdout is not None and hasattr(sys.stdout, 'buffer'):
                sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace', line_buffering=True)
            if sys.stderr is not None and hasattr(sys.stderr, 'buffer'):
                sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace', line_buffering=True)
            
            # Windows æ§åˆ¶å°ä»£ç é¡µè®¾ç½®ä¸º UTF-8ï¼ˆCMDæ¨¡å¼ï¼‰
            if hasattr(sys.stdout, 'reconfigure'):
                sys.stdout.reconfigure(encoding='utf-8', errors='replace')
                sys.stderr.reconfigure(encoding='utf-8', errors='replace')
        except Exception:
            pass  # å¦‚æœè®¾ç½®å¤±è´¥ï¼Œç»§ç»­è¿è¡Œ
    
    print("\n" + "="*60)
    print("  å•†å“æ¯”å¯¹åˆ†æå·¥å…· v8.5 å¯åŠ¨ä¸­...")
    print("="*60)
    
    cfg = Config()

    # ğŸ†• é‡è½½æ¨¡å‹ç¯å¢ƒå˜é‡ï¼ˆGUIæ¨¡å¼ä¼ é€’ï¼Œéœ€è¦åœ¨ Config å®ä¾‹åŒ–åå†æ¬¡è¯»å–ï¼‰
    embedding_model_override = os.environ.get('EMBEDDING_MODEL')
    reranker_model_override = os.environ.get('RERANKER_MODEL')
    if embedding_model_override:
        cfg.SENTENCE_BERT_MODEL = embedding_model_override
        print(f"âœ… åµŒå…¥æ¨¡å‹å·²åˆ‡æ¢: {embedding_model_override}")
    if reranker_model_override:
        cfg.ONLINE_CROSS_ENCODER = reranker_model_override
        print(f"âœ… ç²¾æ’æ¨¡å‹å·²åˆ‡æ¢: {reranker_model_override}")

    # éœ€è¦åœ¨å‡½æ•°é¡¶éƒ¨å£°æ˜ï¼Œä»¥ä¾¿åç»­å¼‚å¸¸åˆ†æ”¯å¯ä»¥ä¿®æ”¹è¯¥å…¨å±€å˜é‡
    global SIMPLE_FALLBACK

    # ç¯å¢ƒå˜é‡è¦†ç›–ï¼ˆä¾¿äºä¸çˆ¬è™«è”åŠ¨ï¼‰ï¼š
    # COMPARE_STORE_A_FILE / COMPARE_STORE_B_FILE: ç›´æ¥æŒ‡å®šA/Båº—æ•°æ®æ–‡ä»¶çš„ç»å¯¹è·¯å¾„
    # COMPARE_STORE_A_NAME / COMPARE_STORE_B_NAME: è¦†ç›–åº—é“ºæ˜¾ç¤ºåç§°
    env_a_file = os.environ.get('COMPARE_STORE_A_FILE')
    env_b_file = os.environ.get('COMPARE_STORE_B_FILE')
    env_a_name = os.environ.get('COMPARE_STORE_A_NAME')
    env_b_name = os.environ.get('COMPARE_STORE_B_NAME')
    if env_a_name:
        cfg.STORE_A_NAME = env_a_name
    if env_b_name:
        cfg.STORE_B_NAME = env_b_name
    # è‹¥æä¾›äº†æ–‡ä»¶ä½†æœªæä¾›æ˜¾ç¤ºåï¼Œåˆ™ç”¨æ–‡ä»¶åä¸»å¹²ä½œä¸ºæ˜¾ç¤ºåï¼ˆä¸è‡ªåŠ¨æ¯”ä»·å­è¿›ç¨‹ä¿æŒä¸€è‡´ï¼‰
    try:
        from pathlib import Path as _Path
        if (not env_a_name) and env_a_file:
            cfg.STORE_A_NAME = _Path(env_a_file).stem[:40]
        if (not env_b_name) and env_b_file:
            cfg.STORE_B_NAME = _Path(env_b_file).stem[:40]
    except Exception:
        pass

    print("\n" + "="*50)
    print("â³ [æ­¥éª¤ 1/7] æ£€æµ‹ç¡¬ä»¶åŠ é€Ÿå™¨ (GPU/CPU)...")
    forced = getattr(Config, 'FORCE_DEVICE', None)
    
    # æ£€æŸ¥æ˜¯å¦æœ‰ç¯å¢ƒå˜é‡å¼ºåˆ¶ç¦ç”¨CUDA
    if os.environ.get('CUDA_VISIBLE_DEVICES') == '':
        print("ğŸ› ï¸ æ£€æµ‹åˆ°CUDA_VISIBLE_DEVICES=''ï¼Œå¼ºåˆ¶ä½¿ç”¨CPUæ¨¡å¼")
        device = 'cpu'
    elif forced in ('cuda', 'cpu'):
        device = forced
        print(f"ğŸ› ï¸ æŒ‰é…ç½®å¼ºåˆ¶ä½¿ç”¨è®¾å¤‡: {device}")
    else:
        # å®‰å…¨çš„CUDAå¯ç”¨æ€§æ£€æŸ¥
        cuda_available = False
        try:
            cuda_available = torch.cuda.is_available()
            if cuda_available:
                # å°è¯•ç®€å•çš„CUDAæ“ä½œä»¥ç¡®è®¤çœŸæ­£å¯ç”¨
                test_tensor = torch.tensor([1.0]).cuda()
                del test_tensor
                torch.cuda.empty_cache()
        except Exception as cuda_error:
            print(f"âš ï¸ CUDAæ£€æµ‹å¤±è´¥: {cuda_error}")
            cuda_available = False
        
        device = 'cuda' if cuda_available else 'cpu'
        
        # ğŸš€ è‡ªåŠ¨å¯ç”¨GPUåŠ é€Ÿï¼šå¦‚æœæ£€æµ‹åˆ°GPUï¼Œè‡ªåŠ¨è®¾ç½®ç¯å¢ƒå˜é‡
        if cuda_available and os.environ.get('USE_TORCH_SIM') != '1':
            os.environ['USE_TORCH_SIM'] = '1'
            print("ğŸš€ æ£€æµ‹åˆ°NVIDIA GPUï¼Œè‡ªåŠ¨å¯ç”¨GPUåŠ é€Ÿï¼ˆå‘é‡ç›¸ä¼¼åº¦è®¡ç®—ï¼‰")
    
    if device == 'cuda':
        print("âœ… ä½¿ç”¨ GPU è¿è¡Œï¼ˆå·²å®‰è£… GPU ç‰ˆ PyTorchï¼‰")
        try:
            gpu_name = torch.cuda.get_device_name(0)
            gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
            print(f"   GPU: {gpu_name} ({gpu_memory:.1f} GB)")
        except:
            pass
    else:
        print("â„¹ï¸ ä½¿ç”¨ CPU è¿è¡Œï¼ˆæœªæ£€æµ‹åˆ°å¯ç”¨ GPU æˆ–æœªæŒ‡å®šä½¿ç”¨ GPUï¼‰")

    print("\n" + "="*50)
    print("â³ [æ­¥éª¤ 2/7] æ­£åœ¨åŠ è½½æ–‡æœ¬åˆ†ææ¨¡å‹ (è‹¥æœ¬åœ°æ— ç¼“å­˜ï¼Œå°†è‡ªåŠ¨ä¸‹è½½)...")
    
    # äº¤äº’å¼é€‰æ‹© Sentence-BERT æ¨¡å‹ï¼ˆç²—ç­›ï¼‰
    selected_model = select_embedding_model(cfg)
    if selected_model != cfg.SENTENCE_BERT_MODEL:
        # æŸ¥æ‰¾æ¨¡å‹çš„å‹å¥½åç§°
        model_display_name = selected_model
        for model_info in getattr(cfg, 'AVAILABLE_MODELS', {}).values():
            if model_info['name'] == selected_model:
                model_display_name = model_info['display_name']
                break
        cfg.SENTENCE_BERT_MODEL = selected_model
        print(f"\nğŸ“ å·²åˆ‡æ¢ Sentence-BERT åˆ°: {model_display_name}")
        print(f"   æ¨¡å‹ID: {selected_model}")
    
    # äº¤äº’å¼é€‰æ‹© Cross-Encoder æ¨¡å‹ï¼ˆç²¾æ’ï¼‰
    selected_ce_model = select_cross_encoder_model(cfg)
    if selected_ce_model != cfg.ONLINE_CROSS_ENCODER:
        # æŸ¥æ‰¾æ¨¡å‹çš„å‹å¥½åç§°
        ce_display_name = selected_ce_model
        for model_info in getattr(cfg, 'AVAILABLE_CROSS_ENCODERS', {}).values():
            if model_info['name'] == selected_ce_model:
                ce_display_name = model_info['display_name']
                break
        cfg.ONLINE_CROSS_ENCODER = selected_ce_model
        print(f"\nğŸ“ å·²åˆ‡æ¢ Cross-Encoder åˆ°: {ce_display_name}")
        print(f"   æ¨¡å‹ID: {selected_ce_model}")
    
    # ç¯å¢ƒå˜é‡è¦†ç›–æœ¬åœ°æ¨¡å‹è·¯å¾„/ç­–ç•¥
    env_local_sbert = os.environ.get('LOCAL_SENTENCE_BERT_PATH')
    env_use_local_sbert = os.environ.get('USE_LOCAL_SENTENCE_BERT')
    if env_local_sbert:
        cfg.LOCAL_SENTENCE_BERT_PATH = env_local_sbert
        cfg.USE_LOCAL_SENTENCE_BERT = True if str(env_use_local_sbert or '1') == '1' else cfg.USE_LOCAL_SENTENCE_BERT

    env_local_ce = os.environ.get('LOCAL_CROSS_ENCODER_PATH')
    env_use_local_ce = os.environ.get('USE_LOCAL_CROSS_ENCODER')
    if env_local_ce:
        cfg.LOCAL_CROSS_ENCODER_PATH = env_local_ce
        cfg.USE_LOCAL_CROSS_ENCODER = True if str(env_use_local_ce or '1') == '1' else cfg.USE_LOCAL_CROSS_ENCODER

    # æ™ºèƒ½æ£€æµ‹æ¨¡å‹æ˜¯å¦éœ€è¦ä¸‹è½½ï¼ˆå¼€å‘ç¯å¢ƒæç¤ºï¼Œæ‰“åŒ…ç¯å¢ƒè·³è¿‡ï¼‰
    # âš ï¸ å…³é”®ï¼šå¿…é¡»å…ˆå®šä¹‰ model_exists é»˜è®¤å€¼ï¼ˆæ‰“åŒ…ç¯å¢ƒä¹Ÿä¼šç”¨åˆ°ï¼‰
    model_exists = False
    
    # æ‰“åŒ…ç¯å¢ƒï¼šæ£€æµ‹å†…ç½®æ¨¡å‹æ˜¯å¦å­˜åœ¨
    if getattr(sys, 'frozen', False):
        local_model_path = get_local_model_path(cfg.SENTENCE_BERT_MODEL)
        model_exists = os.path.exists(local_model_path)
        if not model_exists:
            print(f"âš ï¸  æ‰“åŒ…ç¯å¢ƒæœªæ‰¾åˆ°æ¨¡å‹: {local_model_path}")
    
    # å¼€å‘ç¯å¢ƒï¼šæ£€æµ‹å’Œæç¤º
    if not getattr(sys, 'frozen', False):
        if getattr(cfg, 'USE_LOCAL_SENTENCE_BERT', False) and os.path.exists(cfg.LOCAL_SENTENCE_BERT_PATH):
            model_exists = True
        else:
            model_exists = check_model_exists(cfg.SENTENCE_BERT_MODEL)
        
        if model_exists:
            print("âš¡ æ£€æµ‹åˆ°æœ¬åœ°æ¨¡å‹ç¼“å­˜ï¼Œå¿«é€ŸåŠ è½½ä¸­...")
        else:
            # åŠ¨æ€è·å–æ¨¡å‹å¤§å°ä¿¡æ¯
            model_size = "æœªçŸ¥å¤§å°"
            download_time = "å‡ åˆ†é’Ÿ"
            for model_info in getattr(cfg, 'AVAILABLE_MODELS', {}).values():
                if model_info['name'] == cfg.SENTENCE_BERT_MODEL:
                    model_size = model_info.get('size', 'æœªçŸ¥å¤§å°')
                    # æ ¹æ®å¤§å°ä¼°ç®—ä¸‹è½½æ—¶é—´
                    if 'GB' in model_size or 'gb' in model_size:
                        size_num = float(model_size.replace('~', '').replace('GB', '').replace('gb', '').strip())
                        if size_num >= 2:
                            download_time = "10-20åˆ†é’Ÿ"
                        elif size_num >= 1:
                            download_time = "5-10åˆ†é’Ÿ"
                        else:
                            download_time = "3-5åˆ†é’Ÿ"
                    elif 'MB' in model_size or 'mb' in model_size:
                        download_time = "1-3åˆ†é’Ÿ"
                    break
            
            print(f"ğŸ’¡ é¦–æ¬¡ä½¿ç”¨æ­¤æ¨¡å‹ï¼Œéœ€è¦ä¸‹è½½æ¨¡å‹æ–‡ä»¶ï¼ˆ{model_size}ï¼Œé¢„è®¡{download_time}ï¼‰")
            print(f"ğŸ“¥ ä¸‹è½½æ¨¡å‹: {cfg.SENTENCE_BERT_MODEL}")
            print("â³ è¯·è€å¿ƒç­‰å¾…ï¼Œæ¨¡å‹å°†è‡ªåŠ¨ç¼“å­˜åˆ°æœ¬åœ°...")
    
    try:
        # åªè¦ USE_LOCAL_SENTENCE_BERT=1 æˆ–æœ¬åœ°æ¨¡å‹ç›®å½•å­˜åœ¨ï¼Œå¼ºåˆ¶åªç”¨æœ¬åœ°è·¯å¾„åŠ è½½ï¼Œå½»åº•æ–­ç½‘
        use_local = getattr(cfg, 'USE_LOCAL_SENTENCE_BERT', False)
        local_path = getattr(cfg, 'LOCAL_SENTENCE_BERT_PATH', None)
        local_path_exists = local_path and os.path.exists(local_path)
        for _k in ['HTTP_PROXY','HTTPS_PROXY','ALL_PROXY','http_proxy','https_proxy','all_proxy']:
            os.environ.pop(_k, None)
        os.environ['HF_HUB_OFFLINE'] = '1'
        os.environ['TRANSFORMERS_OFFLINE'] = '1'
        if use_local and local_path_exists:
            print(f"ğŸ“± å¼ºåˆ¶ä»…ç”¨æœ¬åœ°ç›®å½•åŠ è½½ Sentence-BERT: {local_path}")
            try:
                model = SentenceTransformer(local_path, device=device, use_auth_token=False)
            except Exception as e:
                if 'cuda' in str(e).lower() or 'gpu' in str(e).lower():
                    print(f"âš ï¸ GPUæ¨¡å¼åŠ è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                    device = 'cpu'
                    model = SentenceTransformer(local_path, device=device, use_auth_token=False)
                else:
                    raise e
        elif model_exists:
            # æ‰“åŒ…ç¯å¢ƒï¼šç›´æ¥ä½¿ç”¨ get_local_model_path() è·å–å†…ç½®æ¨¡å‹è·¯å¾„
            # å¼€å‘ç¯å¢ƒï¼šä» huggingface ç¼“å­˜åŠ è½½
            if getattr(sys, 'frozen', False):
                # æ‰“åŒ…ç¯å¢ƒï¼šä½¿ç”¨å†…ç½®æ¨¡å‹
                print("ğŸ“± ä½¿ç”¨æ‰“åŒ…çš„å†…ç½®æ¨¡å‹åŠ è½½ Sentence-BERT...")
                bundled_model_path = get_local_model_path(cfg.SENTENCE_BERT_MODEL)
                try:
                    model = SentenceTransformer(bundled_model_path, device=device, use_auth_token=False)
                except Exception as e:
                    if 'cuda' in str(e).lower() or 'gpu' in str(e).lower():
                        print(f"âš ï¸ GPUæ¨¡å¼åŠ è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                        device = 'cpu'
                        model = SentenceTransformer(bundled_model_path, device=device, use_auth_token=False)
                    else:
                        raise e
            else:
                # å¼€å‘ç¯å¢ƒï¼šä» huggingface ç¼“å­˜åŠ è½½
                print("ğŸ“± å¼ºåˆ¶ä»…ç”¨æœ¬åœ°ç¼“å­˜åŠ è½½ Sentence-BERT...")
                from pathlib import Path
                # è‡ªåŠ¨å®šä½ huggingface hub ç¼“å­˜ä¸‹çš„ snapshots å­ç›®å½•
                hub_cache = Path.home() / ".cache" / "huggingface" / "hub" / f"models--sentence-transformers--{cfg.SENTENCE_BERT_MODEL}" / "snapshots"
                if hub_cache.exists():
                    # å–æœ€æ–°çš„å¿«ç…§ç›®å½•
                    latest = sorted(hub_cache.iterdir(), key=lambda p: p.stat().st_mtime, reverse=True)[0]
                    try:
                        model = SentenceTransformer(str(latest), device=device, use_auth_token=False)
                    except Exception as e:
                        if 'cuda' in str(e).lower() or 'gpu' in str(e).lower():
                            print(f"âš ï¸ GPUæ¨¡å¼åŠ è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                            device = 'cpu'
                            model = SentenceTransformer(str(latest), device=device, use_auth_token=False)
                        else:
                            raise e
                else:
                    raise RuntimeError("æœªæ‰¾åˆ°æœ¬åœ°ç¼“å­˜å¿«ç…§ç›®å½•")
        else:
            # å¼€å‘ç¯å¢ƒï¼šé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è½½æ¨¡å‹ï¼ˆä¿ç•™åœ¨çº¿ä¸‹è½½åŠŸèƒ½ï¼‰
            # æ‰“åŒ…ç¯å¢ƒï¼šç›´æ¥ä½¿ç”¨å†…ç½®æ¨¡å‹ï¼ˆä¸åº”è¯¥èµ°åˆ°è¿™é‡Œï¼‰
            if getattr(sys, 'frozen', False):
                # æ‰“åŒ…ç¯å¢ƒä½†æ¨¡å‹ä¸å­˜åœ¨ - æ‰“åŒ…å¼‚å¸¸
                error_msg = (
                    "âŒ æ‰“åŒ…æ¨¡å‹ç¼ºå¤±\n\n"
                    f"æœªæ‰¾åˆ°æ¨¡å‹: {cfg.SENTENCE_BERT_MODEL}\n\n"
                    "è¿™å¯èƒ½æ˜¯æ‰“åŒ…å¼‚å¸¸å¯¼è‡´çš„ï¼Œè¯·è”ç³»ç®¡ç†å‘˜é‡æ–°è·å–å®Œæ•´çš„å®‰è£…åŒ…ã€‚"
                )
                if os.environ.get('GUI_MODE') == '1':
                    try:
                        import tkinter as tk
                        from tkinter import messagebox
                        root = tk.Tk()
                        root.withdraw()
                        messagebox.showerror("æ¨¡å‹ç¼ºå¤±", error_msg)
                        root.destroy()
                    except:
                        print(error_msg)
                else:
                    print(error_msg)
                sys.exit(1)
            
            # å¼€å‘ç¯å¢ƒä¸‹è½½æ¨¡å‹
            print("ğŸ“¥ é¦–æ¬¡ä½¿ç”¨ï¼Œæ­£åœ¨ä¸‹è½½æ¨¡å‹...")
            print("ğŸ’¡ æç¤ºï¼šå¦‚éœ€åŠ é€Ÿï¼Œå¯è®¾ç½®é•œåƒæº: $env:HF_ENDPOINT='https://hf-mirror.com'")
            
            # ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆå¼€å‘ç¯å¢ƒä»ä¼šè§¦å‘ä¸‹è½½ï¼‰
            model_path = get_local_model_path(cfg.SENTENCE_BERT_MODEL)
            
            try:
                model = SentenceTransformer(model_path, device=device, use_auth_token=False)
            except Exception as e:
                if 'cuda' in str(e).lower() or 'gpu' in str(e).lower():
                    print(f"âš ï¸ GPUæ¨¡å¼åŠ è½½å¤±è´¥ï¼Œåˆ‡æ¢åˆ°CPU: {e}")
                    device = 'cpu'
                    model = SentenceTransformer(model_path, device=device, use_auth_token=False)
                else:
                    raise e
            print("âœ… æ¨¡å‹ä¸‹è½½å¹¶åŠ è½½æˆåŠŸï¼ä¸‹æ¬¡è¿è¡Œå°†ç›´æ¥ä½¿ç”¨ç¼“å­˜ã€‚")

        model.encode(["æµ‹è¯•"], show_progress_bar=False)  # æµ‹è¯•æ¨¡å‹æ˜¯å¦å¯ç”¨
        print("âœ… Sentence-BERT æ¨¡å‹åŠ è½½æˆåŠŸï¼")

        # å°è¯•åŠ è½½Cross-Encoderæ¨¡å‹
        cross_encoder = None
        try:
            # å…è®¸é€šè¿‡ç¯å¢ƒå˜é‡å¼ºåˆ¶ç¦ç”¨ Cross-Encoderï¼ˆé¿å…è”ç½‘æˆ–æ½œåœ¨å¡é¡¿ï¼‰
            if os.environ.get('DISABLE_CROSS_ENCODER', '0') == '1':
                print("âš™ï¸ å·²æ ¹æ®ç¯å¢ƒå˜é‡ç¦ç”¨ Cross-Encoder ç²¾æ’ï¼ˆDISABLE_CROSS_ENCODER=1ï¼‰")
                cross_encoder = None
            elif getattr(cfg, 'USE_LOCAL_CROSS_ENCODER', False):
                cross_encoder_path = cfg.LOCAL_CROSS_ENCODER_PATH
                if os.path.exists(cross_encoder_path):
                    cross_encoder = CrossEncoder(cross_encoder_path, device=device) if CrossEncoder else None
                    print("âœ… æœ¬åœ°Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸï¼")
                else:
                    print("âš ï¸ æœ¬åœ°Cross-Encoderæ¨¡å‹è·¯å¾„ä¸å­˜åœ¨ï¼Œå°†ä½¿ç”¨åœ¨çº¿æ¨¡å‹")
                    cross_encoder = CrossEncoder(cfg.ONLINE_CROSS_ENCODER, device=device) if CrossEncoder else None
            else:
                print("â³ æ­£åœ¨åŠ è½½Cross-Encoderæ¨¡å‹...")
                
                # ä½¿ç”¨æœ¬åœ°è·¯å¾„ï¼ˆæ‰“åŒ…ç¯å¢ƒç›´æ¥ç”¨å†…ç½®æ¨¡å‹ï¼Œå¼€å‘ç¯å¢ƒç”¨ç¼“å­˜ï¼‰
                cross_encoder_model_path = get_local_model_path(cfg.ONLINE_CROSS_ENCODER)
                print(f"ğŸ“ ä½¿ç”¨æ‰“åŒ…çš„æœ¬åœ°æ¨¡å‹: {cross_encoder_model_path}")
                
                # å°è¯•å¤šç§åŠ è½½æ–¹å¼ä»¥æé«˜å…¼å®¹æ€§
                cross_encoder = None
                if CrossEncoder:
                    try:
                        # æ–¹å¼1ï¼šæ ‡å‡†åŠ è½½
                        cross_encoder = CrossEncoder(cross_encoder_model_path, device=device)
                        print("âœ… Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸï¼")
                    except Exception as e1:
                        if "metaclip" in str(e1).lower() or "No module named" in str(e1):
                            # æ–¹å¼2ï¼šä½¿ç”¨ trust_remote_codeï¼ˆè§£å†³æ¨¡å—å¯¼å…¥é—®é¢˜ï¼‰
                            try:
                                print(f"   âš™ï¸  æ£€æµ‹åˆ°æ¨¡å—å¯¼å…¥é—®é¢˜ï¼Œå°è¯•ä½¿ç”¨å…¼å®¹æ¨¡å¼...")
                                cross_encoder = CrossEncoder(
                                    cross_encoder_model_path, 
                                    device=device,
                                    trust_remote_code=True  # ä¿¡ä»»è¿œç¨‹ä»£ç ï¼Œç»•è¿‡æ¨¡å—æ£€æŸ¥
                                )
                                print("âœ… Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆå…¼å®¹æ¨¡å¼ï¼‰ï¼")
                            except Exception as e2:
                                # æ–¹å¼3ï¼šé™çº§åˆ°AutoModelForSequenceClassification
                                try:
                                    print(f"   âš™ï¸  å°è¯•ä½¿ç”¨ AutoModel ç›´æ¥åŠ è½½...")
                                    from transformers import AutoModelForSequenceClassification, AutoTokenizer
                                    tokenizer = AutoTokenizer.from_pretrained(cross_encoder_model_path)
                                    model = AutoModelForSequenceClassification.from_pretrained(
                                        cross_encoder_model_path,
                                        trust_remote_code=True
                                    ).to(device)
                                    # æ‰‹åŠ¨åŒ…è£…æˆ CrossEncoder å…¼å®¹å¯¹è±¡
                                    class ManualCrossEncoder:
                                        def __init__(self, model, tokenizer, device):
                                            self.model = model
                                            self.tokenizer = tokenizer
                                            self.device = device
                                        
                                        def predict(self, sentences, batch_size=32, show_progress_bar=False):
                                            import torch
                                            scores = []
                                            for i in range(0, len(sentences), batch_size):
                                                batch = sentences[i:i+batch_size]
                                                inputs = self.tokenizer(
                                                    batch, 
                                                    padding=True, 
                                                    truncation=True, 
                                                    return_tensors="pt",
                                                    max_length=512
                                                ).to(self.device)
                                                with torch.no_grad():
                                                    outputs = self.model(**inputs)
                                                    batch_scores = outputs.logits[:, 0].cpu().numpy()
                                                scores.extend(batch_scores)
                                            return scores
                                    
                                    cross_encoder = ManualCrossEncoder(model, tokenizer, device)
                                    print("âœ… Cross-Encoderæ¨¡å‹åŠ è½½æˆåŠŸï¼ˆAutoModelæ¨¡å¼ï¼‰ï¼")
                                except Exception as e3:
                                    raise e1  # æŠ›å‡ºæœ€åˆçš„é”™è¯¯
                        else:
                            raise e1
        except Exception as ce_error:
            error_msg = str(ce_error)
            print(f"\nâŒ Cross-Encoderæ¨¡å‹åŠ è½½å¤±è´¥ï¼ˆä¸¥é‡é”™è¯¯ï¼‰:")
            print(f"   é”™è¯¯: {error_msg}")
            
            # åˆ¤æ–­é”™è¯¯ç±»å‹å¹¶ç»™å‡ºé’ˆå¯¹æ€§å»ºè®®
            if "couldn't connect" in error_msg or "Connection" in error_msg:
                if getattr(sys, 'frozen', False):
                    # æ‰“åŒ…ç¯å¢ƒä¸åº”è¯¥æœ‰ç½‘ç»œé—®é¢˜ï¼ˆæ¨¡å‹å·²å†…ç½®ï¼‰
                    print(f"\nâŒ æ„å¤–çš„ç½‘ç»œé”™è¯¯ï¼ˆæ‰“åŒ…ç‰ˆæœ¬ä¸åº”è”ç½‘ï¼‰")
                    print(f"   è¿™å¯èƒ½æ˜¯æ‰“åŒ…å¼‚å¸¸æˆ–æ–‡ä»¶æŸå")
                    print(f"   è¯·è”ç³»ç®¡ç†å‘˜é‡æ–°è·å–å®‰è£…åŒ…")
                else:
                    # å¼€å‘ç¯å¢ƒæä¾›ç½‘ç»œè§£å†³æ–¹æ¡ˆ
                    print(f"\nğŸŒ ç½‘ç»œè¿æ¥é—®é¢˜æ£€æµ‹åˆ°ï¼")
                    print(f"   å½“å‰å°è¯•ä¸‹è½½çš„æ¨¡å‹: {cfg.ONLINE_CROSS_ENCODER}")
                    print(f"\nğŸ’¡ å¿«é€Ÿè§£å†³æ–¹æ¡ˆ:")
                    print(f"   1. âš¡ ä½¿ç”¨é•œåƒæºï¼ˆæ¨èï¼‰:")
                    print(f"      åœ¨ç»ˆç«¯æ‰§è¡Œ: $env:HF_ENDPOINT='https://hf-mirror.com'")
                    print(f"   2. ï¿½ æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å¹¶æ”¾åˆ°ç¼“å­˜ç›®å½•")
            elif "metaclip" in error_msg.lower():
                print(f"\nğŸ”§ æ¨¡å‹å…¼å®¹æ€§é—®é¢˜:")
                print(f"   transformers åº“å¯èƒ½ç¼ºå°‘å¿…è¦çš„æ¨¡å‹ç»„ä»¶")
                print(f"\nğŸ’¡ è§£å†³æ–¹æ¡ˆ:")
                print(f"   1. æ›´æ–° transformers: pip install --upgrade transformers")
                print(f"   2. é‡æ–°å®‰è£…ä¾èµ–: pip install -r requirements.txt")
            
            # âŒ ä¸æ¥å—é™çº§ï¼Œç›´æ¥é€€å‡º
            print(f"\nâŒ Cross-Encoder æ˜¯æ ¸å¿ƒç»„ä»¶ï¼Œç¨‹åºæ— æ³•åœ¨é™çº§æ¨¡å¼ä¸‹è¿è¡Œ")
            print(f"   è¯·ä¿®å¤ä¸Šè¿°é—®é¢˜åé‡æ–°å¯åŠ¨")
            sys.exit(1)

    except Exception as e:
        print(f"âŒ æ¨¡å‹åŠ è½½å¤±è´¥: {e}")
        
        if getattr(sys, 'frozen', False):
            # æ‰“åŒ…ç¯å¢ƒæ¨¡å‹åŠ è½½å¤±è´¥ - ä¸¥é‡é”™è¯¯
            print("\nâŒ æ‰“åŒ…ç¯å¢ƒæ¨¡å‹åŠ è½½å¤±è´¥")
            print("   è¿™å¯èƒ½æ˜¯æ‰“åŒ…å¼‚å¸¸æˆ–æ–‡ä»¶æŸåï¼Œè¯·è”ç³»ç®¡ç†å‘˜é‡æ–°è·å–å®‰è£…åŒ…")
            sys.exit(1)
        else:
            # å¼€å‘ç¯å¢ƒæä¾›è¯¦ç»†è§£å†³æ–¹æ¡ˆ
            print("\nğŸ”§ å¯èƒ½çš„è§£å†³æ–¹æ¡ˆ:")
            print("1. ç½‘ç»œé—®é¢˜è§£å†³:")
            print("   - æ£€æŸ¥ç½‘ç»œè¿æ¥æ˜¯å¦ç¨³å®š")
            print("   - è®¾ç½®é•œåƒæº: $env:HF_ENDPOINT='https://hf-mirror.com'")
            print("\n2. ä¾èµ–åº“æ›´æ–°:")
            print("   pip install --upgrade sentence-transformers torch transformers")
            print("\n3. å¼ºåˆ¶ç¦»çº¿æ¨¡å¼:")
            print("   - è®¾ç½®: $env:TRANSFORMERS_OFFLINE='1'")
            
            print("\næ­£åœ¨å°è¯•ä½¿ç”¨å¤‡ç”¨æ–¹æ¡ˆ...")
            
            if SIMPLE_FALLBACK:
                # ä»…åœ¨æ˜ç¡®å…è®¸æ—¶æ‰é™çº§
                SIMPLE_FALLBACK = True
                model = None
                cross_encoder = None
                print("âš¡ å·²åˆ‡æ¢åˆ°ç®€åŒ–å…œåº•æ¨¡å¼ï¼šä½¿ç”¨è½»é‡æ–‡æœ¬ç›¸ä¼¼åº¦å®ŒæˆåŒ¹é…")
            else:
                print("ğŸš« å·²ç¦æ­¢é™çº§å…œåº•æ¨¡å¼ã€‚ä¸ºä¿è¯å‡†ç¡®ç‡ï¼Œç¨‹åºå°†é€€å‡ºã€‚")
                sys.exit(1)

    print("\n" + "="*50)
    print("â³ [æ­¥éª¤ 3/7] æ­£åœ¨æŸ¥æ‰¾æœ¬åœ°æ–‡ä»¶...")
    
    # ä¼˜å…ˆçº§ï¼šç¯å¢ƒå˜é‡ > ä¸Šä¼ ç›®å½• > é…ç½®æ–‡ä»¶
    store_a_file = None
    store_b_file = None
    
    try:
        # 1. ä¼˜å…ˆä½¿ç”¨ç¯å¢ƒå˜é‡æŒ‡å®šçš„æ–‡ä»¶
        if env_a_file and os.path.exists(env_a_file) and env_b_file and os.path.exists(env_b_file):
            store_a_file = env_a_file
            store_b_file = env_b_file
            print(f"âœ… é€šè¿‡ç¯å¢ƒå˜é‡æŒ‡å®šæ–‡ä»¶:")
            print(f"  æœ¬åº—: {store_a_file}")
            print(f"  ç«å¯¹: {store_b_file}")
        
        # 2. å°è¯•ä»ä¸Šä¼ ç›®å½•æ£€æµ‹æ–‡ä»¶ï¼ˆå¦‚æœå¯ç”¨ï¼‰
        elif getattr(cfg, 'USE_UPLOAD_DIRS', True):
            store_a_file, store_b_file, auto_name_a, auto_name_b = detect_files_from_upload_dirs(cfg)
            
            print(f"\nğŸ” è°ƒè¯•ä¿¡æ¯:")
            print(f"  æ£€æµ‹åˆ°çš„æ–‡ä»¶A: {store_a_file}")
            print(f"  æ£€æµ‹åˆ°çš„æ–‡ä»¶B: {store_b_file}")
            print(f"  åº—é“ºåA: {auto_name_a}")
            print(f"  åº—é“ºåB: {auto_name_b}")
            
            # å¦‚æœæ£€æµ‹åˆ°æ–‡ä»¶ï¼Œæ›´æ–°åº—é“ºåç§°
            if store_a_file and store_b_file:
                cfg.STORE_A_NAME = auto_name_a
                cfg.STORE_B_NAME = auto_name_b
                print(f"\nğŸ“ å·²è‡ªåŠ¨è¯†åˆ«åº—é“ºåç§°:")
                print(f"  ğŸª æœ¬åº—: {cfg.STORE_A_NAME}")
                print(f"  ğŸ¬ ç«å¯¹: {cfg.STORE_B_NAME}")
            else:
                print(f"\nâš ï¸ æ–‡ä»¶æ£€æµ‹å¤±è´¥ï¼Œå°†å›é€€åˆ°é…ç½®æ–‡ä»¶æ¨¡å¼")
        
        # 3. å›é€€åˆ°é…ç½®æ–‡ä»¶æŒ‡å®šçš„æ–‡ä»¶å
        if not store_a_file or not store_b_file:
            print("\nğŸ”„ ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­æŒ‡å®šçš„æ–‡ä»¶å...")
            if not store_a_file:
                store_a_file = get_local_filepath(cfg.STORE_A_FILENAME)
            if not store_b_file:
                store_b_file = get_local_filepath(cfg.STORE_B_FILENAME)
        
    except Exception as e:
        print(f"[é”™è¯¯] æ–‡ä»¶æŸ¥æ‰¾å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        sys.exit(1)

    if not all([store_a_file, store_b_file]):
        print("\nâŒ Missing required store files. Please ensure:")
        base_dir = os.path.dirname(os.path.abspath(__file__))
        upload_a = os.path.join(base_dir, getattr(cfg, 'UPLOAD_DIR_STORE_A', 'upload/store_a'))
        upload_b = os.path.join(base_dir, getattr(cfg, 'UPLOAD_DIR_STORE_B', 'upload/store_b'))
        print(f"  1. Put your store Excel file in: {upload_a}")
        print(f"  2. Put competitor Excel file in: {upload_b}")
        print(f"  OR")
        print(f"  3. Set correct filenames in config, OR")
        print(f"  4. Use environment variables COMPARE_STORE_* to specify absolute paths")
        print(f"\nCurrent script directory: {base_dir}")
        sys.exit(1)

    print("\n" + "="*50)
    print(f"â³ [æ­¥éª¤ 4/7] æ­£åœ¨å¤„ç†ã€Œ{cfg.STORE_A_NAME}ã€çš„æ•°æ®...")
    try:
        cache_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), cfg.EMBEDDING_CACHE_FILE)
        print(f"ğŸ’¾ å¯ç”¨å‘é‡ç¼“å­˜: {os.path.basename(cache_path)}")
        df_a_barcode, df_a_no_barcode = load_and_process_store_data(store_a_file, model, cache_path, role='A')
    except Exception as e:
        print(f"[é”™è¯¯] å¤„ç†Aåº—æ•°æ®å¤±è´¥: {e}")
        sys.exit(1)

    print(f"\nâ³ [æ­¥éª¤ 4/7] æ­£åœ¨å¤„ç†ã€Œ{cfg.STORE_B_NAME}ã€çš„æ•°æ®...")
    try:
        cache_path = os.path.join(os.path.dirname(os.path.abspath(__file__)), cfg.EMBEDDING_CACHE_FILE)
        print(f"ğŸ’¾ å¯ç”¨å‘é‡ç¼“å­˜: {os.path.basename(cache_path)}")
        df_b_barcode, df_b_no_barcode = load_and_process_store_data(store_b_file, model, cache_path, role='B')
    except Exception as e:
        print(f"[é”™è¯¯] å¤„ç†Båº—æ•°æ®å¤±è´¥: {e}")
        sys.exit(1)

    print("\n" + "="*50)
    print("â³ [æ­¥éª¤ 5/7] æ­£åœ¨è¿›è¡Œå•†å“åŒ¹é…...")
    try:
        # --- é˜¶æ®µ1: æ¡ç ç²¾ç¡®åŒ¹é… ---
        # ğŸ”§ ä½¿ç”¨ç®€çŸ­åç¼€ A/B æ›¿ä»£åº—é“ºåï¼Œç¡®ä¿ABABæ’åˆ—ç”Ÿæ•ˆ
        barcode_matches_df = match_by_barcode(df_a_barcode, df_b_barcode, "A", "B")
        logging.info(f"ã€é˜¶æ®µ1/3ã€‘æ¡ç ç²¾ç¡®åŒ¹é…æ‰¾åˆ° {len(barcode_matches_df)} ä¸ªå•†å“ã€‚")

        # --- å‡†å¤‡æ¨¡ç³ŠåŒ¹é…æ±  ---
        # æ‰¾å‡ºåœ¨æ¡ç åŒ¹é…ä¸­æœªæˆåŠŸçš„å•†å“
        if not barcode_matches_df.empty:
            # æ­£ç¡®çš„é€»è¾‘ï¼šåˆå¹¶åçš„æ¡ç åˆ—å°±å«'æ¡ç 'ï¼Œç›´æ¥ç”¨å®ƒæ¥è·å–å·²åŒ¹é…çš„æ¡ç åˆ—è¡¨
            matched_barcodes = barcode_matches_df['æ¡ç '].unique()
            
            unmatched_a_with_barcode = df_a_barcode[~df_a_barcode['æ¡ç '].isin(matched_barcodes)]
            unmatched_b_with_barcode = df_b_barcode[~df_b_barcode['æ¡ç '].isin(matched_barcodes)]
        else:
            unmatched_a_with_barcode = df_a_barcode
            unmatched_b_with_barcode = df_b_barcode

        # åˆå¹¶ã€æœ‰æ¡ç ä½†æœªåŒ¹é…ä¸Šçš„ã€‘å’Œã€æ— æ¡ç çš„ã€‘å•†å“ï¼Œå½¢æˆå®Œæ•´çš„æ¨¡ç³ŠåŒ¹é…æ± 
        fuzzy_pool_a = pd.concat([unmatched_a_with_barcode, df_a_no_barcode], ignore_index=True)
        fuzzy_pool_b = pd.concat([unmatched_b_with_barcode, df_b_no_barcode], ignore_index=True)

        logging.info(f"ã€å‡†å¤‡æ¨¡ç³ŠåŒ¹é…ã€‘Aåº—è¿›å…¥æ¨¡ç³ŠåŒ¹é…æ± çš„å•†å“æ•°: {len(fuzzy_pool_a)} (æœ‰ç æœªåŒ¹é…: {len(unmatched_a_with_barcode)}, æ— ç : {len(df_a_no_barcode)})")
        logging.info(f"ã€å‡†å¤‡æ¨¡ç³ŠåŒ¹é…ã€‘Båº—è¿›å…¥æ¨¡ç³ŠåŒ¹é…æ± çš„å•†å“æ•°: {len(fuzzy_pool_b)} (æœ‰ç æœªåŒ¹é…: {len(unmatched_b_with_barcode)}, æ— ç : {len(df_b_no_barcode)})")

        # === å¯é€‰ï¼šæŒ‰Bä¾§åˆ†ç±»è‡ªåŠ¨é™åŸŸï¼ˆå‡å°‘Aä¾§æœç´¢ç©ºé—´ï¼Œæé«˜é€Ÿåº¦ä¸”ä¸é™å‡†ç¡®ç‡ï¼‰ ===
        try:
            auto_scope_cat1 = os.environ.get('AUTO_SCOPE_BY_B_CAT1', '1') == '1'
            auto_scope_cat3 = os.environ.get('AUTO_SCOPE_BY_B_CAT3', '0') == '1'
            max_cat1 = int(os.environ.get('SCOPE_CAT1_MAX', '3'))
            max_cat3 = int(os.environ.get('SCOPE_CAT3_MAX', '6'))

            a_before = len(fuzzy_pool_a)
            scope_msgs = []
            if auto_scope_cat1 and 'ä¸€çº§åˆ†ç±»' in fuzzy_pool_b.columns and 'ä¸€çº§åˆ†ç±»' in fuzzy_pool_a.columns:
                cats1 = sorted(set(str(x) for x in fuzzy_pool_b['ä¸€çº§åˆ†ç±»'].dropna().unique()))
                if 0 < len(cats1) <= max_cat1:
                    fuzzy_pool_a = fuzzy_pool_a[fuzzy_pool_a['ä¸€çº§åˆ†ç±»'].astype(str).isin(cats1)]
                    scope_msgs.append(f"æŒ‰Bçš„ä¸€çº§åˆ†ç±»é™åŸŸ({len(cats1)}ç±») â†’ A: {a_before} -> {len(fuzzy_pool_a)}")
                    a_before = len(fuzzy_pool_a)

            if auto_scope_cat3 and 'ä¸‰çº§åˆ†ç±»' in fuzzy_pool_b.columns and 'ä¸‰çº§åˆ†ç±»' in fuzzy_pool_a.columns:
                cats3 = sorted(set(str(x) for x in fuzzy_pool_b['ä¸‰çº§åˆ†ç±»'].dropna().unique()))
                if 0 < len(cats3) <= max_cat3:
                    fuzzy_pool_a = fuzzy_pool_a[fuzzy_pool_a['ä¸‰çº§åˆ†ç±»'].astype(str).isin(cats3)]
                    scope_msgs.append(f"æŒ‰Bçš„ä¸‰çº§åˆ†ç±»é™åŸŸ({len(cats3)}ç±») â†’ A: {a_before} -> {len(fuzzy_pool_a)}")

            for m in scope_msgs:
                logging.info(f"ã€è‡ªåŠ¨é™åŸŸã€‘{m}")
            if not scope_msgs:
                logging.info("ã€è‡ªåŠ¨é™åŸŸã€‘æœªç”Ÿæ•ˆï¼ˆBåˆ†ç±»æ•°é‡è¶…è¿‡é˜ˆå€¼æˆ–æœªå¯ç”¨ï¼‰")
        except Exception as _:
            logging.info("ã€è‡ªåŠ¨é™åŸŸã€‘æ‰§è¡Œå‡ºé”™ï¼Œå·²å¿½ç•¥")
        # é¢å¤–æç¤ºï¼šå¯èƒ½çš„è€—æ—¶ä¸æ¨¡å¼
        try:
            use_simple = SIMPLE_FALLBACK or (len(fuzzy_pool_a) == 0 or len(fuzzy_pool_b) == 0 or (hasattr(fuzzy_pool_a['vector'].iloc[0], 'shape') and fuzzy_pool_a['vector'].iloc[0].shape == (1,)))
        except Exception:
            use_simple = SIMPLE_FALLBACK
        k_hard = int(os.environ.get('MATCH_TOPK_HARD', '20'))
        k_soft = int(os.environ.get('MATCH_TOPK_SOFT', '100'))
        gpu_sim = (os.environ.get('USE_TORCH_SIM','0')=='1' and torch.cuda.is_available())
        mode_text = 'ç®€åŒ–å…œåº•(æ— å‘é‡/æ— CE)' if use_simple else f"å‘é‡+å¯é€‰CEç²¾æ’{' + GPUç›¸ä¼¼åº¦' if gpu_sim else ''}"
        print(f"â„¹ï¸ åŒ¹é…æ¨¡å¼: {mode_text}ï¼ŒTop-K: ç¡¬{k_hard}/è½¯{k_soft}ï¼›æ ·æœ¬è§„æ¨¡ A={len(fuzzy_pool_a)} / B={len(fuzzy_pool_b)}")
        # æé†’ä»»ä½•è¿‡æ»¤æˆ–é‡‡æ ·é…ç½®
        if os.environ.get('COMPARE_CAT1_LIST') or os.environ.get('COMPARE_CAT1_REGEX'):
            print("ğŸ” å·²æŒ‰ä¸€çº§åˆ†ç±»è¿›è¡Œé¢„è¿‡æ»¤ (COMPARE_CAT1_LIST/COMPARE_CAT1_REGEX)")
        if os.environ.get('COMPARE_MAX_A') or os.environ.get('COMPARE_MAX_B'):
            print(f"ğŸ§ª é‡‡æ ·é™åˆ¶: A={os.environ.get('COMPARE_MAX_A') or 'ä¸é™'} / B={os.environ.get('COMPARE_MAX_B') or 'ä¸é™'}")
        if len(fuzzy_pool_a) * len(fuzzy_pool_b) > 200000:
            print("â±ï¸ æ•°æ®é‡è¾ƒå¤§ï¼ŒåŒ¹é…å¯èƒ½éœ€è¦å‡ åˆ†é’Ÿï¼Œè¯·è€å¿ƒç­‰å¾…...ï¼ˆæœŸé—´ä¼šæœ‰è¿›åº¦æ¡ï¼‰")


        # --- é˜¶æ®µ2: ç¡¬åˆ†ç±»ä¼˜å…ˆåŒ¹é… (é’ˆå¯¹å®Œæ•´çš„æ¨¡ç³ŠåŒ¹é…æ± ) ---
        logging.info(f"ã€é˜¶æ®µ2/3ã€‘æ­£åœ¨å¯¹æ‰€æœ‰æœªåŒ¹é…å•†å“è¿›è¡Œâ€œç¡¬åˆ†ç±»ä¼˜å…ˆâ€åŒ¹é…...")
        # ğŸ”§ ä½¿ç”¨ç®€çŸ­åç¼€ A/B æ›¿ä»£åº—é“ºåï¼Œç¡®ä¿ABABæ’åˆ—ç”Ÿæ•ˆ
        hard_matches_df, unmatched_a_df, unmatched_b_df = perform_hard_category_matching(
            fuzzy_pool_a, fuzzy_pool_b, "A", "B", cross_encoder, cfg
        )
        logging.info(f"âœ… ç¡¬åˆ†ç±»åŒ¹é…æ‰¾åˆ° {len(hard_matches_df)} ä¸ªåŒ¹é…ã€‚")
        logging.info(f"   - å‰©ä½™Aåº—å•†å“: {len(unmatched_a_df)}, Båº—å•†å“: {len(unmatched_b_df)} è¿›å…¥ä¸‹ä¸€é˜¶æ®µã€‚")

        # --- é˜¶æ®µ3: è½¯åˆ†ç±»å…œåº•åŒ¹é… (é’ˆå¯¹å‰©ä½™å•†å“) ---
        logging.info(f"ã€é˜¶æ®µ3/3ã€‘æ­£åœ¨å¯¹å‰©ä½™å•†å“è¿›è¡Œâ€œè½¯åˆ†ç±»å…œåº•â€åŒ¹é…...")
        # ğŸ”§ ä½¿ç”¨ç®€çŸ­åç¼€ A/B æ›¿ä»£åº—é“ºåï¼Œç¡®ä¿ABABæ’åˆ—ç”Ÿæ•ˆ
        soft_matches_df = perform_soft_fuzzy_matching(
            unmatched_a_df, unmatched_b_df, "A", "B", cross_encoder, cfg
        )
        logging.info(f"âœ… è½¯åˆ†ç±»å…œåº•åŒ¹é…æ‰¾åˆ° {len(soft_matches_df)} ä¸ªé¢å¤–åŒ¹é…ã€‚")

        # --- åˆå¹¶æ‰€æœ‰æ¨¡ç³ŠåŒ¹é…ç»“æœ ---
        fuzzy_matches_df = pd.concat([hard_matches_df, soft_matches_df], ignore_index=True)
        
        # ğŸ”§ ã€æ–°å¢ã€‘è·¨é˜¶æ®µå»é‡ï¼šç¡®ä¿åŒä¸€ä¸ªç«å¯¹å•†å“ä¸è¢«ç¡¬åŒ¹é…å’Œè½¯åŒ¹é…é‡å¤
        if not fuzzy_matches_df.empty:
            # æ‰¾åˆ°ç«å¯¹å•†å“åç§°åˆ—ï¼ˆåŒ…å«"_B"çš„åˆ—ï¼‰
            b_cols = [col for col in fuzzy_matches_df.columns if 'å•†å“åç§°' in col and '_B' in col]
            if b_cols:
                b_name_col = b_cols[0]
                before_count = len(fuzzy_matches_df)
                fuzzy_matches_df = fuzzy_matches_df.sort_values('composite_similarity_score', ascending=False)
                fuzzy_matches_df = fuzzy_matches_df.drop_duplicates(subset=[b_name_col], keep='first')
                removed = before_count - len(fuzzy_matches_df)
                if removed > 0:
                    print(f"   ğŸ”§ è·¨é˜¶æ®µå»é‡: ç§»é™¤ {removed} ä¸ªç¡¬åŒ¹é…+è½¯åŒ¹é…çš„é‡å¤å•†å“ï¼ˆä¿ç•™å¾—åˆ†æœ€é«˜ï¼‰")
        
        print(f"âœ… åç§°æ¨¡ç³ŠåŒ¹é…æ€»å…±æ‰¾åˆ° {len(fuzzy_matches_df)} ä¸ªåŒ¹é… (ç¡¬åˆ†ç±»: {len(hard_matches_df)}, è½¯å…œåº•: {len(soft_matches_df)})")
        print("âœ… [æ­¥éª¤ 5/7] å•†å“åŒ¹é…å®Œæˆï¼")
    except Exception as e:
        print(f"[é”™è¯¯] å•†å“åŒ¹é…å¤±è´¥: {e}")
        sys.exit(1)

    print("\n" + "="*50)
    print("â³ [æ­¥éª¤ 6/7] æ­£åœ¨ç”Ÿæˆæœ€ç»ˆæŠ¥å‘Š...")
    try:
        df_all_a = pd.concat([df_a_barcode, df_a_no_barcode], ignore_index=True)
        df_all_b = pd.concat([df_b_barcode, df_b_no_barcode], ignore_index=True)
        (df_a_unique, df_b_unique, df_sales_comp, df_discount_filter,
         df_a_unique_dedup, df_b_unique_dedup, df_differential, df_category_gaps, cost_sheets) = generate_final_reports(
            df_all_a, df_all_b, barcode_matches_df, fuzzy_matches_df, "A", "B", cfg
        )
        
    # æŒ‰éœ€æ±‚å˜æ›´ï¼šä¸å†ç»Ÿè®¡/å¯¼å‡ºâ€œæœ‰æ¡ç ä½†æœªåŒ¹é…â€ä¿¡æ¯
        
        print("âœ… [æ­¥éª¤ 6/7] æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ï¼")
    except Exception as e:
        import traceback
        print(f"[é”™è¯¯] ç”ŸæˆæŠ¥å‘Šå¤±è´¥: {e}")
        print("\nå®Œæ•´é”™è¯¯å †æ ˆ:")
        traceback.print_exc()
        sys.exit(1)

    print("\n" + "="*50)
    
    # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶åï¼Œé¿å…æ–‡ä»¶è¢«å ç”¨ï¼›ç»Ÿä¸€å¯¼å‡ºåˆ° reports/ ç›®å½•
    import datetime
    timestamp = datetime.datetime.now().strftime("%Y%m%d_%H%M%S")
    output_file_name = f'matched_products_comparison_final_{timestamp}.xlsx'
    # æ„é€ è¾“å‡ºç›®å½•å¹¶ç¡®ä¿å­˜åœ¨
    script_dir = os.path.dirname(os.path.abspath(__file__))
    out_dir = os.path.join(script_dir, getattr(cfg, 'OUTPUT_DIR', 'reports'))
    os.makedirs(out_dir, exist_ok=True)
    output_path = os.path.join(out_dir, output_file_name)

    print(f"â³ [æ­¥éª¤ 7/7] æ­£åœ¨å°†æ‰€æœ‰ç»“æœå¯¼å‡ºåˆ° Excel æ–‡ä»¶: {output_path}...")
    
    try:
        
        # æ£€æŸ¥å¹¶åˆ é™¤å¯èƒ½å­˜åœ¨çš„åŒåæ–‡ä»¶
        if os.path.exists(output_path):
            try:
                os.remove(output_path)
                import time
                time.sleep(0.5)  # çŸ­æš‚ç­‰å¾…ç¡®ä¿æ–‡ä»¶è¢«é‡Šæ”¾
            except Exception as e:
                print(f"âš ï¸ è­¦å‘Šï¼šæ— æ³•åˆ é™¤ç°æœ‰æ–‡ä»¶ {output_path}: {e}")
        
        with pd.ExcelWriter(output_path, engine='openpyxl') as writer:
            # === æ ¸å¿ƒåŒ¹é…ç»“æœï¼ˆæ¡ç åŒ¹é…å’Œåç§°åŒ¹é…ä¸¥æ ¼åˆ†ç¦»ï¼‰===
            export_to_excel(writer, barcode_matches_df, '1-æ¡ç ç²¾ç¡®åŒ¹é…', cfg)
            
            # ğŸ“‹ 2-åç§°æ¨¡ç³ŠåŒ¹é…ï¼šå¯¹é½æ‰‹åŠ¨æ¯”ä»·ï¼Œç©ºè¡¨ä¹Ÿè¦ç”Ÿæˆä¸€ä¸ªç©ºç™½Sheetï¼Œä¾¿äºç»“æ„ä¸€è‡´
            print(f"ğŸ“‹ 2-åç§°æ¨¡ç³ŠåŒ¹é…(æ— æ¡ç ): åŒ¹é…æ¡æ•° {len(fuzzy_matches_df)}ï¼ˆç©ºè¡¨ä¹Ÿä¼šå¯¼å‡ºï¼‰")
            export_to_excel(writer, fuzzy_matches_df, '2-åç§°æ¨¡ç³ŠåŒ¹é…(æ— æ¡ç )', cfg)
            
            # === æ–°å¢ï¼šå·®å¼‚å“å¯¹æ¯” ===
            if not df_differential.empty:
                print(f"ğŸ“Š å·®å¼‚å“å¯¹æ¯”: {len(df_differential)} å¯¹å·®å¼‚å“åŒ¹é…")
                export_to_excel(writer, df_differential, '3-å·®å¼‚å“å¯¹æ¯”', cfg)
            
            # === ç‹¬æœ‰å•†å“ï¼ˆåŸå§‹+å»é‡ç‰ˆæœ¬ï¼‰ ===
            export_to_excel(writer, df_a_unique, f'4-{cfg.STORE_A_NAME}-ç‹¬æœ‰å•†å“(å…¨éƒ¨)', cfg)
            export_to_excel(writer, df_b_unique, f'5-{cfg.STORE_B_NAME}-ç‹¬æœ‰å•†å“(å…¨éƒ¨)', cfg)
            # æŒ‰éœ€æ±‚å˜æ›´ï¼šä¸å†å¯¼å‡ºâ€œ6-é”€é‡å¯¹æ¯”(Båº—ç•…é”€ä¸”æˆ‘åº—æœ‰ä¼˜åŠ¿)â€
            # æ–°å¢ï¼šAæŠ˜æ‰£>=BæŠ˜æ‰£ä¸”åŒæ–¹åº“å­˜>0ã€Bæœˆå”®>0ï¼ˆç®€åŒ–å‘½åï¼‰
            export_to_excel(writer, df_discount_filter, '9-åº“å­˜>0&AæŠ˜æ‰£â‰¥BæŠ˜æ‰£', cfg)
            
            # å¯¼å‡ºå»é‡åçš„ç‹¬æœ‰å•†å“
            if not df_a_unique_dedup.empty:
                print(f"  [å»é‡A] {cfg.STORE_A_NAME}-ç‹¬æœ‰å•†å“(å»é‡): {len(df_a_unique_dedup)} ç§å•†å“")
                export_to_excel(writer, df_a_unique_dedup, f'6-{cfg.STORE_A_NAME}-ç‹¬æœ‰å•†å“(å»é‡)', cfg)
            if not df_b_unique_dedup.empty:
                print(f"  [å»é‡B] {cfg.STORE_B_NAME}-ç‹¬æœ‰å•†å“(å»é‡): {len(df_b_unique_dedup)} ç§å•†å“")
                export_to_excel(writer, df_b_unique_dedup, f'7-{cfg.STORE_B_NAME}-ç‹¬æœ‰å•†å“(å»é‡)', cfg)
            
            # å¯¼å‡ºå“ç±»ç¼ºå£åˆ†æ
            if not df_category_gaps.empty:
                print(f"  [ç¼ºå£] å“ç±»ç¼ºå£åˆ†æ: {len(df_category_gaps)} ä¸ªç¼ºå¤±å“ç±»")
                export_to_excel(writer, df_category_gaps, '8-å“ç±»ç¼ºå£åˆ†æ', cfg)
            
            # ğŸ†• å¯¼å‡ºæˆæœ¬åˆ†æ Sheetï¼ˆç¬¬ä¸€é˜¶æ®µåŠŸèƒ½ï¼‰
            print(f"\nğŸ” æˆæœ¬åˆ†æå¯¼å‡ºæ£€æŸ¥:")
            print(f"   ENABLE_COST_PREDICTION = {cfg.ENABLE_COST_PREDICTION}")
            print(f"   EXPORT_COST_SHEETS = {cfg.EXPORT_COST_SHEETS}")
            print(f"   cost_sheets æ˜¯å¦ä¸ºç©º = {not cost_sheets}")
            print(f"   cost_sheets é”® = {list(cost_sheets.keys()) if cost_sheets else 'æ— '}")
            
            if cfg.ENABLE_COST_PREDICTION and cfg.EXPORT_COST_SHEETS and cost_sheets:
                print(f"\nğŸ’° æ­£åœ¨å¯¼å‡ºæˆæœ¬åˆ†ææŠ¥è¡¨...")
                sheet_num = 10  # ä»10å·å¼€å§‹ç¼–å·ï¼Œé¿å…ä¸ç°æœ‰Sheetå†²çª
                for sheet_name, sheet_df in cost_sheets.items():
                    if not sheet_df.empty:
                        print(f"  [æˆæœ¬] {sheet_name}: {len(sheet_df)} æ¡è®°å½•")
                        export_to_excel(writer, sheet_df, f'{sheet_num}-{sheet_name}')
                        sheet_num += 1
                print(f"âœ… æˆæœ¬åˆ†ææŠ¥è¡¨å·²å¯¼å‡ºï¼ˆå…± {len(cost_sheets)} ä¸ªSheetï¼‰")
            else:
                print(f"   âš ï¸  æˆæœ¬åˆ†ææœªå¯¼å‡ºï¼ŒåŸå› :")
                if not cfg.ENABLE_COST_PREDICTION:
                    print(f"      - ENABLE_COST_PREDICTION = False")
                if not cfg.EXPORT_COST_SHEETS:
                    print(f"      - EXPORT_COST_SHEETS = False")
                if not cost_sheets:
                    print(f"      - cost_sheets ä¸ºç©ºï¼ˆå¯èƒ½æˆæœ¬é¢„æµ‹å¤±è´¥æˆ–æ— æ•°æ®ï¼‰")
            
            # æ¸…æ´—æ•°æ®å¯¼å‡ºï¼šå¯é…ç½®å¼€å…³
            if getattr(cfg, 'EXPORT_CLEANED_SHEETS', True):
                print(f"âœ… æ­£åœ¨å¯¼å‡ºæ¸…æ´—åçš„æ•°æ®...")

                # åˆå¹¶Aåº—å’ŒBåº—çš„æ‰€æœ‰æ•°æ®ï¼ˆåŒ…æ‹¬æœ‰æ¡ç å’Œæ— æ¡ç çš„ï¼‰
                df_a_all = pd.concat([df_a_barcode, df_a_no_barcode], ignore_index=True) if not df_a_no_barcode.empty else df_a_barcode
                df_b_all = pd.concat([df_b_barcode, df_b_no_barcode], ignore_index=True) if not df_b_no_barcode.empty else df_b_barcode

                # æå–æ¸…æ´—åçš„åˆ—ï¼ˆåŒ…å«æ‰€æœ‰å¤„ç†è¿‡çš„å­—æ®µå’Œåˆ†ç±»å¯¹æ¯”ï¼‰
                cleaned_cols = [
                    'å•†å“åç§°', 'cleaned_å•†å“åç§°',
                    'ç¾å›¢ä¸€çº§åˆ†ç±»', 'ä¸€çº§åˆ†ç±»', 'cleaned_ä¸€çº§åˆ†ç±»',
                    'ç¾å›¢ä¸‰çº§åˆ†ç±»', 'ä¸‰çº§åˆ†ç±»', 'cleaned_ä¸‰çº§åˆ†ç±»',
                    'standardized_brand', 'specs',
                    'å•†å®¶åˆ†ç±»', 'æ¡ç ', 'åº—å†…ç ', 'åŸä»·', 'å”®ä»·', 'æœˆå”®', 'åº“å­˜'
                ]

                # Aåº—æ¸…æ´—æ•°æ®
                cleaned_cols_a = [col for col in cleaned_cols if col in df_a_all.columns]
                if len(cleaned_cols_a) > 0:
                    df_a_cleaned = df_a_all[cleaned_cols_a].copy()
                    df_a_cleaned['æ•°æ®æº'] = cfg.STORE_A_NAME
                    export_to_excel(writer, df_a_cleaned, f'6-{cfg.STORE_A_NAME}-æ¸…æ´—æ•°æ®')

                # Båº—æ¸…æ´—æ•°æ®
                cleaned_cols_b = [col for col in cleaned_cols if col in df_b_all.columns]
                if len(cleaned_cols_b) > 0:
                    df_b_cleaned = df_b_all[cleaned_cols_b].copy()
                    df_b_cleaned['æ•°æ®æº'] = cfg.STORE_B_NAME
                    export_to_excel(writer, df_b_cleaned, f'7-{cfg.STORE_B_NAME}-æ¸…æ´—æ•°æ®')

                # åˆå¹¶æ¸…æ´—æ•°æ®å¯¹æ¯”ï¼ˆåªåŒ…å«ä¸¤åº—éƒ½æœ‰çš„åˆ—ï¼‰
                common_cleaned_cols = list(set(cleaned_cols_a) & set(cleaned_cols_b))
                if len(common_cleaned_cols) > 0:
                    try:
                        df_combined_cleaned = pd.concat([
                            df_a_all[common_cleaned_cols].assign(æ•°æ®æº=cfg.STORE_A_NAME),
                            df_b_all[common_cleaned_cols].assign(æ•°æ®æº=cfg.STORE_B_NAME)
                        ], ignore_index=True)
                        export_to_excel(writer, df_combined_cleaned, '8-åˆå¹¶æ¸…æ´—æ•°æ®å¯¹æ¯”')
                        print(f"âœ… æ¸…æ´—æ•°æ®å·²å¯¼å‡ºåˆ°ç‹¬ç«‹Sheetä¸­ï¼Œä¾¿äºæŸ¥é˜…å¯¹æ¯”")
                    except Exception as e:
                        print(f"âš ï¸ åˆå¹¶æ¸…æ´—æ•°æ®æ—¶å‡ºé”™: {e}")
            else:
                print("â„¹ï¸ å·²æ ¹æ®é…ç½®å…³é—­æ¸…æ´—æ•°æ® Sheet çš„å¯¼å‡ºï¼ˆ6/7/8 å·è¡¨ï¼‰ã€‚")
        
        print(f"âœ… [æ­¥éª¤ 7/7] Excel æ–‡ä»¶å¯¼å‡ºæˆåŠŸï¼å·²ä¿å­˜è‡³: {output_path}")
    except Exception as e:
        print(f"[é”™è¯¯] Excelå¯¼å‡ºå¤±è´¥: {e}")
        sys.exit(1)

    # ğŸš€ ä¿å­˜æ‰€æœ‰ç¼“å­˜å¹¶æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    print("\n" + "="*50)
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜ç¼“å­˜...")
    print("="*50)
    cache_manager.save_all()
    cache_manager.print_stats()

    print("\n" + "="*50)
    print(f"ğŸ‰ å…¨éƒ¨æµç¨‹å®Œæˆï¼")
    print("="*50)

if __name__ == '__main__':
    # æˆæƒæ£€æŸ¥ï¼ˆä»…åœ¨æ‰“åŒ…ç¯å¢ƒä¸‹æ‰§è¡Œï¼‰
    if not check_authorization():
        sys.exit(1)
    
    main()

