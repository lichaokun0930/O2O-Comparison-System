# O2O æ¯”ä»·ç¨‹åºæ€§èƒ½ä¼˜åŒ–å¼€å‘è®¡åˆ’

**æ–‡æ¡£ç‰ˆæœ¬**: v1.0  
**åˆ›å»ºæ—¥æœŸ**: 2025å¹´11æœˆ6æ—¥  
**é¡¹ç›®å‘¨æœŸ**: 4å‘¨ï¼ˆåˆ†é˜¶æ®µå®æ–½ï¼‰  
**ç›®æ ‡**: åœ¨ä¸æŸå¤±å‡†ç¡®ç‡çš„å‰æä¸‹ï¼Œæå‡æ€§èƒ½3-5å€ï¼Œä¼˜åŒ–ç”¨æˆ·ä½“éªŒ

---

## ğŸ“Š æ€»ä½“ç›®æ ‡

| æŒ‡æ ‡ | å½“å‰çŠ¶æ€ | ç›®æ ‡çŠ¶æ€ | æå‡å¹…åº¦ |
|------|---------|---------|---------|
| **å¤„ç†é€Ÿåº¦** | 25åˆ†é’Ÿï¼ˆ8K+10K SKUï¼‰ | 8åˆ†é’Ÿ | **3å€** |
| **åŒ¹é…å‡†ç¡®ç‡** | 92% | 94% | **+2%** |
| **å†…å­˜å ç”¨** | 8GBå³°å€¼ | 4GBå³°å€¼ | **-50%** |
| **å´©æºƒç‡** | å¶å‘å´©æºƒ | 0å´©æºƒ | **æ–­ç‚¹ç»­è·‘** |
| **ç”¨æˆ·ä½“éªŒ** | å‚æ•°éš¾è°ƒ | æ™ºèƒ½æ¨è | **é™ä½é—¨æ§›** |

---

## ğŸ¯ é˜¶æ®µåˆ’åˆ†ï¼ˆå››é˜¶æ®µå®æ–½ï¼‰

### **é˜¶æ®µ1ï¼šå¿«é€Ÿæ”¶ç›Šä¼˜åŒ–**ï¼ˆç¬¬1å‘¨ï¼‰
- **ç›®æ ‡**: é›¶é£é™©å¿«é€Ÿè§æ•ˆï¼Œé€Ÿåº¦æå‡30%
- **å·¥ä½œé‡**: 3äººå¤©
- **é£é™©**: â­ æä½

### **é˜¶æ®µ2ï¼šä½“éªŒä¼˜åŒ–**ï¼ˆç¬¬2å‘¨ï¼‰
- **ç›®æ ‡**: æå‡å‡†ç¡®ç‡5%ï¼Œä¼˜åŒ–äº¤äº’
- **å·¥ä½œé‡**: 5äººå¤©
- **é£é™©**: â­â­ ä½

### **é˜¶æ®µ3ï¼šæ¶æ„ä¼˜åŒ–**ï¼ˆç¬¬3å‘¨ï¼‰
- **ç›®æ ‡**: é€Ÿåº¦æå‡200%ï¼Œå†…å­˜ä¼˜åŒ–50%
- **å·¥ä½œé‡**: 10äººå¤©
- **é£é™©**: â­â­â­ ä¸­

### **é˜¶æ®µ4ï¼šé«˜çº§ç‰¹æ€§**ï¼ˆç¬¬4å‘¨ï¼‰
- **ç›®æ ‡**: æ–­ç‚¹ç»­è·‘ã€å¯è§†åŒ–åˆ†æ
- **å·¥ä½œé‡**: 8äººå¤©
- **é£é™©**: â­â­â­ ä¸­

---

## ğŸ“… **é˜¶æ®µ1ï¼šå¿«é€Ÿæ”¶ç›Šä¼˜åŒ–**ï¼ˆç¬¬1å‘¨ï¼Œç«‹å³å®æ–½ï¼‰

### **ä¼˜åŒ–é¡¹1.1ï¼šæ­£åˆ™é¢„ç¼–è¯‘** â­â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- æ–‡æœ¬æ¸…æ´—é€Ÿåº¦æå‡ **3å€**
- å¤„ç†1ä¸‡å•†å“èŠ‚çœ **2-3åˆ†é’Ÿ**
- é›¶é£é™©ï¼Œç«‹å³è§æ•ˆ

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
# å½“å‰é—®é¢˜ï¼ˆline 372+ï¼‰
def clean_text(text):
    text = re.sub(r'[\[\]ã€ã€‘()]', '', text)  # æ¯æ¬¡éƒ½ç¼–è¯‘æ­£åˆ™
    text = re.sub(r'\s+', ' ', text)
    return text

# ä¼˜åŒ–æ–¹æ¡ˆï¼šå…¨å±€é¢„ç¼–è¯‘
import re
from functools import lru_cache

# å…¨å±€æ­£åˆ™å¯¹è±¡ï¼ˆæ¨¡å—é¡¶éƒ¨å®šä¹‰ï¼‰
REGEX_PATTERNS = {
    'brackets': re.compile(r'[\[\]ã€ã€‘()ï¼ˆï¼‰]'),
    'units': re.compile(r'(\d+(?:\.\d+)?)\s*(ml|ML|å…‹|g|G|æ–¤|kg|KG|æ¯«å‡|å‡|L)'),
    'spaces': re.compile(r'\s+'),
    'special': re.compile(r'[^\w\s\u4e00-\u9fff]'),
}

@lru_cache(maxsize=20000)  # ç¼“å­˜2ä¸‡ä¸ªæœ€å¸¸ç”¨æ–‡æœ¬
def clean_text_optimized(text):
    """ä¼˜åŒ–ç‰ˆæ–‡æœ¬æ¸…æ´—ï¼ˆé¢„ç¼–è¯‘æ­£åˆ™+LRUç¼“å­˜ï¼‰"""
    if pd.isna(text):
        return ""
    text = str(text).strip()
    text = REGEX_PATTERNS['brackets'].sub('', text)
    text = REGEX_PATTERNS['units'].sub(r'\1\2', text)
    text = REGEX_PATTERNS['spaces'].sub(' ', text)
    return text.strip()
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬1å¤©ä¸Šåˆ**ï¼šåœ¨ `product_comparison_tool_local.py` é¡¶éƒ¨ï¼ˆLine 50å·¦å³ï¼‰æ·»åŠ æ­£åˆ™é¢„ç¼–è¯‘å­—å…¸
2. **ç¬¬1å¤©ä¸‹åˆ**ï¼šä¿®æ”¹ `clean_text()` å‡½æ•°ï¼Œæ·»åŠ  `@lru_cache` è£…é¥°å™¨
3. **ç¬¬1å¤©**ï¼šå›å½’æµ‹è¯•ï¼Œå¯¹æ¯”ä¼˜åŒ–å‰åç»“æœä¸€è‡´æ€§
4. **ç¬¬2å¤©**ï¼šæ€§èƒ½åŸºå‡†æµ‹è¯•ï¼ˆè®°å½•æé€Ÿæ¯”ä¾‹ï¼‰

#### éªŒæ”¶æ ‡å‡†
- âœ… æ–‡æœ¬æ¸…æ´—ç»“æœä¸åŸç‰ˆ100%ä¸€è‡´
- âœ… æ¸…æ´—é€Ÿåº¦æå‡â‰¥2.5å€
- âœ… æ— æ–°å¢BUG

#### é£é™©è¯„ä¼°
- **é£é™©ç­‰çº§**: â­ æ— é£é™©
- **å›æ»šæ–¹æ¡ˆ**: åˆ é™¤ç¼“å­˜è£…é¥°å™¨ï¼Œæ¢å¤åŸå‡½æ•°
- **é¢„è®¡BUG**: 0ä¸ª

---

### **ä¼˜åŒ–é¡¹1.2ï¼šCSVç¼“å­˜åŠ é€Ÿ** â­â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- Excelè¯»å–é€Ÿåº¦æå‡ **10å€**ï¼ˆ30ç§’ â†’ 3ç§’ï¼‰
- é‡å¤è¿è¡Œæ—¶èŠ‚çœ **80%** æ•°æ®åŠ è½½æ—¶é—´
- ç”¨æˆ·ä½“éªŒæ˜¾è‘—æå‡

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
# æ–°å¢å‡½æ•°ï¼šæ™ºèƒ½æ•°æ®è¯»å–
def smart_load_excel(file_path: str, force_reload: bool = False) -> pd.DataFrame:
    """
    æ™ºèƒ½åŠ è½½Excelï¼ˆä¼˜å…ˆä½¿ç”¨CSVç¼“å­˜ï¼‰
    
    é€»è¾‘ï¼š
    1. æ£€æŸ¥æ˜¯å¦å­˜åœ¨.cache.csvæ–‡ä»¶
    2. å¯¹æ¯”ä¿®æ”¹æ—¶é—´ï¼ŒExcelæ›´æ–°åˆ™é‡æ–°è¯»å–
    3. å¦åˆ™ç›´æ¥è¯»å–CSVï¼ˆé€Ÿåº¦å¿«10å€ï¼‰
    4. è‡ªåŠ¨ç”ŸæˆCSVç¼“å­˜ä¾›ä¸‹æ¬¡ä½¿ç”¨
    """
    from pathlib import Path
    import time
    
    excel_path = Path(file_path)
    csv_cache = excel_path.parent / 'cache' / f"{excel_path.stem}.cache.csv"
    csv_cache.parent.mkdir(exist_ok=True)
    
    # å¼ºåˆ¶é‡è½½æˆ–ç¼“å­˜ä¸å­˜åœ¨
    if force_reload or not csv_cache.exists():
        print(f"ğŸ“– è¯»å–Excel: {excel_path.name}")
        df = pd.read_excel(file_path, engine='openpyxl')
        # ä¿å­˜ç¼“å­˜
        df.to_csv(csv_cache, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ å·²ç”Ÿæˆç¼“å­˜: {csv_cache.name}")
        return df
    
    # æ£€æŸ¥Excelæ˜¯å¦æ›´æ–°
    excel_mtime = excel_path.stat().st_mtime
    cache_mtime = csv_cache.stat().st_mtime
    
    if excel_mtime > cache_mtime:
        print(f"âš ï¸ æ£€æµ‹åˆ°Excelå·²æ›´æ–°ï¼Œé‡æ–°è¯»å–...")
        df = pd.read_excel(file_path, engine='openpyxl')
        df.to_csv(csv_cache, index=False, encoding='utf-8-sig')
        print(f"ğŸ’¾ ç¼“å­˜å·²æ›´æ–°")
        return df
    
    # ä½¿ç”¨ç¼“å­˜
    print(f"âš¡ ä»ç¼“å­˜åŠ è½½: {csv_cache.name}ï¼ˆæé€Ÿ10å€ï¼‰")
    return pd.read_csv(csv_cache, encoding='utf-8-sig')
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬2å¤©ä¸Šåˆ**ï¼šæ·»åŠ  `smart_load_excel()` å‡½æ•°åˆ°å·¥å…·å‡½æ•°åŒºï¼ˆLine 400å·¦å³ï¼‰
2. **ç¬¬2å¤©ä¸‹åˆ**ï¼šæ›¿æ¢æ‰€æœ‰ `pd.read_excel()` è°ƒç”¨ä¸º `smart_load_excel()`
3. **ç¬¬3å¤©**ï¼šæµ‹è¯•ç¼“å­˜æ›´æ–°é€»è¾‘ï¼ˆä¿®æ”¹ExcelåéªŒè¯è‡ªåŠ¨é‡è½½ï¼‰
4. **ç¬¬3å¤©**ï¼šæ·»åŠ æ‰‹åŠ¨æ¸…é™¤ç¼“å­˜åŠŸèƒ½ï¼ˆConfigé…ç½®é¡¹ï¼‰

#### éªŒæ”¶æ ‡å‡†
- âœ… é¦–æ¬¡åŠ è½½æ—¶é—´ä¸åŸç‰ˆç›¸åŒ
- âœ… ç¬¬äºŒæ¬¡åŠ è½½é€Ÿåº¦æå‡â‰¥8å€
- âœ… Excelæ›´æ–°åè‡ªåŠ¨æ£€æµ‹å¹¶é‡è½½
- âœ… ç¼“å­˜ç›®å½•ç»“æ„æ¸…æ™°ï¼ˆcache/æœ¬åº—.cache.csvï¼‰

#### é£é™©è¯„ä¼°
- **é£é™©ç­‰çº§**: â­â­ ä½é£é™©
- **æ½œåœ¨é—®é¢˜**: ç¼“å­˜æœªæ›´æ–°å¯¼è‡´æ•°æ®è¿‡æœŸ
- **ç¼“è§£æªæ–½**: è‡ªåŠ¨æ£€æµ‹ä¿®æ”¹æ—¶é—´ + æ‰‹åŠ¨å¼ºåˆ¶åˆ·æ–°å¼€å…³
- **å›æ»šæ–¹æ¡ˆ**: æ¢å¤ `pd.read_excel()`

---

### **ä¼˜åŒ–é¡¹1.3ï¼šè´¨é‡è‡ªæ£€æŠ¥å‘Š** â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- è‡ªåŠ¨å‘ç° **ä½è´¨é‡åŒ¹é…**ï¼ˆå¾—åˆ†<0.5ï¼‰
- ç”Ÿæˆ **è´¨é‡è¯„çº§**ï¼ˆâ­â­â­ä¼˜ç§€/â­â­è‰¯å¥½/â­å¾…å¤æ ¸ï¼‰
- å¸®åŠ©ç”¨æˆ·å¿«é€Ÿå®šä½é—®é¢˜

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
def generate_quality_report(matched_df: pd.DataFrame) -> dict:
    """
    ç”ŸæˆåŒ¹é…è´¨é‡æŠ¥å‘Š
    
    è¿”å›ï¼š
    - high_confidence: é«˜ç½®ä¿¡åº¦åŒ¹é…ï¼ˆâ‰¥0.8ï¼‰
    - medium_confidence: ä¸­ç­‰ç½®ä¿¡åº¦ï¼ˆ0.5-0.8ï¼‰
    - low_confidence: ä½ç½®ä¿¡åº¦ï¼ˆ<0.5ï¼Œéœ€äººå·¥å¤æ ¸ï¼‰
    - quality_stats: ç»Ÿè®¡ä¿¡æ¯
    """
    if matched_df.empty or 'composite_similarity_score' not in matched_df.columns:
        return {'error': 'æ— åŒ¹é…æ•°æ®'}
    
    score_col = 'composite_similarity_score'
    
    # åˆ†çº§ç»Ÿè®¡
    high = matched_df[matched_df[score_col] >= 0.8]
    medium = matched_df[(matched_df[score_col] >= 0.5) & (matched_df[score_col] < 0.8)]
    low = matched_df[matched_df[score_col] < 0.5]
    
    total = len(matched_df)
    report = {
        'total_matches': total,
        'high_confidence': {
            'count': len(high),
            'percentage': len(high) / total * 100 if total > 0 else 0,
            'label': 'â­â­â­ä¼˜ç§€'
        },
        'medium_confidence': {
            'count': len(medium),
            'percentage': len(medium) / total * 100 if total > 0 else 0,
            'label': 'â­â­è‰¯å¥½'
        },
        'low_confidence': {
            'count': len(low),
            'percentage': len(low) / total * 100 if total > 0 else 0,
            'label': 'â­å¾…å¤æ ¸'
        },
        'average_score': matched_df[score_col].mean(),
        'median_score': matched_df[score_col].median(),
        'low_quality_samples': low.head(5).to_dict('records') if len(low) > 0 else []
    }
    
    return report

def print_quality_report(report: dict):
    """æ‰“å°è´¨é‡æŠ¥å‘Šåˆ°æ§åˆ¶å°"""
    print("\n" + "="*70)
    print("ğŸ“Š åŒ¹é…è´¨é‡æŠ¥å‘Š")
    print("="*70)
    print(f"æ€»åŒ¹é…æ•°: {report['total_matches']}")
    print(f"\nè´¨é‡åˆ†å¸ƒ:")
    print(f"  âœ… {report['high_confidence']['label']}: {report['high_confidence']['count']} å¯¹ ({report['high_confidence']['percentage']:.1f}%)")
    print(f"  âš ï¸  {report['medium_confidence']['label']}: {report['medium_confidence']['count']} å¯¹ ({report['medium_confidence']['percentage']:.1f}%)")
    print(f"  âŒ {report['low_confidence']['label']}: {report['low_confidence']['count']} å¯¹ ({report['low_confidence']['percentage']:.1f}%) - å»ºè®®äººå·¥å¤æ ¸")
    print(f"\nå¹³å‡å¾—åˆ†: {report['average_score']:.3f}")
    print(f"ä¸­ä½æ•°å¾—åˆ†: {report['median_score']:.3f}")
    
    if report['low_quality_samples']:
        print(f"\nğŸ” ä½è´¨é‡åŒ¹é…æ¡ˆä¾‹ï¼ˆå‰5ä¸ªï¼‰:")
        for i, sample in enumerate(report['low_quality_samples'], 1):
            name_a = sample.get('å•†å“åç§°_æœ¬åº—', sample.get('å•†å“åç§°_A', 'N/A'))
            name_b = sample.get('å•†å“åç§°_ç«å¯¹', sample.get('å•†å“åç§°_B', 'N/A'))
            score = sample.get('composite_similarity_score', 0)
            print(f"  {i}. {name_a[:30]}")
            print(f"     â†” {name_b[:30]}")
            print(f"     å¾—åˆ†: {score:.3f}")
    print("="*70)

# åœ¨åŒ¹é…ç»“æœä¸­æ·»åŠ è´¨é‡è¯„çº§åˆ—
def add_quality_rating(matched_df: pd.DataFrame) -> pd.DataFrame:
    """ä¸ºåŒ¹é…ç»“æœæ·»åŠ è´¨é‡è¯„çº§åˆ—"""
    if 'composite_similarity_score' not in matched_df.columns:
        return matched_df
    
    def rate_quality(score):
        if score >= 0.8:
            return 'â­â­â­ä¼˜ç§€'
        elif score >= 0.6:
            return 'â­â­è‰¯å¥½'
        elif score >= 0.4:
            return 'â­å¾…å¤æ ¸'
        else:
            return 'âŒéœ€äººå·¥æ ¸æŸ¥'
    
    matched_df['è´¨é‡è¯„çº§'] = matched_df['composite_similarity_score'].apply(rate_quality)
    return matched_df
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬4å¤©ä¸Šåˆ**ï¼šæ·»åŠ  `generate_quality_report()` å’Œ `print_quality_report()` å‡½æ•°
2. **ç¬¬4å¤©ä¸‹åˆ**ï¼šåœ¨ä¸»æµç¨‹ä¸­è°ƒç”¨è´¨é‡æŠ¥å‘Šç”Ÿæˆï¼ˆ`main()` å‡½æ•°æœ«å°¾ï¼‰
3. **ç¬¬5å¤©**ï¼šåœ¨Excelå¯¼å‡ºæ—¶æ·»åŠ "è´¨é‡è¯„çº§"åˆ—
4. **ç¬¬5å¤©**ï¼šæµ‹è¯•å„ç§è´¨é‡åˆ†å¸ƒåœºæ™¯

#### éªŒæ”¶æ ‡å‡†
- âœ… æ§åˆ¶å°è¾“å‡ºç¾è§‚çš„è´¨é‡æŠ¥å‘Š
- âœ… Excelæ–°å¢"è´¨é‡è¯„çº§"åˆ—
- âœ… ä½è´¨é‡æ¡ˆä¾‹å±•ç¤ºæ­£ç¡®ï¼ˆå•†å“å+å¾—åˆ†ï¼‰

#### é£é™©è¯„ä¼°
- **é£é™©ç­‰çº§**: â­ æ— é£é™©
- **é¢„è®¡BUG**: 0ä¸ª
- **å›æ»šæ–¹æ¡ˆ**: åˆ é™¤ç›¸å…³å‡½æ•°è°ƒç”¨

---

### **é˜¶æ®µ1æ€»ç»“**

#### å®Œæˆæ—¶é—´
- **é¢„è®¡**: 5ä¸ªå·¥ä½œæ—¥
- **ç¼“å†²**: +2å¤©ï¼ˆæµ‹è¯•+æ–‡æ¡£ï¼‰

#### é¢„æœŸæ”¶ç›Š
| æŒ‡æ ‡ | æå‡å¹…åº¦ |
|------|---------|
| æ–‡æœ¬æ¸…æ´—é€Ÿåº¦ | +200% |
| æ•°æ®åŠ è½½é€Ÿåº¦ | +900%ï¼ˆç¬¬äºŒæ¬¡è¿è¡Œï¼‰ |
| ç”¨æˆ·ä½“éªŒ | è´¨é‡é—®é¢˜å¯è§†åŒ– |
| **æ€»ä½“é€Ÿåº¦** | **+30%** |

#### äº¤ä»˜ç‰©
- âœ… ä¼˜åŒ–åçš„ `product_comparison_tool_local.py`ï¼ˆå‘åå…¼å®¹ï¼‰
- âœ… å›å½’æµ‹è¯•æŠ¥å‘Šï¼ˆå‡†ç¡®ç‡éªŒè¯ï¼‰
- âœ… æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Šï¼ˆé€Ÿåº¦å¯¹æ¯”ï¼‰
- âœ… ç”¨æˆ·ä½¿ç”¨æ–‡æ¡£æ›´æ–°

---

## ğŸ“… **é˜¶æ®µ2ï¼šä½“éªŒä¼˜åŒ–**ï¼ˆç¬¬2å‘¨ï¼‰

### **ä¼˜åŒ–é¡¹2.1ï¼šæ™ºèƒ½å‚æ•°æ¨è** â­â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- æ ¹æ®æ•°æ®ç‰¹å¾è‡ªåŠ¨æ¨èæœ€ä¼˜å‚æ•°
- é™ä½ç”¨æˆ·è°ƒå‚é—¨æ§›
- å‡†ç¡®ç‡æå‡ **2-5%**

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
def analyze_dataset_features(df: pd.DataFrame, store_name: str) -> dict:
    """
    åˆ†ææ•°æ®é›†ç‰¹å¾ï¼Œè¿”å›ç»Ÿè®¡ä¿¡æ¯
    
    åˆ†æç»´åº¦ï¼š
    1. å“ç‰Œä¿¡æ¯å®Œæ•´åº¦
    2. è§„æ ¼ä¿¡æ¯å®Œæ•´åº¦
    3. å•†å“åç§°é•¿åº¦åˆ†å¸ƒ
    4. åˆ†ç±»è¦†ç›–åº¦
    5. ä»·æ ¼åŒºé—´åˆ†å¸ƒ
    """
    stats = {
        'store_name': store_name,
        'total_skus': len(df),
        'brand_coverage': df['å“ç‰Œ'].notna().sum() / len(df) if 'å“ç‰Œ' in df.columns else 0,
        'spec_coverage': df['è§„æ ¼åç§°'].notna().sum() / len(df) if 'è§„æ ¼åç§°' in df.columns else 0,
        'barcode_coverage': df['æ¡ç '].notna().sum() / len(df) if 'æ¡ç ' in df.columns else 0,
        'avg_name_length': df['å•†å“åç§°'].str.len().mean() if 'å•†å“åç§°' in df.columns else 0,
        'category_count': df['ç¾å›¢ä¸€çº§åˆ†ç±»'].nunique() if 'ç¾å›¢ä¸€çº§åˆ†ç±»' in df.columns else 0,
        'price_range': {
            'min': df['å”®ä»·'].min() if 'å”®ä»·' in df.columns else 0,
            'max': df['å”®ä»·'].max() if 'å”®ä»·' in df.columns else 0,
            'median': df['å”®ä»·'].median() if 'å”®ä»·' in df.columns else 0
        }
    }
    return stats

def recommend_parameters(stats_a: dict, stats_b: dict) -> dict:
    """
    åŸºäºæ•°æ®ç‰¹å¾æ¨èæœ€ä¼˜å‚æ•°
    
    æ¨èé€»è¾‘ï¼š
    1. å“ç‰Œè¦†ç›–ç‡é«˜ â†’ æé«˜brand_weight
    2. è§„æ ¼ä¿¡æ¯ä¸°å¯Œ â†’ æé«˜specs_weight
    3. å•†å“åè¾ƒçŸ­ â†’ é™ä½text_weightï¼Œæé«˜category_weight
    4. æ¡ç è¦†ç›–ç‡ä½ â†’ é™ä½composite_thresholdï¼ˆæ”¾å®½åŒ¹é…ï¼‰
    """
    avg_brand_coverage = (stats_a['brand_coverage'] + stats_b['brand_coverage']) / 2
    avg_spec_coverage = (stats_a['spec_coverage'] + stats_b['spec_coverage']) / 2
    avg_name_length = (stats_a['avg_name_length'] + stats_b['avg_name_length']) / 2
    avg_barcode_coverage = (stats_a['barcode_coverage'] + stats_b['barcode_coverage']) / 2
    
    # åŸºç¡€å‚æ•°ï¼ˆé»˜è®¤å€¼ï¼‰
    params = {
        'text_weight': 0.5,
        'brand_weight': 0.3,
        'category_weight': 0.1,
        'specs_weight': 0.1,
        'composite_threshold': 0.2
    }
    
    recommendations = []
    
    # è§„åˆ™1ï¼šå“ç‰Œä¿¡æ¯ä¸°å¯Œ
    if avg_brand_coverage > 0.8:
        params['brand_weight'] = 0.35
        params['text_weight'] = 0.45
        recommendations.append("âœ… å“ç‰Œä¿¡æ¯ä¸°å¯Œï¼ˆ{}%ï¼‰ï¼Œæé«˜å“ç‰Œæƒé‡è‡³0.35".format(int(avg_brand_coverage*100)))
    
    # è§„åˆ™2ï¼šè§„æ ¼ä¿¡æ¯ä¸°å¯Œ
    if avg_spec_coverage > 0.7:
        params['specs_weight'] = 0.15
        params['text_weight'] = 0.4
        params['brand_weight'] = 0.3
        recommendations.append("âœ… è§„æ ¼ä¿¡æ¯ä¸°å¯Œï¼ˆ{}%ï¼‰ï¼Œæé«˜è§„æ ¼æƒé‡è‡³0.15".format(int(avg_spec_coverage*100)))
    
    # è§„åˆ™3ï¼šå•†å“åè¾ƒçŸ­ï¼ˆå“ç‰Œ+å“ç±»ç‰¹å¾æ›´é‡è¦ï¼‰
    if avg_name_length < 15:
        params['text_weight'] = 0.4
        params['category_weight'] = 0.15
        recommendations.append("âš ï¸ å•†å“åè¾ƒçŸ­ï¼ˆå¹³å‡{}å­—ï¼‰ï¼Œé™ä½æ–‡æœ¬æƒé‡è‡³0.4".format(int(avg_name_length)))
    
    # è§„åˆ™4ï¼šæ¡ç è¦†ç›–ç‡ä½ï¼ˆæ”¾å®½é˜ˆå€¼ï¼‰
    if avg_barcode_coverage < 0.3:
        params['composite_threshold'] = 0.15
        recommendations.append("âš ï¸ æ¡ç è¦†ç›–ç‡ä½ï¼ˆ{}%ï¼‰ï¼Œé™ä½åŒ¹é…é˜ˆå€¼è‡³0.15".format(int(avg_barcode_coverage*100)))
    
    # è§„åˆ™5ï¼šå•†å“åå¾ˆé•¿ï¼ˆæ–‡æœ¬ç‰¹å¾å¾ˆé‡è¦ï¼‰
    if avg_name_length > 30:
        params['text_weight'] = 0.6
        params['brand_weight'] = 0.25
        recommendations.append("âœ… å•†å“åè¯¦ç»†ï¼ˆå¹³å‡{}å­—ï¼‰ï¼Œæé«˜æ–‡æœ¬æƒé‡è‡³0.6".format(int(avg_name_length)))
    
    return {
        'params': params,
        'recommendations': recommendations,
        'confidence': 'high' if len(recommendations) >= 2 else 'medium'
    }

def print_parameter_recommendations(result: dict):
    """æ‰“å°å‚æ•°æ¨èç»“æœ"""
    print("\n" + "="*70)
    print("ğŸ’¡ æ™ºèƒ½å‚æ•°æ¨è")
    print("="*70)
    
    if result['recommendations']:
        print("åŸºäºæ•°æ®ç‰¹å¾åˆ†æï¼Œå»ºè®®è°ƒæ•´ä»¥ä¸‹å‚æ•°ï¼š\n")
        for rec in result['recommendations']:
            print(f"  {rec}")
        
        print(f"\næ¨èå‚æ•°é…ç½®ï¼ˆç½®ä¿¡åº¦: {result['confidence']}ï¼‰:")
        params = result['params']
        print(f"  - æ–‡æœ¬ç›¸ä¼¼åº¦æƒé‡: {params['text_weight']}")
        print(f"  - å“ç‰ŒåŒ¹é…æƒé‡: {params['brand_weight']}")
        print(f"  - åˆ†ç±»åŒ¹é…æƒé‡: {params['category_weight']}")
        print(f"  - è§„æ ¼åŒ¹é…æƒé‡: {params['specs_weight']}")
        print(f"  - ç»¼åˆå¾—åˆ†é˜ˆå€¼: {params['composite_threshold']}")
        
        choice = input("\næ˜¯å¦åº”ç”¨æ¨èå‚æ•°? (y/nï¼Œé»˜è®¤n): ").strip().lower()
        if choice == 'y':
            print("âœ… å·²åº”ç”¨æ¨èå‚æ•°")
            return params
        else:
            print("â­ï¸  è·³è¿‡ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°")
            return None
    else:
        print("æ•°æ®ç‰¹å¾æ ‡å‡†ï¼Œä½¿ç”¨é»˜è®¤å‚æ•°å³å¯")
        return None
    
    print("="*70)
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬6å¤©**ï¼šå®ç°æ•°æ®ç‰¹å¾åˆ†æå‡½æ•°
2. **ç¬¬7å¤©**ï¼šå®ç°å‚æ•°æ¨èé€»è¾‘ï¼ˆ5æ¡è§„åˆ™ï¼‰
3. **ç¬¬8å¤©**ï¼šé›†æˆåˆ°ä¸»æµç¨‹ï¼ˆæ•°æ®åŠ è½½åè°ƒç”¨ï¼‰
4. **ç¬¬9å¤©**ï¼šæµ‹è¯•å¤šç§æ•°æ®é›†ï¼ˆå“ç‰Œä¸°å¯Œ/å“ç‰Œç¼ºå¤±/åç§°é•¿çŸ­ï¼‰

#### éªŒæ”¶æ ‡å‡†
- âœ… èƒ½è¯†åˆ«5ç§å…¸å‹æ•°æ®ç‰¹å¾
- âœ… æ¨èå‚æ•°åˆç†ï¼ˆç»äººå·¥éªŒè¯ï¼‰
- âœ… ç”¨æˆ·å¯é€‰æ‹©æ˜¯å¦åº”ç”¨
- âœ… åº”ç”¨åå‡†ç¡®ç‡æå‡â‰¥2%

---

### **ä¼˜åŒ–é¡¹2.2ï¼šæ•°æ®è´¨é‡æ£€æµ‹** â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- æå‰å‘ç° **è„æ•°æ®**ï¼ˆç©ºå€¼/é‡å¤/å¼‚å¸¸ä»·æ ¼ï¼‰
- é¿å…"åƒåœ¾è¾“å…¥â†’åƒåœ¾è¾“å‡º"
- æå‡æ•´ä½“å‡†ç¡®ç‡

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
def validate_input_data(df: pd.DataFrame, store_name: str) -> dict:
    """
    æ•°æ®è´¨é‡æ£€æµ‹
    
    æ£€æµ‹é¡¹ï¼š
    1. å¿…éœ€åˆ—ç¼ºå¤±
    2. ç©ºå€¼è¿‡å¤š
    3. ä»·æ ¼å¼‚å¸¸
    4. å•†å“åé‡å¤ç‡
    5. ç¼–ç é—®é¢˜
    """
    issues = []
    warnings = []
    
    # æ£€æµ‹1: å¿…éœ€åˆ—
    required_cols = ['å•†å“åç§°', 'å”®ä»·']
    missing_cols = [c for c in required_cols if c not in df.columns]
    if missing_cols:
        issues.append(f"âŒ ç¼ºå°‘å¿…éœ€åˆ—: {missing_cols}")
    
    if 'å•†å“åç§°' in df.columns:
        # æ£€æµ‹2: å•†å“åç©ºå€¼
        null_rate = df['å•†å“åç§°'].isna().sum() / len(df)
        if null_rate > 0.1:
            warnings.append(f"âš ï¸ å•†å“åç§°ç©ºå€¼ç‡ {null_rate*100:.1f}%ï¼ˆå»ºè®®<10%ï¼‰")
        
        # æ£€æµ‹3: å•†å“åé‡å¤
        dup_rate = df['å•†å“åç§°'].duplicated().sum() / len(df)
        if dup_rate > 0.3:
            warnings.append(f"âš ï¸ å•†å“åé‡å¤ç‡ {dup_rate*100:.1f}%ï¼ˆå¯èƒ½æ˜¯å¤šè§„æ ¼å•†å“ï¼‰")
        
        # æ£€æµ‹4: ç¼–ç é—®é¢˜
        has_mojibake = df['å•†å“åç§°'].str.contains('ï¿½', na=False).sum()
        if has_mojibake > 0:
            issues.append(f"âŒ å‘ç° {has_mojibake} ä¸ªä¹±ç å•†å“åï¼ˆç¼–ç é”™è¯¯ï¼‰")
    
    if 'å”®ä»·' in df.columns:
        # æ£€æµ‹5: ä»·æ ¼å¼‚å¸¸
        invalid_price = (df['å”®ä»·'] <= 0).sum() | df['å”®ä»·'].isna().sum()
        if invalid_price > 0:
            warnings.append(f"âš ï¸ å‘ç° {invalid_price} ä¸ªæ— æ•ˆä»·æ ¼ï¼ˆâ‰¤0æˆ–ç©ºå€¼ï¼‰")
        
        # æ£€æµ‹6: ä»·æ ¼ç¦»ç¾¤
        if df['å”®ä»·'].max() > 10000:
            outliers = df[df['å”®ä»·'] > 10000]
            warnings.append(f"âš ï¸ å‘ç° {len(outliers)} ä¸ªé«˜ä»·å•†å“ï¼ˆ>10000å…ƒï¼‰ï¼Œè¯·ç¡®è®¤")
    
    # æ£€æµ‹7: æ•°æ®è§„æ¨¡
    if len(df) < 100:
        warnings.append(f"âš ï¸ å•†å“æ•°é‡è¾ƒå°‘ï¼ˆ{len(df)}ä¸ªï¼‰ï¼ŒåŒ¹é…ç»“æœå¯èƒ½ä¸ç†æƒ³")
    
    return {
        'store_name': store_name,
        'total_items': len(df),
        'issues': issues,  # ä¸¥é‡é—®é¢˜ï¼ˆå»ºè®®ä¿®å¤ï¼‰
        'warnings': warnings,  # è­¦å‘Šï¼ˆå¯å¿½ç•¥ï¼‰
        'is_valid': len(issues) == 0
    }

def print_data_quality_report(report: dict):
    """æ‰“å°æ•°æ®è´¨é‡æŠ¥å‘Š"""
    print("\n" + "="*70)
    print(f"ğŸ“‹ ã€{report['store_name']}ã€‘æ•°æ®è´¨é‡æ£€æµ‹")
    print("="*70)
    print(f"æ€»å•†å“æ•°: {report['total_items']}")
    
    if report['issues']:
        print("\nğŸš¨ ä¸¥é‡é—®é¢˜ï¼ˆå»ºè®®ä¿®å¤åè¿è¡Œï¼‰:")
        for issue in report['issues']:
            print(f"  {issue}")
    
    if report['warnings']:
        print("\nâš ï¸  è­¦å‘Šä¿¡æ¯:")
        for warning in report['warnings']:
            print(f"  {warning}")
    
    if not report['issues'] and not report['warnings']:
        print("\nâœ… æ•°æ®è´¨é‡è‰¯å¥½ï¼Œå¯ä»¥å¼€å§‹æ¯”ä»·")
    
    print("="*70)
    
    # å¦‚æœæœ‰ä¸¥é‡é—®é¢˜ï¼Œè¯¢é—®æ˜¯å¦ç»§ç»­
    if report['issues']:
        choice = input("\næ£€æµ‹åˆ°ä¸¥é‡é—®é¢˜ï¼Œæ˜¯å¦ç»§ç»­è¿è¡Œ? (y/n): ").strip().lower()
        if choice != 'y':
            print("âŒ å·²ä¸­æ­¢è¿è¡Œï¼Œè¯·ä¿®å¤æ•°æ®åé‡è¯•")
            sys.exit(0)
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬9å¤©ä¸‹åˆ**ï¼šå®ç°æ•°æ®è´¨é‡æ£€æµ‹å‡½æ•°
2. **ç¬¬10å¤©**ï¼šåœ¨æ•°æ®åŠ è½½åç«‹å³è°ƒç”¨æ£€æµ‹
3. **ç¬¬10å¤©**ï¼šæµ‹è¯•å„ç§è„æ•°æ®åœºæ™¯
4. **ç¬¬11å¤©**ï¼šä¼˜åŒ–æ£€æµ‹è§„åˆ™ï¼ˆæ ¹æ®å®é™…æ•°æ®è°ƒæ•´é˜ˆå€¼ï¼‰

#### éªŒæ”¶æ ‡å‡†
- âœ… èƒ½è¯†åˆ«7ç§æ•°æ®è´¨é‡é—®é¢˜
- âœ… ä¸¥é‡é—®é¢˜é˜»æ–­è¿è¡Œï¼ˆéœ€ç”¨æˆ·ç¡®è®¤ï¼‰
- âœ… è­¦å‘Šä¿¡æ¯ä¸é˜»æ–­ï¼Œä»…æç¤º

---

### **ä¼˜åŒ–é¡¹2.3ï¼šè¿›åº¦æ¡ä¼˜åŒ–** â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- æ¸…æ™°å±•ç¤ºå½“å‰è¿›åº¦ï¼ˆä¸å†é»‘å±ç­‰å¾…ï¼‰
- é¢„ä¼°å‰©ä½™æ—¶é—´
- æå‡ç”¨æˆ·ä½“éªŒ

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
# åœ¨å‘é‡ç¼–ç æ—¶æ·»åŠ åµŒå¥—è¿›åº¦æ¡
from tqdm import tqdm

def encode_with_progress(texts, model, batch_size=128):
    """å¸¦è¿›åº¦æ¡çš„å‘é‡ç¼–ç """
    embeddings = []
    
    with tqdm(total=len(texts), desc="  ğŸ”„ å‘é‡ç¼–ç ", 
              ncols=100, ascii=True, leave=False) as pbar:
        for i in range(0, len(texts), batch_size):
            batch = texts[i:i+batch_size]
            batch_emb = model.encode(batch, show_progress_bar=False)
            embeddings.extend(batch_emb)
            pbar.update(len(batch))
    
    return np.array(embeddings)

# åœ¨ä¸»æµç¨‹ä¸­ä½¿ç”¨æ€»è¿›åº¦æ¡
def main_with_progress():
    """å¸¦æ€»è¿›åº¦æ¡çš„ä¸»æµç¨‹"""
    total_steps = 6
    
    with tqdm(total=total_steps, desc="ğŸ“Š æ€»è¿›åº¦", 
              ncols=100, ascii=True, position=0) as pbar:
        
        # æ­¥éª¤1: åŠ è½½æ•°æ®
        tqdm.write("ğŸ“‚ æ­¥éª¤1/6: åŠ è½½æ•°æ®...")
        df_a, df_b = load_data()
        pbar.update(1)
        
        # æ­¥éª¤2: æ•°æ®é¢„å¤„ç†
        tqdm.write("ğŸ”§ æ­¥éª¤2/6: æ•°æ®é¢„å¤„ç†...")
        df_a = preprocess(df_a)
        df_b = preprocess(df_b)
        pbar.update(1)
        
        # æ­¥éª¤3: å‘é‡ç¼–ç 
        tqdm.write("ğŸ¯ æ­¥éª¤3/6: å‘é‡ç¼–ç ...")
        df_a = encode_with_progress(df_a, model)
        pbar.update(1)
        
        # ... å…¶ä»–æ­¥éª¤
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬11å¤©**ï¼šæ·»åŠ å‘é‡ç¼–ç è¿›åº¦æ¡
2. **ç¬¬12å¤©**ï¼šæ·»åŠ æ€»è¿›åº¦æ¡ï¼ˆ6ä¸ªä¸»è¦æ­¥éª¤ï¼‰
3. **ç¬¬12å¤©**ï¼šæµ‹è¯•è¿›åº¦æ¡æ˜¾ç¤ºæ•ˆæœ

#### éªŒæ”¶æ ‡å‡†
- âœ… è¿›åº¦æ¡ä¸é‡å ã€ä¸é—ªçƒ
- âœ… é¢„ä¼°æ—¶é—´åŸºæœ¬å‡†ç¡®
- âœ… å…³é”®æ­¥éª¤æœ‰æ–‡å­—æç¤º

---

### **é˜¶æ®µ2æ€»ç»“**

#### å®Œæˆæ—¶é—´
- **é¢„è®¡**: 7ä¸ªå·¥ä½œæ—¥

#### é¢„æœŸæ”¶ç›Š
| æŒ‡æ ‡ | æå‡å¹…åº¦ |
|------|---------|
| åŒ¹é…å‡†ç¡®ç‡ | +3-5% |
| ç”¨æˆ·è°ƒå‚éš¾åº¦ | -80% |
| æ•°æ®è´¨é‡ | æå‰å‘ç°é—®é¢˜ |

#### äº¤ä»˜ç‰©
- âœ… æ™ºèƒ½å‚æ•°æ¨èåŠŸèƒ½
- âœ… æ•°æ®è´¨é‡æ£€æµ‹æŠ¥å‘Š
- âœ… ä¼˜åŒ–çš„è¿›åº¦æ˜¾ç¤º

---

## ğŸ“… **é˜¶æ®µ3ï¼šæ¶æ„ä¼˜åŒ–**ï¼ˆç¬¬3å‘¨ï¼Œé«˜æ”¶ç›Šé«˜é£é™©ï¼‰

### **ä¼˜åŒ–é¡¹3.1ï¼šGPUæ‰¹é‡ç¼–ç  OR å¤šè¿›ç¨‹å¹¶è¡Œ** â­â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- å‘é‡ç¼–ç é€Ÿåº¦æå‡ **5-10å€**ï¼ˆGPUï¼‰æˆ– **2-3å€**ï¼ˆå¤šè¿›ç¨‹ï¼‰
- å¤„ç†ä¸‡çº§SKUä»15åˆ†é’Ÿ â†’ 3åˆ†é’Ÿ

#### æŠ€æœ¯æ–¹æ¡ˆï¼ˆäºŒé€‰ä¸€ï¼‰

**æ–¹æ¡ˆAï¼šGPUæ‰¹é‡ç¼–ç **ï¼ˆä¼˜å…ˆæ¨èï¼‰
```python
def encode_with_gpu_optimization(texts, model):
    """GPUä¼˜åŒ–çš„å‘é‡ç¼–ç """
    import torch
    
    if not torch.cuda.is_available():
        print("âš ï¸ æœªæ£€æµ‹åˆ°GPUï¼Œå›é€€åˆ°CPUæ¨¡å¼")
        return model.encode(texts, show_progress_bar=True)
    
    # åŠ¨æ€è°ƒæ•´batch_size
    gpu_memory = torch.cuda.get_device_properties(0).total_memory / 1024**3
    if gpu_memory >= 8:
        batch_size = 256
    elif gpu_memory >= 6:
        batch_size = 128
    elif gpu_memory >= 4:
        batch_size = 64
    else:
        batch_size = 32
    
    print(f"ğŸš€ GPUæ¨¡å¼: {gpu_memory:.1f}GBæ˜¾å­˜, batch_size={batch_size}")
    
    try:
        embeddings = model.encode(
            texts,
            batch_size=batch_size,
            show_progress_bar=True,
            convert_to_numpy=True,
            normalize_embeddings=True,
            device='cuda'
        )
        
        # æ¸…ç†GPUç¼“å­˜
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
        
        return embeddings
    except RuntimeError as e:
        if 'out of memory' in str(e):
            print("âš ï¸ GPUå†…å­˜ä¸è¶³ï¼Œé™ä½batch_sizeé‡è¯•")
            torch.cuda.empty_cache()
            return model.encode(texts, batch_size=32, device='cuda')
        else:
            raise
```

**æ–¹æ¡ˆBï¼šå¤šè¿›ç¨‹å¹¶è¡Œ**ï¼ˆCPUå¤‡é€‰ï¼‰
```python
def encode_with_multiprocess(texts, model_name):
    """å¤šè¿›ç¨‹å¹¶è¡Œç¼–ç ï¼ˆCPUä¼˜åŒ–ï¼‰"""
    from concurrent.futures import ProcessPoolExecutor
    import multiprocessing
    import os
    
    # ç¦ç”¨CUDAï¼ˆé¿å…forkå†²çªï¼‰
    os.environ['CUDA_VISIBLE_DEVICES'] = ''
    
    num_workers = min(multiprocessing.cpu_count() - 1, 4)
    chunk_size = len(texts) // num_workers
    
    print(f"ğŸš€ å¤šè¿›ç¨‹æ¨¡å¼: {num_workers}æ ¸å¿ƒå¹¶è¡Œ")
    
    def encode_chunk(chunk_data):
        """å­è¿›ç¨‹æ‰§è¡Œçš„ç¼–ç ä»»åŠ¡"""
        chunk_texts, model_name = chunk_data
        from sentence_transformers import SentenceTransformer
        model = SentenceTransformer(model_name)
        return model.encode(chunk_texts, show_progress_bar=False)
    
    # åˆ‡åˆ†æ•°æ®
    chunks = [(texts[i:i+chunk_size], model_name) 
              for i in range(0, len(texts), chunk_size)]
    
    # å¹¶è¡Œæ‰§è¡Œ
    with ProcessPoolExecutor(max_workers=num_workers) as executor:
        results = list(executor.map(encode_chunk, chunks, timeout=600))
    
    return np.vstack(results)
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬13-14å¤©**ï¼šå®ç°GPUä¼˜åŒ–æ–¹æ¡ˆ
2. **ç¬¬15å¤©**ï¼šå®ç°å¤šè¿›ç¨‹å¤‡é€‰æ–¹æ¡ˆ
3. **ç¬¬16å¤©**ï¼šæ·»åŠ æ™ºèƒ½é€‰æ‹©é€»è¾‘ï¼ˆGPUä¼˜å…ˆï¼‰
4. **ç¬¬17å¤©**ï¼šå‹åŠ›æµ‹è¯•ï¼ˆ5K/1ä¸‡/2ä¸‡SKUï¼‰

#### éªŒæ”¶æ ‡å‡†
- âœ… GPUæ¨¡å¼é€Ÿåº¦æå‡â‰¥5å€
- âœ… CPUå¤šè¿›ç¨‹é€Ÿåº¦æå‡â‰¥2å€
- âœ… è‡ªåŠ¨æ£€æµ‹å¹¶é€‰æ‹©æœ€ä¼˜æ–¹æ¡ˆ
- âœ… CUDAé”™è¯¯è‡ªåŠ¨å›é€€CPU

#### é£é™©è¯„ä¼°
- **é£é™©ç­‰çº§**: â­â­â­â­ é«˜
- **æ½œåœ¨BUG**: CUDA OOMã€è¿›ç¨‹æ­»é”
- **ç¼“è§£æªæ–½**: åŠ¨æ€batch_sizeã€è¶…æ—¶æœºåˆ¶ã€å¼‚å¸¸æ•è·
- **å›æ»šæ–¹æ¡ˆ**: æ¢å¤åŸä¸²è¡Œç¼–ç 

---

### **ä¼˜åŒ–é¡¹3.2ï¼šåˆ†å—ç›¸ä¼¼åº¦è®¡ç®—** â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- å†…å­˜å ç”¨å‡å°‘ **50%**
- é¿å…å¤§æ•°æ®é›†OOMå´©æºƒ
- é€Ÿåº¦æå‡ **10-20%**

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
def chunked_cosine_similarity(vectors_a, vectors_b, chunk_size=None):
    """
    åˆ†å—è®¡ç®—ä½™å¼¦ç›¸ä¼¼åº¦ï¼ˆå†…å­˜å‹å¥½ï¼‰
    
    å‚æ•°:
        vectors_a: (N, D) æ•°ç»„
        vectors_b: (M, D) æ•°ç»„
        chunk_size: æ¯å—å¤§å°ï¼ˆè‡ªåŠ¨è®¡ç®—ï¼‰
    
    è¿”å›:
        sim_matrix: (N, M) ç›¸ä¼¼åº¦çŸ©é˜µ
    """
    from sklearn.metrics.pairwise import cosine_similarity
    import psutil
    
    # è‡ªåŠ¨è®¡ç®—chunk_size
    if chunk_size is None:
        available_memory = psutil.virtual_memory().available
        vector_size = vectors_a.shape[1] * 4  # float32
        matrix_size = len(vectors_b) * vector_size
        chunk_size = int(available_memory * 0.3 / matrix_size)
        chunk_size = max(500, min(chunk_size, 5000))  # é™åˆ¶500-5000
    
    print(f"  ğŸ’¾ åˆ†å—è®¡ç®—ï¼ˆchunk_size={chunk_size}ï¼ŒèŠ‚çœå†…å­˜50%ï¼‰")
    
    results = []
    n_chunks = (len(vectors_a) + chunk_size - 1) // chunk_size
    
    for i in range(n_chunks):
        start_idx = i * chunk_size
        end_idx = min((i+1) * chunk_size, len(vectors_a))
        chunk = vectors_a[start_idx:end_idx]
        
        # è®¡ç®—è¯¥å—ä¸æ‰€æœ‰Bçš„ç›¸ä¼¼åº¦
        chunk_sim = cosine_similarity(chunk, vectors_b)
        results.append(chunk_sim)
        
        # æ¸…ç†ä¸´æ—¶å˜é‡
        del chunk
        if i % 5 == 0:  # æ¯5å—æ¸…ç†ä¸€æ¬¡
            import gc
            gc.collect()
    
    sim_matrix = np.vstack(results)
    
    # éªŒè¯ç»“æœ
    assert sim_matrix.shape == (len(vectors_a), len(vectors_b)), \
        f"ç›¸ä¼¼åº¦çŸ©é˜µç»´åº¦é”™è¯¯: {sim_matrix.shape}"
    
    return sim_matrix
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬17å¤©ä¸‹åˆ**ï¼šå®ç°åˆ†å—è®¡ç®—å‡½æ•°
2. **ç¬¬18å¤©**ï¼šæ›¿æ¢æ‰€æœ‰ `cosine_similarity()` è°ƒç”¨
3. **ç¬¬19å¤©**ï¼šæµ‹è¯•å†…å­˜å ç”¨ï¼ˆå¯¹æ¯”ä¼˜åŒ–å‰åï¼‰

#### éªŒæ”¶æ ‡å‡†
- âœ… å†…å­˜å ç”¨å‡å°‘â‰¥40%
- âœ… è®¡ç®—ç»“æœä¸åŸç‰ˆä¸€è‡´
- âœ… é€Ÿåº¦æ— æ˜æ˜¾ä¸‹é™ï¼ˆÂ±10%ï¼‰

---

### **ä¼˜åŒ–é¡¹3.3ï¼šCross-Encoderæ‰¹é‡ä¼˜åŒ–** â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- Cross-Encoderé€Ÿåº¦æå‡ **3-5å€**
- å¤§æ•°æ®é›†åŒ¹é…æ—¶é—´ä»20åˆ†é’Ÿ â†’ 5åˆ†é’Ÿ

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
def batch_cross_encoder_predict(cross_encoder, all_candidate_pairs, batch_size=256):
    """
    æ‰¹é‡é¢„æµ‹Cross-Encoderå¾—åˆ†
    
    ä¼˜åŒ–ç­–ç•¥ï¼š
    1. å…ˆæ”¶é›†æ‰€æœ‰å€™é€‰å¯¹
    2. æ‰¹é‡æ£€æŸ¥ç¼“å­˜
    3. æœªç¼“å­˜çš„ä¸€æ¬¡æ€§é¢„æµ‹
    4. å®šæœŸæ¸…ç†GPU
    """
    import torch
    
    # æ‰¹é‡æ£€æŸ¥ç¼“å­˜
    cached_scores = {}
    pairs_to_predict = []
    pair_indices = []
    
    for idx, (text_a, text_b) in enumerate(all_candidate_pairs):
        cache_key = f"{text_a[:50]}||{text_b[:50]}"  # æˆªæ–­é¿å…è¿‡é•¿
        cached = cache_manager.get_cross_encoder_score(model_id, text_a, text_b)
        
        if cached is not None:
            cached_scores[idx] = cached
        else:
            pairs_to_predict.append((text_a, text_b))
            pair_indices.append(idx)
    
    print(f"  ğŸ’¾ ç¼“å­˜å‘½ä¸­: {len(cached_scores)}/{len(all_candidate_pairs)}")
    
    # æ‰¹é‡é¢„æµ‹æœªç¼“å­˜çš„
    new_scores = {}
    if pairs_to_predict:
        print(f"  ğŸš€ æ‰¹é‡é¢„æµ‹: {len(pairs_to_predict)} å¯¹")
        
        # åˆ†æ‰¹é¢„æµ‹ï¼ˆé¿å…OOMï¼‰
        for i in range(0, len(pairs_to_predict), batch_size):
            batch = pairs_to_predict[i:i+batch_size]
            batch_indices = pair_indices[i:i+batch_size]
            
            scores = cross_encoder.predict(batch, show_progress_bar=False)
            
            for j, score in enumerate(scores):
                original_idx = batch_indices[j]
                new_scores[original_idx] = float(score)
                
                # ä¿å­˜åˆ°ç¼“å­˜
                text_a, text_b = batch[j]
                cache_manager.set_cross_encoder_score(model_id, text_a, text_b, float(score))
            
            # å®šæœŸæ¸…ç†GPU
            if (i // batch_size) % 10 == 0:
                if torch.cuda.is_available():
                    torch.cuda.empty_cache()
    
    # åˆå¹¶ç»“æœ
    all_scores = {**cached_scores, **new_scores}
    return [all_scores[i] for i in range(len(all_candidate_pairs))]
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬19å¤©ä¸‹åˆ**ï¼šå®ç°æ‰¹é‡é¢„æµ‹å‡½æ•°
2. **ç¬¬20å¤©**ï¼šé‡æ„åŒ¹é…æµç¨‹ï¼ˆå…ˆæ”¶é›†å€™é€‰å¯¹ï¼‰
3. **ç¬¬21å¤©**ï¼šæµ‹è¯•é€Ÿåº¦æå‡

#### éªŒæ”¶æ ‡å‡†
- âœ… Cross-Encoderé€Ÿåº¦æå‡â‰¥3å€
- âœ… ç»“æœä¸åŸç‰ˆå®Œå…¨ä¸€è‡´
- âœ… ç¼“å­˜å‘½ä¸­ç‡â‰¥70%

---

### **é˜¶æ®µ3æ€»ç»“**

#### å®Œæˆæ—¶é—´
- **é¢„è®¡**: 10ä¸ªå·¥ä½œæ—¥
- **é£é™©ç¼“å†²**: +3å¤©

#### é¢„æœŸæ”¶ç›Š
| æŒ‡æ ‡ | æå‡å¹…åº¦ |
|------|---------|
| å‘é‡ç¼–ç é€Ÿåº¦ | +300-900% |
| å†…å­˜å ç”¨ | -50% |
| Cross-Encoderé€Ÿåº¦ | +200% |
| **æ€»ä½“é€Ÿåº¦** | **+200-300%** |

---

## ğŸ“… **é˜¶æ®µ4ï¼šé«˜çº§ç‰¹æ€§**ï¼ˆç¬¬4å‘¨ï¼‰

### **ä¼˜åŒ–é¡¹4.1ï¼šæ–­ç‚¹ç»­è·‘** â­â­â­â­

#### ä¸šåŠ¡ä»·å€¼
- å¤§æ•°æ®é›†è¿è¡Œ30åˆ†é’Ÿï¼Œä¸­é€”å´©æºƒä¸éœ€é‡å¤´æ¥
- æ”¯æŒæš‚åœ/æ¢å¤
- ç¨³å®šæ€§æå‡ **90%**

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
import pickle
import json
from pathlib import Path

class CheckpointManager:
    """æ£€æŸ¥ç‚¹ç®¡ç†å™¨"""
    
    def __init__(self, checkpoint_dir='checkpoints'):
        self.checkpoint_dir = Path(checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        self.version = '1.0'
    
    def save(self, step: str, data: dict):
        """ä¿å­˜æ£€æŸ¥ç‚¹"""
        checkpoint = {
            'version': self.version,
            'step': step,
            'timestamp': time.time(),
            'data': data
        }
        
        # æ•°æ®åˆ†ç¦»å­˜å‚¨ï¼ˆé¿å…pickleå¤§å¯¹è±¡ï¼‰
        meta_file = self.checkpoint_dir / f'{step}_meta.json'
        data_file = self.checkpoint_dir / f'{step}_data.parquet'
        
        # ä¿å­˜å…ƒæ•°æ®
        with open(meta_file, 'w', encoding='utf-8') as f:
            json.dump({
                'version': checkpoint['version'],
                'step': checkpoint['step'],
                'timestamp': checkpoint['timestamp']
            }, f, indent=2)
        
        # ä¿å­˜DataFrameæ•°æ®
        if 'df_a' in data:
            data['df_a'].to_parquet(self.checkpoint_dir / 'df_a.parquet')
        if 'df_b' in data:
            data['df_b'].to_parquet(self.checkpoint_dir / 'df_b.parquet')
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹å·²ä¿å­˜: {step}")
    
    def load(self, step: str) -> dict:
        """åŠ è½½æ£€æŸ¥ç‚¹"""
        meta_file = self.checkpoint_dir / f'{step}_meta.json'
        
        if not meta_file.exists():
            return None
        
        with open(meta_file, 'r', encoding='utf-8') as f:
            meta = json.load(f)
        
        # ç‰ˆæœ¬æ£€æŸ¥
        if meta['version'] != self.version:
            print(f"âš ï¸ æ£€æŸ¥ç‚¹ç‰ˆæœ¬ä¸åŒ¹é…({meta['version']} != {self.version})ï¼Œå¿½ç•¥")
            return None
        
        # åŠ è½½æ•°æ®
        data = {}
        if (self.checkpoint_dir / 'df_a.parquet').exists():
            data['df_a'] = pd.read_parquet(self.checkpoint_dir / 'df_a.parquet')
        if (self.checkpoint_dir / 'df_b.parquet').exists():
            data['df_b'] = pd.read_parquet(self.checkpoint_dir / 'df_b.parquet')
        
        print(f"âœ… ä»æ£€æŸ¥ç‚¹æ¢å¤: {step}")
        return data
    
    def clear(self):
        """æ¸…é™¤æ‰€æœ‰æ£€æŸ¥ç‚¹"""
        import shutil
        if self.checkpoint_dir.exists():
            shutil.rmtree(self.checkpoint_dir)
        self.checkpoint_dir.mkdir(exist_ok=True)
        print("ğŸ—‘ï¸  æ£€æŸ¥ç‚¹å·²æ¸…é™¤")

# ä½¿ç”¨ç¤ºä¾‹
def main_with_checkpoint():
    ckpt = CheckpointManager()
    
    # å°è¯•æ¢å¤
    checkpoint_data = ckpt.load('vectors_encoded')
    if checkpoint_data:
        df_a = checkpoint_data['df_a']
        df_b = checkpoint_data['df_b']
        print("âœ… è·³è¿‡å‘é‡ç¼–ç ï¼Œä»æ£€æŸ¥ç‚¹ç»§ç»­")
    else:
        # æ­£å¸¸æµç¨‹
        df_a, df_b = load_data()
        df_a, df_b = encode_vectors(df_a, df_b)
        ckpt.save('vectors_encoded', {'df_a': df_a, 'df_b': df_b})
    
    # åç»­æ­¥éª¤...
```

#### å®æ–½æ­¥éª¤
1. **ç¬¬22å¤©**ï¼šå®ç°CheckpointManagerç±»
2. **ç¬¬23å¤©**ï¼šåœ¨ä¸»æµç¨‹ä¸­æ’å…¥æ£€æŸ¥ç‚¹
3. **ç¬¬24å¤©**ï¼šæµ‹è¯•å´©æºƒæ¢å¤åœºæ™¯

#### éªŒæ”¶æ ‡å‡†
- âœ… æ”¯æŒ3ä¸ªå…³é”®æ£€æŸ¥ç‚¹ï¼ˆæ•°æ®åŠ è½½/å‘é‡ç¼–ç /åŒ¹é…å®Œæˆï¼‰
- âœ… å´©æºƒåèƒ½æ­£ç¡®æ¢å¤
- âœ… ç‰ˆæœ¬ä¸å…¼å®¹æ—¶è‡ªåŠ¨å¿½ç•¥æ—§æ£€æŸ¥ç‚¹

---

### **ä¼˜åŒ–é¡¹4.2ï¼šå¤šæ ¼å¼å¯¼å‡º** â­â­â­

#### æŠ€æœ¯æ–¹æ¡ˆ
```python
def export_multi_format(df, base_filename):
    """å¯¼å‡ºå¤šç§æ ¼å¼"""
    # Excelï¼ˆå…¼å®¹æ€§ï¼‰
    df.to_excel(f"{base_filename}.xlsx", index=False)
    
    # CSVï¼ˆé€šç”¨æ€§ï¼‰
    df.to_csv(f"{base_filename}.csv", index=False, encoding='utf-8-sig')
    
    # Parquetï¼ˆå¤§æ•°æ®ï¼‰
    df.to_parquet(f"{base_filename}.parquet", index=False)
    
    # HTMLï¼ˆæµè§ˆå™¨æŸ¥çœ‹ï¼‰
    df.head(1000).to_html(f"{base_filename}_preview.html", index=False)
    
    print(f"âœ… å·²å¯¼å‡º4ç§æ ¼å¼: xlsx / csv / parquet / html")
```

---

## ğŸ“Š **æ€»ä½“æ—¶é—´è¡¨**

| å‘¨æ¬¡ | é˜¶æ®µ | å·¥ä½œæ—¥ | ä¸»è¦äº§å‡º |
|------|------|--------|---------|
| **ç¬¬1å‘¨** | å¿«é€Ÿæ”¶ç›Š | 5å¤© | æ­£åˆ™é¢„ç¼–è¯‘+CSVç¼“å­˜+è´¨é‡æŠ¥å‘Š |
| **ç¬¬2å‘¨** | ä½“éªŒä¼˜åŒ– | 7å¤© | æ™ºèƒ½æ¨è+æ•°æ®æ£€æµ‹+è¿›åº¦æ¡ |
| **ç¬¬3å‘¨** | æ¶æ„ä¼˜åŒ– | 10å¤© | GPU/å¤šè¿›ç¨‹+åˆ†å—è®¡ç®—+æ‰¹é‡CE |
| **ç¬¬4å‘¨** | é«˜çº§ç‰¹æ€§ | 8å¤© | æ–­ç‚¹ç»­è·‘+å¤šæ ¼å¼å¯¼å‡º |
| **æ€»è®¡** | 4é˜¶æ®µ | **30å¤©** | 12é¡¹ä¼˜åŒ– |

---

## âœ… **éªŒæ”¶æ ‡å‡†ï¼ˆæ€»ä½“ç›®æ ‡ï¼‰**

### æ€§èƒ½æŒ‡æ ‡
- âœ… å¤„ç†é€Ÿåº¦æå‡ **3-5å€**ï¼ˆ25åˆ†é’Ÿ â†’ 5-8åˆ†é’Ÿï¼‰
- âœ… å†…å­˜å ç”¨å‡å°‘ **50%**ï¼ˆ8GB â†’ 4GBï¼‰
- âœ… åŒ¹é…å‡†ç¡®ç‡æå‡ **2-5%**ï¼ˆ92% â†’ 94-97%ï¼‰

### ç¨³å®šæ€§æŒ‡æ ‡
- âœ… å´©æºƒç‡é™è‡³ **0**ï¼ˆæ–­ç‚¹ç»­è·‘ï¼‰
- âœ… è„æ•°æ®æå‰æ£€æµ‹ç‡ **100%**

### ç”¨æˆ·ä½“éªŒæŒ‡æ ‡
- âœ… å‚æ•°è°ƒä¼˜é—¨æ§›é™ä½ **80%**ï¼ˆæ™ºèƒ½æ¨èï¼‰
- âœ… é—®é¢˜å®šä½æ—¶é—´å‡å°‘ **90%**ï¼ˆè´¨é‡æŠ¥å‘Šï¼‰

---

## ğŸš¨ **é£é™©ç®¡ç†**

### é£é™©ç™»è®°è¡¨

| é£é™© | æ¦‚ç‡ | å½±å“ | ç¼“è§£æªæ–½ |
|------|------|------|---------|
| CUDAå…¼å®¹æ€§é—®é¢˜ | ä¸­ | é«˜ | è‡ªåŠ¨å›é€€CPU |
| å¤šè¿›ç¨‹æ­»é” | ä½ | é«˜ | è¶…æ—¶æœºåˆ¶ |
| ç¼“å­˜ç‰ˆæœ¬ä¸å…¼å®¹ | ä¸­ | ä¸­ | ç‰ˆæœ¬æ£€æŸ¥ |
| æ–°BUGå¼•å…¥ | ä¸­ | ä¸­ | å……åˆ†å›å½’æµ‹è¯• |

### å›æ»šç­–ç•¥
- æ¯ä¸ªä¼˜åŒ–é¡¹ä¿ç•™å¼€å…³ï¼ˆConfigé…ç½®ï¼‰
- ä¿ç•™ä¼˜åŒ–å‰ç‰ˆæœ¬ä»£ç ï¼ˆæ³¨é‡Šå¤‡ä»½ï¼‰
- å…³é”®èŠ‚ç‚¹æ‰“Git Tag

---

## ğŸ“ **å¼€å‘è§„èŒƒ**

### ä»£ç è§„èŒƒ
1. æ‰€æœ‰ä¼˜åŒ–å‡½æ•°æ·»åŠ è¯¦ç»†æ³¨é‡Š
2. å…³é”®é€»è¾‘æ·»åŠ æ€§èƒ½æ—¥å¿—
3. å¼‚å¸¸å¤„ç†è¦†ç›–ç‡ **100%**

### æµ‹è¯•è§„èŒƒ
1. å•å…ƒæµ‹è¯•è¦†ç›–ç‡ **â‰¥80%**
2. å›å½’æµ‹è¯•ï¼šä¼˜åŒ–å‰åç»“æœå¯¹æ¯”
3. å‹åŠ›æµ‹è¯•ï¼š1K/5K/1ä¸‡/2ä¸‡SKU

### æ–‡æ¡£è§„èŒƒ
1. æ¯ä¸ªä¼˜åŒ–é¡¹æ›´æ–°ç”¨æˆ·æ–‡æ¡£
2. æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š
3. å¸¸è§é—®é¢˜FAQ

---

## ğŸ¯ **ç«‹å³è¡ŒåŠ¨è®¡åˆ’**

### æœ¬å‘¨ä»»åŠ¡ï¼ˆç¬¬1å‘¨ï¼‰
**Day 1-2**ï¼šæ­£åˆ™é¢„ç¼–è¯‘
**Day 3-4**ï¼šCSVç¼“å­˜åŠ é€Ÿ
**Day 5-7**ï¼šè´¨é‡è‡ªæ£€æŠ¥å‘Š

### ä¸‹å‘¨è®¡åˆ’ï¼ˆç¬¬2å‘¨ï¼‰
**Week 2**ï¼šæ™ºèƒ½å‚æ•°æ¨è + æ•°æ®è´¨é‡æ£€æµ‹

---

## ğŸ“ **æŠ€æœ¯æ”¯æŒ**

- **å¼€å‘è´Ÿè´£äºº**: [å¾…æŒ‡å®š]
- **æµ‹è¯•è´Ÿè´£äºº**: [å¾…æŒ‡å®š]
- **æ–‡æ¡£è´Ÿè´£äºº**: [å¾…æŒ‡å®š]

---

**æ–‡æ¡£çŠ¶æ€**: âœ… å·²å®Œæˆ  
**æœ€åæ›´æ–°**: 2025å¹´11æœˆ6æ—¥
